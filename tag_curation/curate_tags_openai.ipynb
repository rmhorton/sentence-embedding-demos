{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a80d3834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_DIR = \"D:/ml_data/stackexchange/\"\n",
    "DATA_DIR = \"C:/Users/marinch/Source/repos/sentence-embedding-demos/tag_curation/ml_data/stackexchange/\"\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5ce072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_file = 'ai_stackexchange_posts_embeddings.jsonl'\n",
    "# read jsonl file into a dataframe\n",
    "df = pd.read_json(jsonl_file, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b6bcead",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_file2 = 'ai_stackexchange_posts_embeddings-14100.jsonl'\n",
    "# concat the contents of jsonl_file2 to the dataframe\n",
    "df2 = pd.read_json(jsonl_file2, lines=True)\n",
    "df = pd.concat([df, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b1aac05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'model': 'text-embedding-ada-002', 'input': '...</td>\n",
       "      <td>{'object': 'list', 'data': [{'object': 'embedd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'model': 'text-embedding-ada-002', 'input': '...</td>\n",
       "      <td>{'object': 'list', 'data': [{'object': 'embedd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'model': 'text-embedding-ada-002', 'input': '\n",
       "'}</td>\n",
       "      <td>{'object': 'list', 'data': [{'object': 'embedd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'model': 'text-embedding-ada-002', 'input': '...</td>\n",
       "      <td>{'object': 'list', 'data': [{'object': 'embedd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'model': 'text-embedding-ada-002', 'input': '...</td>\n",
       "      <td>{'object': 'list', 'data': [{'object': 'embedd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23194</th>\n",
       "      <td>{'model': 'text-embedding-ada-002', 'input': '...</td>\n",
       "      <td>{'object': 'list', 'data': [{'object': 'embedd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23195</th>\n",
       "      <td>{'model': 'text-embedding-ada-002', 'input': '...</td>\n",
       "      <td>{'object': 'list', 'data': [{'object': 'embedd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23196</th>\n",
       "      <td>{'model': 'text-embedding-ada-002', 'input': '...</td>\n",
       "      <td>{'object': 'list', 'data': [{'object': 'embedd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23197</th>\n",
       "      <td>{'model': 'text-embedding-ada-002', 'input': '...</td>\n",
       "      <td>{'object': 'list', 'data': [{'object': 'embedd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23198</th>\n",
       "      <td>{'model': 'text-embedding-ada-002', 'input': '...</td>\n",
       "      <td>{'object': 'list', 'data': [{'object': 'embedd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23199 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 request  \\\n",
       "0      {'model': 'text-embedding-ada-002', 'input': '...   \n",
       "1      {'model': 'text-embedding-ada-002', 'input': '...   \n",
       "2      {'model': 'text-embedding-ada-002', 'input': '\n",
       "'}   \n",
       "3      {'model': 'text-embedding-ada-002', 'input': '...   \n",
       "4      {'model': 'text-embedding-ada-002', 'input': '...   \n",
       "...                                                  ...   \n",
       "23194  {'model': 'text-embedding-ada-002', 'input': '...   \n",
       "23195  {'model': 'text-embedding-ada-002', 'input': '...   \n",
       "23196  {'model': 'text-embedding-ada-002', 'input': '...   \n",
       "23197  {'model': 'text-embedding-ada-002', 'input': '...   \n",
       "23198  {'model': 'text-embedding-ada-002', 'input': '...   \n",
       "\n",
       "                                                response  \n",
       "0      {'object': 'list', 'data': [{'object': 'embedd...  \n",
       "1      {'object': 'list', 'data': [{'object': 'embedd...  \n",
       "2      {'object': 'list', 'data': [{'object': 'embedd...  \n",
       "3      {'object': 'list', 'data': [{'object': 'embedd...  \n",
       "4      {'object': 'list', 'data': [{'object': 'embedd...  \n",
       "...                                                  ...  \n",
       "23194  {'object': 'list', 'data': [{'object': 'embedd...  \n",
       "23195  {'object': 'list', 'data': [{'object': 'embedd...  \n",
       "23196  {'object': 'list', 'data': [{'object': 'embedd...  \n",
       "23197  {'object': 'list', 'data': [{'object': 'embedd...  \n",
       "23198  {'object': 'list', 'data': [{'object': 'embedd...  \n",
       "\n",
       "[23199 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set column names to request and response\n",
    "df.columns = ['request', 'response']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cc3d86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a text column based on request.input\n",
    "df['text'] = df['request'].apply(lambda x: x['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "703b0b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10457 [{'error': {'message': \"This model's maximum context length is 8191 tokens, however you requested 8759 tokens (8759 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}, {'error': {'message': \"This model's maximum context length is 8191 tokens, however you requested 8759 tokens (8759 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}, {'error': {'message': \"This model's maximum context length is 8191 tokens, however you requested 8759 tokens (8759 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}, {'error': {'message': \"This model's maximum context length is 8191 tokens, however you requested 8759 tokens (8759 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}, {'error': {'message': \"This model's maximum context length is 8191 tokens, however you requested 8759 tokens (8759 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}]\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "# remove any rows where the response does not contain an embedding\n",
    "responses = df['response'].tolist()\n",
    "i = 0\n",
    "for r in responses:\n",
    "    # try to get the embedding from the response\n",
    "    try:\n",
    "        r['data'][0]['embedding']\n",
    "        i += 1\n",
    "    except:\n",
    "        print(i, r)\n",
    "        # drop row i\n",
    "        df.drop(i, inplace=True)\n",
    "        print('-----------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d4c5c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add an embedding column based on response.data\n",
    "df['vector'] = df['response'].apply(lambda x: x['data'][0]['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1a77987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop request and response columns\n",
    "df = df.drop(['request', 'response'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdfb5254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23198, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43a39ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22534, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop rows where text is empty\n",
    "df = df[df['text'] != '\\n']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbbbf34c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22510, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop duplicates\n",
    "df = df.drop_duplicates(subset=['text'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f1b2756",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'ai_Posts.csv'\n",
    "\n",
    "ai_posts = pd.read_csv(csv_file)\n",
    "\n",
    "def combine_title_and_body(title, body):\n",
    "    # I'm having trouble getting over the fact that a missing string has a numeric type.\n",
    "    if title==title: # not NaN\n",
    "        text = title + ' ' + body\n",
    "    elif body == body:\n",
    "        text = body\n",
    "    else:\n",
    "        text = ''\n",
    "    return str(text)\n",
    "\n",
    "ai_posts['text'] = [ combine_title_and_body(row.title, row.body).replace('<|endoftext|>', '<endoftext>').strip() + \"\\n\" for idx, row in ai_posts.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66c065d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23179, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_posts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "864c5ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22515, 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop rows where text is empty\n",
    "ai_posts = ai_posts[ai_posts['text'] != '\\n']\n",
    "ai_posts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89865732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22511, 6)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop duplicates\n",
    "ai_posts = ai_posts.drop_duplicates(subset=['text'])\n",
    "ai_posts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a10f405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>tags</th>\n",
       "      <th>last_activity_date</th>\n",
       "      <th>text</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ai</td>\n",
       "      <td>What is \"backprop\"?</td>\n",
       "      <td>What does \"backprop\" mean? Is the \"backprop\" t...</td>\n",
       "      <td>neural-networks;backpropagation;terminology;de...</td>\n",
       "      <td>2021-07-08T10:45:23.250</td>\n",
       "      <td>What is \"backprop\"? What does \"backprop\" mean?...</td>\n",
       "      <td>[-0.011622741, -0.009513571, 0.030356765, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ai</td>\n",
       "      <td>How does noise affect generalization?</td>\n",
       "      <td>Does increasing the noise in data help to impr...</td>\n",
       "      <td>neural-networks;machine-learning;statistical-a...</td>\n",
       "      <td>2019-02-23T22:36:37.133</td>\n",
       "      <td>How does noise affect generalization? Does inc...</td>\n",
       "      <td>[-0.019474396, 0.01699275, 0.02936180100000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ai</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Backprop\" is the same as \"backpropagation\": i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-08-02T15:40:24.820</td>\n",
       "      <td>\"Backprop\" is the same as \"backpropagation\": i...</td>\n",
       "      <td>[-0.016287422, 0.01834437, 0.016636714, -0.014...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ai</td>\n",
       "      <td>How to find the optimal number of neurons per ...</td>\n",
       "      <td>When you're writing your algorithm, how do you...</td>\n",
       "      <td>neural-networks;hyperparameter-optimization;ar...</td>\n",
       "      <td>2021-01-19T23:54:07.813</td>\n",
       "      <td>How to find the optimal number of neurons per ...</td>\n",
       "      <td>[0.025172584, 0.030881435000000002, 0.01347391...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ai</td>\n",
       "      <td>Are humans intelligent according to the defini...</td>\n",
       "      <td>Given the following definition of an intellige...</td>\n",
       "      <td>philosophy;definitions;intelligent-agent</td>\n",
       "      <td>2019-06-15T18:29:55.520</td>\n",
       "      <td>Are humans intelligent according to the defini...</td>\n",
       "      <td>[-0.008540249, -0.0049193646, -0.0036919457, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22505</th>\n",
       "      <td>ai</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The purpose of evaluating the state and action...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-12-04T16:24:35.890</td>\n",
       "      <td>The purpose of evaluating the state and action...</td>\n",
       "      <td>[-0.022334248, 0.0014772362000000001, 0.034776...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22506</th>\n",
       "      <td>ai</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In machine translation, convolution is a techn...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-12-04T16:29:33.587</td>\n",
       "      <td>In machine translation, convolution is a techn...</td>\n",
       "      <td>[-0.02475856, 0.017485898, 0.0040235943, 0.006...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22507</th>\n",
       "      <td>ai</td>\n",
       "      <td>NaN</td>\n",
       "      <td>One of the key features of ChatGPT is its abil...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-12-04T16:32:24.353</td>\n",
       "      <td>One of the key features of ChatGPT is its abil...</td>\n",
       "      <td>[-0.029978512000000002, 0.0046682926, 0.012393...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22508</th>\n",
       "      <td>ai</td>\n",
       "      <td>My cross entropy loss gradient calculation is ...</td>\n",
       "      <td>Given a neural network model for Covid-19 clas...</td>\n",
       "      <td>neural-networks;homework</td>\n",
       "      <td>2022-12-04T16:49:03.533</td>\n",
       "      <td>My cross entropy loss gradient calculation is ...</td>\n",
       "      <td>[-0.00069859653, 0.019111514, 0.022301253, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22509</th>\n",
       "      <td>ai</td>\n",
       "      <td>Constraint Satisfaction Problem for 8-puzzle</td>\n",
       "      <td>My question is more related to the fundamental...</td>\n",
       "      <td>search;constraint-satisfaction-problems</td>\n",
       "      <td>2022-12-04T18:53:10.693</td>\n",
       "      <td>Constraint Satisfaction Problem for 8-puzzle M...</td>\n",
       "      <td>[0.014403689, 0.0044053467, 0.018643316, -0.01...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22510 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      topic                                              title  \\\n",
       "0        ai                                What is \"backprop\"?   \n",
       "1        ai              How does noise affect generalization?   \n",
       "2        ai                                                NaN   \n",
       "3        ai  How to find the optimal number of neurons per ...   \n",
       "4        ai  Are humans intelligent according to the defini...   \n",
       "...     ...                                                ...   \n",
       "22505    ai                                                NaN   \n",
       "22506    ai                                                NaN   \n",
       "22507    ai                                                NaN   \n",
       "22508    ai  My cross entropy loss gradient calculation is ...   \n",
       "22509    ai       Constraint Satisfaction Problem for 8-puzzle   \n",
       "\n",
       "                                                    body  \\\n",
       "0      What does \"backprop\" mean? Is the \"backprop\" t...   \n",
       "1      Does increasing the noise in data help to impr...   \n",
       "2      \"Backprop\" is the same as \"backpropagation\": i...   \n",
       "3      When you're writing your algorithm, how do you...   \n",
       "4      Given the following definition of an intellige...   \n",
       "...                                                  ...   \n",
       "22505  The purpose of evaluating the state and action...   \n",
       "22506  In machine translation, convolution is a techn...   \n",
       "22507  One of the key features of ChatGPT is its abil...   \n",
       "22508  Given a neural network model for Covid-19 clas...   \n",
       "22509  My question is more related to the fundamental...   \n",
       "\n",
       "                                                    tags  \\\n",
       "0      neural-networks;backpropagation;terminology;de...   \n",
       "1      neural-networks;machine-learning;statistical-a...   \n",
       "2                                                    NaN   \n",
       "3      neural-networks;hyperparameter-optimization;ar...   \n",
       "4               philosophy;definitions;intelligent-agent   \n",
       "...                                                  ...   \n",
       "22505                                                NaN   \n",
       "22506                                                NaN   \n",
       "22507                                                NaN   \n",
       "22508                           neural-networks;homework   \n",
       "22509            search;constraint-satisfaction-problems   \n",
       "\n",
       "            last_activity_date  \\\n",
       "0      2021-07-08T10:45:23.250   \n",
       "1      2019-02-23T22:36:37.133   \n",
       "2      2016-08-02T15:40:24.820   \n",
       "3      2021-01-19T23:54:07.813   \n",
       "4      2019-06-15T18:29:55.520   \n",
       "...                        ...   \n",
       "22505  2022-12-04T16:24:35.890   \n",
       "22506  2022-12-04T16:29:33.587   \n",
       "22507  2022-12-04T16:32:24.353   \n",
       "22508  2022-12-04T16:49:03.533   \n",
       "22509  2022-12-04T18:53:10.693   \n",
       "\n",
       "                                                    text  \\\n",
       "0      What is \"backprop\"? What does \"backprop\" mean?...   \n",
       "1      How does noise affect generalization? Does inc...   \n",
       "2      \"Backprop\" is the same as \"backpropagation\": i...   \n",
       "3      How to find the optimal number of neurons per ...   \n",
       "4      Are humans intelligent according to the defini...   \n",
       "...                                                  ...   \n",
       "22505  The purpose of evaluating the state and action...   \n",
       "22506  In machine translation, convolution is a techn...   \n",
       "22507  One of the key features of ChatGPT is its abil...   \n",
       "22508  My cross entropy loss gradient calculation is ...   \n",
       "22509  Constraint Satisfaction Problem for 8-puzzle M...   \n",
       "\n",
       "                                                  vector  \n",
       "0      [-0.011622741, -0.009513571, 0.030356765, 0.00...  \n",
       "1      [-0.019474396, 0.01699275, 0.02936180100000000...  \n",
       "2      [-0.016287422, 0.01834437, 0.016636714, -0.014...  \n",
       "3      [0.025172584, 0.030881435000000002, 0.01347391...  \n",
       "4      [-0.008540249, -0.0049193646, -0.0036919457, -...  \n",
       "...                                                  ...  \n",
       "22505  [-0.022334248, 0.0014772362000000001, 0.034776...  \n",
       "22506  [-0.02475856, 0.017485898, 0.0040235943, 0.006...  \n",
       "22507  [-0.029978512000000002, 0.0046682926, 0.012393...  \n",
       "22508  [-0.00069859653, 0.019111514, 0.022301253, -0....  \n",
       "22509  [0.014403689, 0.0044053467, 0.018643316, -0.01...  \n",
       "\n",
       "[22510 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge the two dataframes on the text column\n",
    "ai_posts = pd.merge(ai_posts, df, on='text')\n",
    "ai_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "965f564d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neural-networks                                                                       97\n",
       "reinforcement-learning                                                                86\n",
       "machine-learning                                                                      61\n",
       "neural-networks;machine-learning                                                      38\n",
       "natural-language-processing                                                           37\n",
       "                                                                                      ..\n",
       "neural-networks;machine-learning;hyper-parameters;neural-architecture-search           1\n",
       "reinforcement-learning;rewards;markov-decision-process;environment;markov-property     1\n",
       "deep-learning;tensorflow;keras;feedforward-neural-networks;multilayer-perceptrons      1\n",
       "neural-networks;deep-learning;convolutional-neural-networks;residual-networks;vgg      1\n",
       "neural-networks;homework                                                               1\n",
       "Name: tags, Length: 8819, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of times each tag was applied\n",
    "tag_counts = ai_posts['tags'].value_counts()\n",
    "tag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8728e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('neural-networks', 2332),\n",
       " ('reinforcement-learning', 2200),\n",
       " ('machine-learning', 2099),\n",
       " ('deep-learning', 1804),\n",
       " ('convolutional-neural-networks', 1067),\n",
       " ('natural-language-processing', 628),\n",
       " ('reference-request', 453),\n",
       " ('computer-vision', 450),\n",
       " ('deep-rl', 446),\n",
       " ('comparison', 430),\n",
       " ('classification', 426),\n",
       " ('training', 410),\n",
       " ('terminology', 376),\n",
       " ('q-learning', 354),\n",
       " ('recurrent-neural-networks', 334),\n",
       " ('python', 324),\n",
       " ('tensorflow', 320),\n",
       " ('dqn', 309),\n",
       " ('papers', 306),\n",
       " ('image-recognition', 278),\n",
       " ('long-short-term-memory', 270),\n",
       " ('ai-design', 265),\n",
       " ('datasets', 251),\n",
       " ('objective-functions', 250),\n",
       " ('keras', 240),\n",
       " ('game-ai', 238),\n",
       " ('backpropagation', 236),\n",
       " ('math', 227),\n",
       " ('generative-adversarial-networks', 220),\n",
       " ('object-detection', 210),\n",
       " ('optimization', 207),\n",
       " ('definitions', 197),\n",
       " ('gradient-descent', 188),\n",
       " ('transformer', 186),\n",
       " ('applications', 184),\n",
       " ('markov-decision-process', 183),\n",
       " ('pytorch', 180),\n",
       " ('philosophy', 179),\n",
       " ('agi', 178),\n",
       " ('policy-gradients', 178),\n",
       " ('genetic-algorithms', 172),\n",
       " ('deep-neural-networks', 169),\n",
       " ('activation-functions', 155),\n",
       " ('search', 154),\n",
       " ('data-preprocessing', 148),\n",
       " ('image-processing', 148),\n",
       " ('time-series', 134),\n",
       " ('research', 127),\n",
       " ('algorithm', 126),\n",
       " ('evolutionary-algorithms', 123),\n",
       " ('autoencoders', 123),\n",
       " ('regression', 122),\n",
       " ('prediction', 121),\n",
       " ('rewards', 120),\n",
       " ('unsupervised-learning', 119),\n",
       " ('generative-model', 118),\n",
       " ('hyperparameter-optimization', 117),\n",
       " ('image-segmentation', 110),\n",
       " ('attention', 109),\n",
       " ('proofs', 109),\n",
       " ('monte-carlo-tree-search', 107),\n",
       " ('actor-critic-methods', 107),\n",
       " ('implementation', 106),\n",
       " ('object-recognition', 105),\n",
       " ('algorithm-request', 103),\n",
       " ('models', 98),\n",
       " ('value-functions', 98),\n",
       " ('variational-autoencoder', 97),\n",
       " ('proximal-policy-optimization', 96),\n",
       " ('overfitting', 91),\n",
       " ('supervised-learning', 91),\n",
       " ('hyper-parameters', 85),\n",
       " ('convolution', 85),\n",
       " ('bert', 84),\n",
       " ('feedforward-neural-networks', 83),\n",
       " ('function-approximation', 82),\n",
       " ('weights', 82),\n",
       " ('convergence', 82),\n",
       " ('sutton-barto', 80),\n",
       " ('reward-functions', 79),\n",
       " ('graph-neural-networks', 78),\n",
       " ('word-embedding', 77),\n",
       " ('policies', 77),\n",
       " ('monte-carlo-methods', 76),\n",
       " ('architecture', 76),\n",
       " ('probability-distribution', 76),\n",
       " ('geometric-deep-learning', 74),\n",
       " ('open-ai', 73),\n",
       " ('computational-learning-theory', 72),\n",
       " ('loss', 72),\n",
       " ('neat', 71),\n",
       " ('temporal-difference-methods', 70),\n",
       " ('transfer-learning', 69),\n",
       " ('intelligent-agent', 68),\n",
       " ('multilayer-perceptrons', 68),\n",
       " ('minimax', 68),\n",
       " ('notation', 67),\n",
       " ('yolo', 66),\n",
       " ('pattern-recognition', 65),\n",
       " ('chat-bots', 64)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_tags = []\n",
    "for tag_str in ai_posts['tags'].values:\n",
    "    if tag_str == tag_str: # not NaN\n",
    "        tags = tag_str.split(';')\n",
    "        all_tags.extend(tags)\n",
    "    \n",
    "\n",
    "len(all_tags) # 36223\n",
    "\n",
    "Counter(all_tags).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60ebb6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "philosophy flag column created\n",
      "0    22330\n",
      "1      180\n",
      "Name: philosophy_flag, dtype: int64\n",
      "proofs flag column created\n",
      "0    22401\n",
      "1      109\n",
      "Name: proofs_flag, dtype: int64\n",
      "q-learning flag column created\n",
      "0    22154\n",
      "1      356\n",
      "Name: q-learning_flag, dtype: int64\n",
      "deep-rl flag column created\n",
      "0    22064\n",
      "1      446\n",
      "Name: deep-rl_flag, dtype: int64\n",
      "superintelligence flag column created\n",
      "0    22478\n",
      "1       32\n",
      "Name: superintelligence_flag, dtype: int64\n",
      "classification flag column created\n",
      "0    21950\n",
      "1      560\n",
      "Name: classification_flag, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose a variety of interesting and reasonably common tags\n",
    "target_tags = ['philosophy', 'proofs', 'q-learning', 'deep-rl',  'superintelligence', 'classification']\n",
    "\n",
    "for tt in target_tags:\n",
    "    flag_col = tt + '_flag'\n",
    "    ai_posts[flag_col] = [1 if tt in str(tag_str) else 0 for tag_str in ai_posts['tags'].values]\n",
    "    print(f'{tt} flag column created')\n",
    "    print(ai_posts[flag_col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bf94dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "tag_name = 'classification'\n",
    "flag_col = tag_name + '_flag'\n",
    "score_col = tag_name + '_score'\n",
    "\n",
    "X_all = [v for v in ai_posts['vector']]\n",
    "y_all = [f for f in ai_posts[flag_col]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.25, random_state=42)\n",
    "\n",
    "Cs = np.logspace(-4, 4, 5)\n",
    "\n",
    "clf = LogisticRegressionCV(Cs=Cs, max_iter=10000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "score_test = clf.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3143c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_auc(labels, scores):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(labels, scores)\n",
    "\n",
    "    auc = metrics.roc_auc_score(labels, scores)\n",
    "\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('fpr')\n",
    "    plt.ylabel('tpr')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.text(0.8, 0.2, f\"AUC:{auc:0.3f}\")\n",
    "\n",
    "plot_auc(y_test, score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5b4c569",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_posts[score_col] = clf.predict_proba(X_all)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87d2c490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>classification_flag</th>\n",
       "      <th>classification_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8935</th>\n",
       "      <td>Class imbalance and \"all zeros\" one-hot encoding? I tried this example for a multi class classifier, but when looking at the data I realized two things:\\n\\nThere are many examples of \"all zeros\" vectors, that is, messages that don't belong in any classification.\\nThese all-zeros are actually the majority, by far.\\n\\nIs it valid to have an all-zeros output for a certain input? I would guess a Sigmoid activation would have no problems with this, by simply not trying to force a one out of all the \"near zero\" outputs.\\nBut I also think an \"accuracy\" metric will be skewed too optimistically: if all outputs are zero 90% of the time, the network will quickly overfit to always output 0 all the time, and get 90% score.\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0.976336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12457</th>\n",
       "      <td>MNIST Classification code performing with 88%-90% whereas other codes online perform 95% on first epoch I have been trying to write code to implement plain neural net without convolution from scratch. I took some help online here and added my code to my github account.\\nI don't understand why the prediction made by my code is only 88%-90% accurate after the 1st epoch, whereas his code is 95% accurate after 1st epoch with the same parameters (Same Xavier initialization for weights, biases are not initialized, same hidden layer neurons). While his architecture uses 2 hidden layers, my code performed worse with 2 hidden layers. For 1 hidden layer, his code performs similar (~96%).\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0.943118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20504</th>\n",
       "      <td>Could I cluster the audio clips in order to improve the speed of their classification? I have a neural network which is very resource intensive and is used to classify audio clips. The classification is done in batches, where I record for a set period of time and then go through and classify the audio.\\nHowever, the time for classification is far too long.\\nSo I was thinking what if I could somehow embed the audio and then cluster them into similar audio files. This way I'd only need to classify a couple audio clips in the cluster and assume they're all the same.\\nIs this possible?\\nAll suggestion, improvements or help is very much appreciated!\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0.902755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15392</th>\n",
       "      <td>Can we use transformers for audio classification tasks? Since transformers are good at processing sequential data, can we also use them for audio classification problems (same as RNNs)?\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0.877393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7100</th>\n",
       "      <td>CNN output generally has more than one category in one-hot categorization? I'm a bit of a CNN newbie, and I'm trying to train one to image classify pictures of pretty similar looking particles. I'm making the inputs and labels by hand from a set of 48x48 grayscale images, and labeling them with a one-hot vector based on their position in the sequence (for example, the 400/1000th image might have a one-hot in the 4th position if I have 10 categories in the run). I'm using sigmoidal output activation and categorical cross entropy loss. I've played around with a few different optimizers, as well. I'm implementing in python keras. \\nUnfortunately, although I have pretty good accuracy numbers for the training and validation, when I actually look at the outputs being produced, it generally gives multiple categories, which is not at all what I want. For example, if I have 6 categories and a label of 3, it might give the following probability vector:\\n[ .99 .98  1.0  .99  0.02  0.05 ]\\nIt was my understanding that categorical cross entropy would not allow this type of categorization, and yet it is prevalent in my code. I am under the impression that I'm doing something fundamentally wrong, but I cant figure out what. Any help would be appreciated.\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0.871406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14251</th>\n",
       "      <td>Given the same features, do logistic regression and neural networks produce the same output? I have a binary classification problem. I have variables (features) var1, var2, var3, ..., var14.\\nUsing these variables (aka features) in a logistic regression, I get their weights.\\nIf I use the same set of variables in a neural network:\\n\\nShould I get a different output?\\nor\\n\\nShould I get the same output?\\n\\n\\nI developed a ROC Curve, and I have both lines overlaying on each other. I am not sure if I am missing something here.\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0.863790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12552</th>\n",
       "      <td>My CNN model performs bad on new (self-created) pictures, what are possible reasons? I wanted to train a model that recognizes sign language. I have found a dataset for this and was able to create a model that would get 94% accuracy on the test set. I have trained models before and my main goal is not to have the best model (I know 94% could easiy be tuned up). However these models where always for class exercises and thus were never used on 'real' new data.\\nSo I took a new picture of my hand that I know I wanted to be a certain letter (let's assume A).\\nSince my model was trained on 28x28 images, I needed to re-size my own image because it was larger. After that I fed this image to my model only to get a wrong classification.\\nhttps://imgur.com/a/QE6snTa\\nThese are my pictures (upper-left = my own image (expected class A), upper-right = an image of class A (that my model correctly classifies as A), bottom = picture of class Z (the class my image was classified as)).\\nYou can clearly see that my own image looks for more like the image of class A (that I wanted my model to predict), than the model it did predict.\\nWhat could be reasons that my model does not work on real-life images? (If code is wanted I can provide it ofcourse but since I don't know where I go wrong, it seemed out of line to copy all the code).\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0.843492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17263</th>\n",
       "      <td>How to detect the description of spine segments in short text using a neural network? The input data is a set of text chunks containing the description of the pathology or the surgical procedure:\\nFor instance:\\n\\nTere is a lumbar stenosis L3/4\\nPatient ist suffering from [...], MRI and X ray showed lumbar stenosis L3/4, segmental instability L3-5, foraminal stenosis L5/S1 both sides\\nThe patient [...] underwent an MRI showing cervical stenosis C4-7 with myelopathy\\n[...] showed lumbar adult scoliosis L2-S1 with Cobb angle of 42Â°\\nPatient fell from the chair [...] showed osteoporotic fracture L3\\n\\nNow, the ideal classificator would give me:\\n\\nSegments: L3,L4; typeofpathology: degenerative; subtypepathology: stenosis\\nSegments: L3,L4,L5,S1; type of pathology: degenerative; subtypepathology: stenosis, instability\\nSegments C4, C5, C6, C7;type of pathology: degenerative; subtypepathology: myelopathy\\nSegments L2,L3,L4,L5,S1;type of pathology: deformity; subtypepathology: de novo scoliosis\\nSegments L3; type of pathology: pathological fracture; subtypepathology: -\\n\\nI think that this cannot be reasonably achieved by a pre-programmed algorithm, because the amount of the text before the description can vary, and the choice of words can vary too. Is there an approach using neural networks or NLP tools that would have chance at reaching such classification? How large would the dataset used for training have to be (approximately)?\\nMaybe it would be reasonable to separate the two problems: detection of the segments AND detection of the pathology. For the segments, one could search for a pattern of C? T? L? or S? with ? being a number and then include all such segment descriptions in the next 20-30 characters, then use an algorithm to mark the continuous segments from the upper to the lower vertebra.\\nDone this, do neural networks offer any significant advantages over simple keyword matching classification? Most importantly, which NLP neural network tools would be the ones you would start trying with?\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0.843280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6613</th>\n",
       "      <td>How do two perceptrons produce different linear decision boundaries when learning? I've learned that you can use two perceptrons to ultimately create a classifier for non-linearly separable data. I'm trying to understand how / if  these two perceptrons converge to two different decision boundaries though.\\n\\nSource: https://tdb-alcorn.github.io/2017/12/17/seeing-like-a-perceptron.html\\nI don't understand how the second perceptron creates a different decision boundary when it has the same input as the first perceptron? I know the weights can be initialized differently but does this second perceptron classify something else? Shouldn't the decision boundaries be converge to be the the same ultimately after training?\\n\\nSource: https://tdb-alcorn.github.io/2017/12/17/seeing-like-a-perceptron.html\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0.816007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4337</th>\n",
       "      <td>Is 'job title classification' rather a problem of NLP or machine learning? first of all I want to specify the data available and what needs to be achieved: I have a huge amount of vacancies (in the millions). The information about the job title and the job description of each vacancy are stored separately. I also have a list of professions (around 3000), to which the vacancies shall be mapped.\\nExample: java-developer, java web engineer and java software developer shall all be mapped to the profession java engineer.\\nNow about my current researches and problems: Since a lot of potential training data is present, I thought a machine learning approach could be useful. I have been reading about different algorithms and wanted to give neural networks a shot. \\nVery fast I faced the problem, that I couldn't find a satisfying way to transform text of variable length to numerical vectors of constant size (needed by neural networks). As discussed here, this seems to be a non trivial problem. \\nI dug deeper and came across Bag of Words (BOW) and Text Frequency - Inverse Document Frequency (TFIDF), which seemed suitable at first glance. But here I faced other problems: If I feed all the job titles to TFIDF, the resulting word-weight-vectors will probably be very large (in the tenth of thousands). The search term on the other hand will mostly consist of between 1 and 5 words (we currently match the job title only). Hence, the neural network must be able to reliably map an ultra sparse input vector to one of a few thousand basic jobs. This sounds very difficult for me and I doubt a good classification quality.\\nAnother problem with BOW and TFIDF is, that they cannot handle typos and new words (I guess). They cannot be found in TFIDF's word list, which results in a vector filled with zeros. To sum it up: I was first excited to use TFIDF, but now think it doesn't work well for what I want to do.\\nThinking more about it, I now have doubt if neural networks or other machine learning approaches are even good solutions for this task at all. Maybe there are much better algorithms in the field of natural language processing. \\nThis moment (before digging into NLP) I decided to first gather the opinions of some more experienced AI users, so I don't miss the best solution. \\nSo what would be a useful approach to this in your opinion (best would be an approach that is capable of handling synonyms and typos)? Thanks in advance!\\np. s.: I am currently thinking about feeding the whole job description into the TFIDF and also do matches for new incoming vacancies with the whole document (instead of job title only). This will expand the size of the word-weight-vector, but it will be less sparse. Does this seem logical to you?\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0.796093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               text  \\\n",
       "8935                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Class imbalance and \"all zeros\" one-hot encoding? I tried this example for a multi class classifier, but when looking at the data I realized two things:\\n\\nThere are many examples of \"all zeros\" vectors, that is, messages that don't belong in any classification.\\nThese all-zeros are actually the majority, by far.\\n\\nIs it valid to have an all-zeros output for a certain input? I would guess a Sigmoid activation would have no problems with this, by simply not trying to force a one out of all the \"near zero\" outputs.\\nBut I also think an \"accuracy\" metric will be skewed too optimistically: if all outputs are zero 90% of the time, the network will quickly overfit to always output 0 all the time, and get 90% score.\\n   \n",
       "12457                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              MNIST Classification code performing with 88%-90% whereas other codes online perform 95% on first epoch I have been trying to write code to implement plain neural net without convolution from scratch. I took some help online here and added my code to my github account.\\nI don't understand why the prediction made by my code is only 88%-90% accurate after the 1st epoch, whereas his code is 95% accurate after 1st epoch with the same parameters (Same Xavier initialization for weights, biases are not initialized, same hidden layer neurons). While his architecture uses 2 hidden layers, my code performed worse with 2 hidden layers. For 1 hidden layer, his code performs similar (~96%).\\n   \n",
       "20504                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Could I cluster the audio clips in order to improve the speed of their classification? I have a neural network which is very resource intensive and is used to classify audio clips. The classification is done in batches, where I record for a set period of time and then go through and classify the audio.\\nHowever, the time for classification is far too long.\\nSo I was thinking what if I could somehow embed the audio and then cluster them into similar audio files. This way I'd only need to classify a couple audio clips in the cluster and assume they're all the same.\\nIs this possible?\\nAll suggestion, improvements or help is very much appreciated!\\n   \n",
       "15392                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Can we use transformers for audio classification tasks? Since transformers are good at processing sequential data, can we also use them for audio classification problems (same as RNNs)?\\n   \n",
       "7100                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  CNN output generally has more than one category in one-hot categorization? I'm a bit of a CNN newbie, and I'm trying to train one to image classify pictures of pretty similar looking particles. I'm making the inputs and labels by hand from a set of 48x48 grayscale images, and labeling them with a one-hot vector based on their position in the sequence (for example, the 400/1000th image might have a one-hot in the 4th position if I have 10 categories in the run). I'm using sigmoidal output activation and categorical cross entropy loss. I've played around with a few different optimizers, as well. I'm implementing in python keras. \\nUnfortunately, although I have pretty good accuracy numbers for the training and validation, when I actually look at the outputs being produced, it generally gives multiple categories, which is not at all what I want. For example, if I have 6 categories and a label of 3, it might give the following probability vector:\\n[ .99 .98  1.0  .99  0.02  0.05 ]\\nIt was my understanding that categorical cross entropy would not allow this type of categorization, and yet it is prevalent in my code. I am under the impression that I'm doing something fundamentally wrong, but I cant figure out what. Any help would be appreciated.\\n   \n",
       "14251                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Given the same features, do logistic regression and neural networks produce the same output? I have a binary classification problem. I have variables (features) var1, var2, var3, ..., var14.\\nUsing these variables (aka features) in a logistic regression, I get their weights.\\nIf I use the same set of variables in a neural network:\\n\\nShould I get a different output?\\nor\\n\\nShould I get the same output?\\n\\n\\nI developed a ROC Curve, and I have both lines overlaying on each other. I am not sure if I am missing something here.\\n   \n",
       "12552                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       My CNN model performs bad on new (self-created) pictures, what are possible reasons? I wanted to train a model that recognizes sign language. I have found a dataset for this and was able to create a model that would get 94% accuracy on the test set. I have trained models before and my main goal is not to have the best model (I know 94% could easiy be tuned up). However these models where always for class exercises and thus were never used on 'real' new data.\\nSo I took a new picture of my hand that I know I wanted to be a certain letter (let's assume A).\\nSince my model was trained on 28x28 images, I needed to re-size my own image because it was larger. After that I fed this image to my model only to get a wrong classification.\\nhttps://imgur.com/a/QE6snTa\\nThese are my pictures (upper-left = my own image (expected class A), upper-right = an image of class A (that my model correctly classifies as A), bottom = picture of class Z (the class my image was classified as)).\\nYou can clearly see that my own image looks for more like the image of class A (that I wanted my model to predict), than the model it did predict.\\nWhat could be reasons that my model does not work on real-life images? (If code is wanted I can provide it ofcourse but since I don't know where I go wrong, it seemed out of line to copy all the code).\\n   \n",
       "17263                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               How to detect the description of spine segments in short text using a neural network? The input data is a set of text chunks containing the description of the pathology or the surgical procedure:\\nFor instance:\\n\\nTere is a lumbar stenosis L3/4\\nPatient ist suffering from [...], MRI and X ray showed lumbar stenosis L3/4, segmental instability L3-5, foraminal stenosis L5/S1 both sides\\nThe patient [...] underwent an MRI showing cervical stenosis C4-7 with myelopathy\\n[...] showed lumbar adult scoliosis L2-S1 with Cobb angle of 42Â°\\nPatient fell from the chair [...] showed osteoporotic fracture L3\\n\\nNow, the ideal classificator would give me:\\n\\nSegments: L3,L4; typeofpathology: degenerative; subtypepathology: stenosis\\nSegments: L3,L4,L5,S1; type of pathology: degenerative; subtypepathology: stenosis, instability\\nSegments C4, C5, C6, C7;type of pathology: degenerative; subtypepathology: myelopathy\\nSegments L2,L3,L4,L5,S1;type of pathology: deformity; subtypepathology: de novo scoliosis\\nSegments L3; type of pathology: pathological fracture; subtypepathology: -\\n\\nI think that this cannot be reasonably achieved by a pre-programmed algorithm, because the amount of the text before the description can vary, and the choice of words can vary too. Is there an approach using neural networks or NLP tools that would have chance at reaching such classification? How large would the dataset used for training have to be (approximately)?\\nMaybe it would be reasonable to separate the two problems: detection of the segments AND detection of the pathology. For the segments, one could search for a pattern of C? T? L? or S? with ? being a number and then include all such segment descriptions in the next 20-30 characters, then use an algorithm to mark the continuous segments from the upper to the lower vertebra.\\nDone this, do neural networks offer any significant advantages over simple keyword matching classification? Most importantly, which NLP neural network tools would be the ones you would start trying with?\\n   \n",
       "6613                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          How do two perceptrons produce different linear decision boundaries when learning? I've learned that you can use two perceptrons to ultimately create a classifier for non-linearly separable data. I'm trying to understand how / if  these two perceptrons converge to two different decision boundaries though.\\n\\nSource: https://tdb-alcorn.github.io/2017/12/17/seeing-like-a-perceptron.html\\nI don't understand how the second perceptron creates a different decision boundary when it has the same input as the first perceptron? I know the weights can be initialized differently but does this second perceptron classify something else? Shouldn't the decision boundaries be converge to be the the same ultimately after training?\\n\\nSource: https://tdb-alcorn.github.io/2017/12/17/seeing-like-a-perceptron.html\\n   \n",
       "4337   Is 'job title classification' rather a problem of NLP or machine learning? first of all I want to specify the data available and what needs to be achieved: I have a huge amount of vacancies (in the millions). The information about the job title and the job description of each vacancy are stored separately. I also have a list of professions (around 3000), to which the vacancies shall be mapped.\\nExample: java-developer, java web engineer and java software developer shall all be mapped to the profession java engineer.\\nNow about my current researches and problems: Since a lot of potential training data is present, I thought a machine learning approach could be useful. I have been reading about different algorithms and wanted to give neural networks a shot. \\nVery fast I faced the problem, that I couldn't find a satisfying way to transform text of variable length to numerical vectors of constant size (needed by neural networks). As discussed here, this seems to be a non trivial problem. \\nI dug deeper and came across Bag of Words (BOW) and Text Frequency - Inverse Document Frequency (TFIDF), which seemed suitable at first glance. But here I faced other problems: If I feed all the job titles to TFIDF, the resulting word-weight-vectors will probably be very large (in the tenth of thousands). The search term on the other hand will mostly consist of between 1 and 5 words (we currently match the job title only). Hence, the neural network must be able to reliably map an ultra sparse input vector to one of a few thousand basic jobs. This sounds very difficult for me and I doubt a good classification quality.\\nAnother problem with BOW and TFIDF is, that they cannot handle typos and new words (I guess). They cannot be found in TFIDF's word list, which results in a vector filled with zeros. To sum it up: I was first excited to use TFIDF, but now think it doesn't work well for what I want to do.\\nThinking more about it, I now have doubt if neural networks or other machine learning approaches are even good solutions for this task at all. Maybe there are much better algorithms in the field of natural language processing. \\nThis moment (before digging into NLP) I decided to first gather the opinions of some more experienced AI users, so I don't miss the best solution. \\nSo what would be a useful approach to this in your opinion (best would be an approach that is capable of handling synonyms and typos)? Thanks in advance!\\np. s.: I am currently thinking about feeding the whole job description into the TFIDF and also do matches for new incoming vacancies with the whole document (instead of job title only). This will expand the size of the word-weight-vector, but it will be less sparse. Does this seem logical to you?\\n   \n",
       "\n",
       "       classification_flag  classification_score  \n",
       "8935                     0              0.976336  \n",
       "12457                    0              0.943118  \n",
       "20504                    0              0.902755  \n",
       "15392                    0              0.877393  \n",
       "7100                     0              0.871406  \n",
       "14251                    0              0.863790  \n",
       "12552                    0              0.843492  \n",
       "17263                    0              0.843280  \n",
       "6613                     0              0.816007  \n",
       "4337                     0              0.796093  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "show_cols = ['text', flag_col, score_col]\n",
    "\n",
    "# Highest scoring examples that do not have the tag\n",
    "ai_posts[ai_posts[flag_col]==0].sort_values(score_col, ascending=False).head(10)[show_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84bcebbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>classification_flag</th>\n",
       "      <th>classification_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12781</th>\n",
       "      <td>Applications of polar decomposition in Machine Learning Assume there exists a new and very efficient algorithm for calculating the polar decomposition of a matrix $A=UP$, where $U$ is a unitary matrix and $P$ is a positive-semidefinite Hermitian matrix. Would there be any interesting applications in Machine Learning? Maybe topic modeling? Or page ranking? I am interested in references to articles and books.\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2255</th>\n",
       "      <td>Detect patterns in sequences of actions I have to analyse sequences of actions that look more or less like this JSON blob. The question I'm trying to answer is whether there are recurring (sub)patterns that different users adopt when asked to perform a certain specific task -- in this case, the task is to build a mathematical formula using this editor. In particular I'd like to know if there are multiple significantly different ways in which people build the same expression.\\nI thought of creating a Markov model, but that would only give me the most likely sequence of actions of length N. An obvious alternative would be to build trees and count how many times a certain path occurs in the dataset. However, the nature of the expression-building process means that the sequences can be polluted by many confounding, non-significant actions (such as streaks of UNDO-REDO, deleting symbols, and the likes).\\nI might go the \"longest common subsequence\" route, but I'm not sure that would tell me if there are \"significantly different\" ways of building the same expression (in quotes because, for now, I don't have a rigorous definition of \"significantly different\", but, for example, one way would be to drag and drop-in-place all the symbols in the correct order, and another way would be to drag all the symbols onto the canvas, and then place them in the correct spots).\\nI thought this might be a nice challenge for some AI algorithm, but I'm quite a noob at that, so I'm open to suggestions.\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11419</th>\n",
       "      <td>How to update edge features in a graph using a loss function? Given a directed, edge attributed graph G, where the edge attribute is a probability value, and a particular node N (with binary features f1 and f2) in G, the algorithm that I want to implement is as follows:\\n\\nList all the outgoing edges from N, let this list be called edgelist_N.\\nFor all the edges in edgelist_N, randomly assign to the edge attribute a probability value such that the sum of all the probabilities assigned to the edges in the edgelist_N equals to 1.\\nTake the top x edges (x can be a hyperparameter).\\nList the nodes in which the edges from step 3 are incoming.\\nConstruct a subgraph with node N, the nodes from step 4 and the edges from step 3. \\nEmbed the subgraph (preferably using a GNN) and obtain it's embedding and use it with a classifier to predict say f1/f2. \\nPropagate the loss so as to update the edge probabilities, that was assigned randomly in step 2.\\n\\nI do not understand how to do step 7, i.e. update the edge attribute with the loss, so that edges which are more relevant in constructing the subgraph can be assigned a higher probability value.\\nAny suggestion would be highly appreciated.\\nThank you very much.\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>How does Pinterest decipher what's on unmarked pictures and categorize them? According to this article, Pinterest acquired VisualGraph, an image recognition and visual search technology startup.\\nHow does Pinterest apply VisualGraph technology for machine vision, image recognition and visual search in order to classify the images?\\nIn short, how do they predict the image categories? Based on what features?\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19788</th>\n",
       "      <td>Detecting cheats visually using AI I really like to play my favorite 3D shooter game online. Unfortunately, it is really old and cheat protection isn't really common there, but cheaters are! It is very frustrating, because it really kills all the fun playing against cheaters.\\nCurrently we rely to an admin that is spectating a person accused as cheater and decides if he gets banned from the server or not. But admins are not available 24/7.\\nThe common cheats are:\\n\\nWallhacks: seeing other players through walls\\nAimbots: aiming the enemy and shooting automatically when just one pixel of the other player enter the cheaters visible field\\n\\nI wonder if it is possible to take randomly short video footage (1 min) for each player (spectating him) and score if it is very likely that he plays with a cheat active or not.\\nI guess for wallhack, it might not be really possible because the player does not behave different as the other players, he is just seeing more (and might look against a wall where a non cheating player might not do so). However, i think is very hard to tell even for a human if a wallhack is enabled or not. I think the error rate is really high here.\\nBut for aimbots i think, it can be possible because for a human it is very obvious:\\nThe cheater moves/looks into one direction and if an enemy appears in back of the cheater he turns immediately (in ~1-2 frame) around and shoots (and mostly hit). A straight player would never see whats going on behind him but might luckily turn around and hit somebody, so one of those hits might not be a 100% guarantee accusing someone being a cheater, but several of those \"lucky shots\" will definitely.\\nI would say, for a human it usually takes one to three of those impossible movements to conclude that it is 99% sure that this person is cheating.\\nSince i only have basic experience with AI (detecting things on images), i don't know what can be suitable from AI \"tooling\"  to detect something like this, because the content of the video frames itself is not really relevant, but the \"change\" between the images is crucial i guess ...\\nHowever, is this possible to detect cheaters \"visually\" using AI ?\\nI googled a bit around and read something about \"next frame prediction\" using convolutional LSTM. Is it something like \"when a frame appears next that was not predicted it might be cheat\" ?\\nSince this seems to be a very complex area i just want to find out the right direction i should look for. Any keywords for me to google here?\\nI am not really deep into AI but I could also imagine that calculating some kind of a \"value\" of each video frame can be used here. If the value between the frames does alter too much, it is likely that the person moves \"unnatural\"\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10503</th>\n",
       "      <td>How to use LSTM to generate a paragraph A LSTM model can be trained to generate text sequences by feeding the first word. After feeding the first word, the model will generate a sequence of words (a sentence). Feed the first word to get the second word, feed the first word + the second word to get the third word, and so on.\\nHowever, about the next sentence, what should be the next first word? The thing is to generate a paragraph of multiple sentences.\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10312</th>\n",
       "      <td>How to train image segmentation task with only one class? Is there a neural network that has architecture optimizations for segmenting only one class (object and background)? I have tried U-net but it is not providing good enough results.\\nI am wondering if this can be due to the fact that my dataset has different image resolutions/aspect ratios.\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21880</th>\n",
       "      <td>machine learning for a budgeting application I am interested in finding references and previous applications where prior year budgets are analyzed to provide guidance for a current year budget.   Specifically, each year some two thousand items are evaluated for funding, with perhaps 500 funded in that year. Information is available in a spreadsheet with multiple parameters that are manually evaluated to determine if an individual item is funded in the budget. I would appreciate any guidance as to how best to make use of such data for say the previous 5 years, where I know what has been funded in those years, to assist in screening items for the current budget year, in particular what approach to ML would be best.  I have attempted a literature search but have not found anything directly relevant.\\nEdit:\\nFound this reference in my literature search, looks to be applicable:\\nhttps://www.datacamp.com/courses/case-study-school-budgeting-with-machine-learning-in-python\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10842</th>\n",
       "      <td>Text detection on English and Chinese language https://arxiv.org/abs/1910.07954\\nIn this paper, we have a convolutional character neural network where we have object detection by taking a character as a basic unit. First, we do character detection and recognition and then we go for text detection.\\n \\nHere (Page number 5 under the subheading Iterative character detection) it is written that a model trained on English and Chinese texts will generalize well in regards to text detection.\\nBut how English and Chinese texts are good for generalization in text detection.\\nIf you have any queries regarding the paper you can ask me in the comment section\\nThanks in advance!\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1604</th>\n",
       "      <td>Is there a measure of AI relative strength, modified by resources? For instance, Strength/Size$\\times$Speed, where size and speed refer to memory and processing.\\nWe now have very strong, narrow AI, but they tend to run on fast hardware without volume restrictions.\\nTo understand why I'm asking, this article on BBC may provide some insight: \"Which life form dominates Earth?\"  (If I was a betting man, I'd put money on tardigrades outlasting humans, and the secret of their success is that they require minimal resources and processing power, unlike higher-order automata.)\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            text  \\\n",
       "12781                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Applications of polar decomposition in Machine Learning Assume there exists a new and very efficient algorithm for calculating the polar decomposition of a matrix $A=UP$, where $U$ is a unitary matrix and $P$ is a positive-semidefinite Hermitian matrix. Would there be any interesting applications in Machine Learning? Maybe topic modeling? Or page ranking? I am interested in references to articles and books.\\n   \n",
       "2255                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Detect patterns in sequences of actions I have to analyse sequences of actions that look more or less like this JSON blob. The question I'm trying to answer is whether there are recurring (sub)patterns that different users adopt when asked to perform a certain specific task -- in this case, the task is to build a mathematical formula using this editor. In particular I'd like to know if there are multiple significantly different ways in which people build the same expression.\\nI thought of creating a Markov model, but that would only give me the most likely sequence of actions of length N. An obvious alternative would be to build trees and count how many times a certain path occurs in the dataset. However, the nature of the expression-building process means that the sequences can be polluted by many confounding, non-significant actions (such as streaks of UNDO-REDO, deleting symbols, and the likes).\\nI might go the \"longest common subsequence\" route, but I'm not sure that would tell me if there are \"significantly different\" ways of building the same expression (in quotes because, for now, I don't have a rigorous definition of \"significantly different\", but, for example, one way would be to drag and drop-in-place all the symbols in the correct order, and another way would be to drag all the symbols onto the canvas, and then place them in the correct spots).\\nI thought this might be a nice challenge for some AI algorithm, but I'm quite a noob at that, so I'm open to suggestions.\\n   \n",
       "11419                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         How to update edge features in a graph using a loss function? Given a directed, edge attributed graph G, where the edge attribute is a probability value, and a particular node N (with binary features f1 and f2) in G, the algorithm that I want to implement is as follows:\\n\\nList all the outgoing edges from N, let this list be called edgelist_N.\\nFor all the edges in edgelist_N, randomly assign to the edge attribute a probability value such that the sum of all the probabilities assigned to the edges in the edgelist_N equals to 1.\\nTake the top x edges (x can be a hyperparameter).\\nList the nodes in which the edges from step 3 are incoming.\\nConstruct a subgraph with node N, the nodes from step 4 and the edges from step 3. \\nEmbed the subgraph (preferably using a GNN) and obtain it's embedding and use it with a classifier to predict say f1/f2. \\nPropagate the loss so as to update the edge probabilities, that was assigned randomly in step 2.\\n\\nI do not understand how to do step 7, i.e. update the edge attribute with the loss, so that edges which are more relevant in constructing the subgraph can be assigned a higher probability value.\\nAny suggestion would be highly appreciated.\\nThank you very much.\\n   \n",
       "584                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  How does Pinterest decipher what's on unmarked pictures and categorize them? According to this article, Pinterest acquired VisualGraph, an image recognition and visual search technology startup.\\nHow does Pinterest apply VisualGraph technology for machine vision, image recognition and visual search in order to classify the images?\\nIn short, how do they predict the image categories? Based on what features?\\n   \n",
       "19788  Detecting cheats visually using AI I really like to play my favorite 3D shooter game online. Unfortunately, it is really old and cheat protection isn't really common there, but cheaters are! It is very frustrating, because it really kills all the fun playing against cheaters.\\nCurrently we rely to an admin that is spectating a person accused as cheater and decides if he gets banned from the server or not. But admins are not available 24/7.\\nThe common cheats are:\\n\\nWallhacks: seeing other players through walls\\nAimbots: aiming the enemy and shooting automatically when just one pixel of the other player enter the cheaters visible field\\n\\nI wonder if it is possible to take randomly short video footage (1 min) for each player (spectating him) and score if it is very likely that he plays with a cheat active or not.\\nI guess for wallhack, it might not be really possible because the player does not behave different as the other players, he is just seeing more (and might look against a wall where a non cheating player might not do so). However, i think is very hard to tell even for a human if a wallhack is enabled or not. I think the error rate is really high here.\\nBut for aimbots i think, it can be possible because for a human it is very obvious:\\nThe cheater moves/looks into one direction and if an enemy appears in back of the cheater he turns immediately (in ~1-2 frame) around and shoots (and mostly hit). A straight player would never see whats going on behind him but might luckily turn around and hit somebody, so one of those hits might not be a 100% guarantee accusing someone being a cheater, but several of those \"lucky shots\" will definitely.\\nI would say, for a human it usually takes one to three of those impossible movements to conclude that it is 99% sure that this person is cheating.\\nSince i only have basic experience with AI (detecting things on images), i don't know what can be suitable from AI \"tooling\"  to detect something like this, because the content of the video frames itself is not really relevant, but the \"change\" between the images is crucial i guess ...\\nHowever, is this possible to detect cheaters \"visually\" using AI ?\\nI googled a bit around and read something about \"next frame prediction\" using convolutional LSTM. Is it something like \"when a frame appears next that was not predicted it might be cheat\" ?\\nSince this seems to be a very complex area i just want to find out the right direction i should look for. Any keywords for me to google here?\\nI am not really deep into AI but I could also imagine that calculating some kind of a \"value\" of each video frame can be used here. If the value between the frames does alter too much, it is likely that the person moves \"unnatural\"\\n   \n",
       "10503                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 How to use LSTM to generate a paragraph A LSTM model can be trained to generate text sequences by feeding the first word. After feeding the first word, the model will generate a sequence of words (a sentence). Feed the first word to get the second word, feed the first word + the second word to get the third word, and so on.\\nHowever, about the next sentence, what should be the next first word? The thing is to generate a paragraph of multiple sentences.\\n   \n",
       "10312                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             How to train image segmentation task with only one class? Is there a neural network that has architecture optimizations for segmenting only one class (object and background)? I have tried U-net but it is not providing good enough results.\\nI am wondering if this can be due to the fact that my dataset has different image resolutions/aspect ratios.\\n   \n",
       "21880                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      machine learning for a budgeting application I am interested in finding references and previous applications where prior year budgets are analyzed to provide guidance for a current year budget.   Specifically, each year some two thousand items are evaluated for funding, with perhaps 500 funded in that year. Information is available in a spreadsheet with multiple parameters that are manually evaluated to determine if an individual item is funded in the budget. I would appreciate any guidance as to how best to make use of such data for say the previous 5 years, where I know what has been funded in those years, to assist in screening items for the current budget year, in particular what approach to ML would be best.  I have attempted a literature search but have not found anything directly relevant.\\nEdit:\\nFound this reference in my literature search, looks to be applicable:\\nhttps://www.datacamp.com/courses/case-study-school-budgeting-with-machine-learning-in-python\\n   \n",
       "10842                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Text detection on English and Chinese language https://arxiv.org/abs/1910.07954\\nIn this paper, we have a convolutional character neural network where we have object detection by taking a character as a basic unit. First, we do character detection and recognition and then we go for text detection.\\n \\nHere (Page number 5 under the subheading Iterative character detection) it is written that a model trained on English and Chinese texts will generalize well in regards to text detection.\\nBut how English and Chinese texts are good for generalization in text detection.\\nIf you have any queries regarding the paper you can ask me in the comment section\\nThanks in advance!\\n   \n",
       "1604                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Is there a measure of AI relative strength, modified by resources? For instance, Strength/Size$\\times$Speed, where size and speed refer to memory and processing.\\nWe now have very strong, narrow AI, but they tend to run on fast hardware without volume restrictions.\\nTo understand why I'm asking, this article on BBC may provide some insight: \"Which life form dominates Earth?\"  (If I was a betting man, I'd put money on tardigrades outlasting humans, and the secret of their success is that they require minimal resources and processing power, unlike higher-order automata.)\\n   \n",
       "\n",
       "       classification_flag  classification_score  \n",
       "12781                    1              0.000038  \n",
       "2255                     1              0.000271  \n",
       "11419                    1              0.000804  \n",
       "584                      1              0.002687  \n",
       "19788                    1              0.003435  \n",
       "10503                    1              0.003513  \n",
       "10312                    1              0.004126  \n",
       "21880                    1              0.004398  \n",
       "10842                    1              0.005339  \n",
       "1604                     1              0.005828  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lowest scoring examples that do have the tag\n",
    "ai_posts[ai_posts[flag_col]==1].sort_values(score_col, ascending=True).head(10)[show_cols]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mladsjune2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
