{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a80d3834",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"D:/ml_data/stackexchange/\"\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89157077",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'ai_Posts.csv'\n",
    "\n",
    "ai_posts = pd.read_csv(csv_file)\n",
    "\n",
    "def combine_title_and_body(title, body):\n",
    "    # I'm having trouble getting over the fact that a missing string has a numeric type.\n",
    "    if title==title: # not NaN\n",
    "        text = title + ' ' + body\n",
    "    elif body == body:\n",
    "        text = body\n",
    "    else:\n",
    "        text = ''\n",
    "    return str(text)\n",
    "\n",
    "ai_posts['text'] = [ combine_title_and_body(row.title, row.body) for idx, row in ai_posts.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c47aff6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featurization took  328.353 seconds\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentxformer = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "ai_posts['vector'] = sentxformer.encode(ai_posts['text'].values).tolist()\n",
    "\n",
    "end = time.time()\n",
    "print(f'featurization took {end - start: 0.3f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efd03df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        neural-networks;backpropagation;terminology;de...\n",
       "1        neural-networks;machine-learning;statistical-a...\n",
       "2                                                      NaN\n",
       "3        neural-networks;hyperparameter-optimization;ar...\n",
       "4                 philosophy;definitions;intelligent-agent\n",
       "                               ...                        \n",
       "23174                                                  NaN\n",
       "23175                                                  NaN\n",
       "23176                                                  NaN\n",
       "23177                             neural-networks;homework\n",
       "23178              search;constraint-satisfaction-problems\n",
       "Name: tags, Length: 23179, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurized_file = DATA_DIR + csv_file.replace('.csv', '_featurized.csv')\n",
    "\n",
    "ai_posts.to_csv(featurized_file, index=False) # ~400MB\n",
    "\n",
    "# ai_posts = pd.read_csv(featurized_file)\n",
    "# ai_posts['vector'] = [eval(v) for v in ai_posts['vector']]\n",
    "\n",
    "ai_posts['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8728e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('neural-networks', 2333),\n",
       " ('reinforcement-learning', 2200),\n",
       " ('machine-learning', 2100),\n",
       " ('deep-learning', 1804),\n",
       " ('convolutional-neural-networks', 1067),\n",
       " ('natural-language-processing', 628),\n",
       " ('reference-request', 453),\n",
       " ('computer-vision', 450),\n",
       " ('deep-rl', 446),\n",
       " ('comparison', 430),\n",
       " ('classification', 426),\n",
       " ('training', 411),\n",
       " ('terminology', 376),\n",
       " ('q-learning', 354),\n",
       " ('recurrent-neural-networks', 334),\n",
       " ('python', 324),\n",
       " ('tensorflow', 320),\n",
       " ('dqn', 309),\n",
       " ('papers', 306),\n",
       " ('image-recognition', 278),\n",
       " ('long-short-term-memory', 270),\n",
       " ('ai-design', 265),\n",
       " ('datasets', 251),\n",
       " ('objective-functions', 250),\n",
       " ('keras', 240),\n",
       " ('game-ai', 238),\n",
       " ('backpropagation', 236),\n",
       " ('math', 227),\n",
       " ('generative-adversarial-networks', 220),\n",
       " ('object-detection', 210),\n",
       " ('optimization', 207),\n",
       " ('definitions', 197),\n",
       " ('gradient-descent', 188),\n",
       " ('transformer', 186),\n",
       " ('applications', 184),\n",
       " ('markov-decision-process', 183),\n",
       " ('pytorch', 180),\n",
       " ('philosophy', 179),\n",
       " ('agi', 178),\n",
       " ('policy-gradients', 178),\n",
       " ('genetic-algorithms', 172),\n",
       " ('deep-neural-networks', 169),\n",
       " ('activation-functions', 155),\n",
       " ('search', 154),\n",
       " ('data-preprocessing', 148),\n",
       " ('image-processing', 148),\n",
       " ('time-series', 134),\n",
       " ('research', 127),\n",
       " ('algorithm', 126),\n",
       " ('evolutionary-algorithms', 123),\n",
       " ('autoencoders', 123),\n",
       " ('regression', 122),\n",
       " ('prediction', 121),\n",
       " ('rewards', 120),\n",
       " ('unsupervised-learning', 119),\n",
       " ('generative-model', 118),\n",
       " ('hyperparameter-optimization', 117),\n",
       " ('image-segmentation', 110),\n",
       " ('attention', 109),\n",
       " ('proofs', 109),\n",
       " ('monte-carlo-tree-search', 107),\n",
       " ('actor-critic-methods', 107),\n",
       " ('implementation', 106),\n",
       " ('object-recognition', 105),\n",
       " ('algorithm-request', 103),\n",
       " ('models', 98),\n",
       " ('value-functions', 98),\n",
       " ('variational-autoencoder', 97),\n",
       " ('proximal-policy-optimization', 96),\n",
       " ('overfitting', 91),\n",
       " ('supervised-learning', 91),\n",
       " ('hyper-parameters', 85),\n",
       " ('convolution', 85),\n",
       " ('bert', 84),\n",
       " ('feedforward-neural-networks', 83),\n",
       " ('function-approximation', 82),\n",
       " ('weights', 82),\n",
       " ('convergence', 82),\n",
       " ('sutton-barto', 80),\n",
       " ('reward-functions', 79),\n",
       " ('graph-neural-networks', 78),\n",
       " ('word-embedding', 77),\n",
       " ('policies', 77),\n",
       " ('monte-carlo-methods', 76),\n",
       " ('architecture', 76),\n",
       " ('probability-distribution', 76),\n",
       " ('geometric-deep-learning', 74),\n",
       " ('open-ai', 73),\n",
       " ('computational-learning-theory', 72),\n",
       " ('loss', 72),\n",
       " ('neat', 71),\n",
       " ('temporal-difference-methods', 70),\n",
       " ('transfer-learning', 69),\n",
       " ('intelligent-agent', 68),\n",
       " ('multilayer-perceptrons', 68),\n",
       " ('minimax', 68),\n",
       " ('notation', 67),\n",
       " ('yolo', 66),\n",
       " ('pattern-recognition', 65),\n",
       " ('chat-bots', 64)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_tags = []\n",
    "for tag_str in ai_posts['tags'].values:\n",
    "    if tag_str == tag_str: # not NaN\n",
    "        tags = tag_str.split(';')\n",
    "        all_tags.extend(tags)\n",
    "    \n",
    "\n",
    "len(all_tags) # 36223\n",
    "\n",
    "Counter(all_tags).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60ebb6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a variety of interesting and reasonably common tags\n",
    "target_tags = ['philosophy', 'proofs', 'q-learning', 'deep-rl',  'superintelligence', 'classification']\n",
    "\n",
    "for tt in target_tags:\n",
    "    flag_col = tt + '_flag'\n",
    "    ai_posts[flag_col] = [1 if tt in str(tag_str) else 0 for tag_str in ai_posts['tags'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bf94dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "tag_name = 'classification'\n",
    "flag_col = tag_name + '_flag'\n",
    "score_col = tag_name + '_score'\n",
    "\n",
    "X_all = [v for v in ai_posts['vector']]\n",
    "y_all = [f for f in ai_posts[flag_col]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.25, random_state=42)\n",
    "\n",
    "Cs = np.logspace(-4, 4, 5)\n",
    "\n",
    "clf = LogisticRegressionCV(Cs=Cs, max_iter=10000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "score_test = clf.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3143c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAk20lEQVR4nO3deZhUxfX/8fcJSJSIy4ASZBFUEBg2dWAgQTESBIwRROMPRVG/ECSKmhhRImLcg0sSQUUEMWpUMCKbioCiiIggSGDYohIX1iiLCrIP1O+PmoF2mIGemb59u/t+Xs8zT+beW3SfK6RPV9WtU+acQ0REoutHYQcgIiLhUiIQEYk4JQIRkYhTIhARiTglAhGRiKsYdgClVa1aNVe3bt2wwxARSSsfffTRBufcccVdS7tEULduXebPnx92GCIiacXMvizpmoaGREQiTolARCTilAhERCJOiUBEJOKUCEREIi6wRGBmT5vZ12a2pITrZmZDzWyFmeWZ2elBxSIiIiULskfwDNDpINc7A/ULfvoATwQYi4iIlCCwdQTOuZlmVvcgTboAzzlfB3uOmR1jZjWcc+uCiklEJNFenLuSiQvXBPoeFfbkc/yGtRx7WhP+/OvshL9+mHMENYFVMcerC84dwMz6mNl8M5u/fv36pAQnIhKPiQvXsGzd5sBev+7Kj7lvcC/u+Fs/DtuxLZD3CHNlsRVzrthdcpxzI4ARADk5OdpJRwKVjG94kjmWrdtM4xpH8dI1bRL7wjt2wF13wUMPQbVq8PST3NatZWLfo0CYiWA1UDvmuBawNqRYJKKK+9Cf+/kmAHLrZYURkqSZxjWOokuLYgczyqdrV5g6Fa6+Gv76Vzj22MS/R4EwE8EkoJ+ZjQFyge80PyCJUJpv9MV96OfWy6JLi5pcllsnkPhESrRlCxx2GBx+OAwYAH/8I3ToEPjbBpYIzGw0cDZQzcxWA38GDgNwzg0HJgPnASuAbcDVQcUiyZMKwyql+UavD31JGVOnQp8+cPnlcN99cPbZSXvrIJ8auvQQ1x1wXVDvL+EonDhrXOOo0GLQh7uklU2b4Kab4NlnoWFD+NWvkh5C2pWhltQT2wsIbOJMJBNNnw49esDGjTBwINx+ux8WSjIlAim32F5AYBNnIpno+OOhXj2YMgVatAgtDCUCKbWi8wDqBYjEyTk/BLRgAQwdCk2bwuzZYMU9TZ88SgSyT7wTvUUnY9ULEInD55/DNdfAm2/CmWfC9u1wxBGhJwFQIpAY8U70ajJWpBT27IHHH4c//Ql+9CMYNswnhB+lTvFnJYKI00SvSMA2bIA77oB27WD4cKiTel+gUiclSdK9OHclt41fvG+oR0M8Igmyezc88wzs3QvVq/s5gddfT8kkAOoRRErROYDCBHD/hU01zCOSKB99BP/3f5CXBzVqQMeOcNJJYUd1UOoRREjRKom59bKUBEQSZft2XxYiNxfWr4fx430SSAPqEUSM5gBEAtK1K0ybBr17+4qhxxwTdkRxUyLIcMVNBotIgmzeDJUq+dXAt90Gt9wC7duHHVWpaWgow8UOB2kyWCSBJk+GJk3g7rv9cbt2aZkEQD2CjKRHQkUCtGED/OEP8Pzz0LgxXHBB2BGVmxJBhoj98I9d+ategEgCvfmmLxL3zTd+bcBtt8GPfxx2VOWmRJAhYlcFa+WvSEBq1IAGDeCJJ3ydoAyhRJDmCnsCGgISCYBzMGoU/PvfvkxEkybw3nspUR8okZQI0kxJi8IKewEikiCffQa//S28/bbfLSyFisQlmhJBmilaGE7DQCIJtmePLxE9cCBUrAhPPunXBqRQkbhEUyJIA3oKSCSJNmyAu+7yj4I+8QTUqhV2RIHL3BSXQbQWQCRgu3bB00/vLxK3cCFMmhSJJADqEaQN9QJEAjJvni8St2SJ/+A/91yoWzfsqJJKPYIU9+LclfsmhEUkgbZtg5tvhtat/bqASZN8Eogg9QhSXOHcgIaDRBKsSxd46y3o0wcefBCOPjrsiEKjRJCCik4O59bL0lNBIonw3Xd+JfDhh8OgQX5l8C9+EXZUodPQUArS5LBIAF57DbKz/RNBAGedpSRQQD2CFFM4J5BbL0uTwyKJsH493HgjjB7ty0J06xZ2RClHPYIUozkBkQSaNs1XCB071vcE5s+Hli3DjirlqEeQZEVLRBSlOQGRBKpZExo18gvDsrPDjiZlqUeQZEX3DS5KcwIi5bB3L4wYAb/7nT/OzoaZM5UEDkE9ghBocZhIAFas8EXiZszwk8CFReLkkNQjSCItDhMJwJ498Ne/QrNmsGABjBwJ06crCZRCoD0CM+sEDAEqAE855wYXuX408DxQpyCWh51z/wgypjAUzgsUJgEN/Ygk0IYNcO+90KEDDBvm5wWkVAJLBGZWAXgc6ACsBuaZ2STn3LKYZtcBy5xzvzaz44CPzewF59yuoOJKthfnruS28YsBlYwWSZidO+G556BXr/1F4urUyci9ApIhyB5BK2CFc+4zADMbA3QBYhOBA6qYmQFHApuA/ABjSrrCJ4Tuv7CpEoBIIsyd6xPA0qVw4om+PtCJJ4YdVVoLco6gJrAq5nh1wblYjwGNgLXAYuBG59zeoi9kZn3MbL6ZzV+/fn1Q8QZGj4OKJMDWrXDTTdCmjS8V8frrkS0Sl2hBJoLi+miuyHFHYCFwAtACeMzMjjrgDzk3wjmX45zLOe644xIdZ2A0OSySQF27wt//Dn37+t7AeeeFHVHGCDIRrAZqxxzXwn/zj3U1MM55K4DPgYYBxpQ0sXMDmhwWKaNvv/WPgQLccQe8+66fED7qgO+LUg5BJoJ5QH0zq2dmlYDuwKQibVYC7QHMrDpwKvBZgDElRWwS0NyASBlNmvTDInFnnukLxUnCBZYInHP5QD9gKrAc+JdzbqmZ9TWzvgXN7gF+ZmaLgenArc65DUHFlCyaIBYph6+/hu7d/X4B1arBxReHHVHGC3QdgXNuMjC5yLnhMb+vBTJytkcTxCJlMGUK9OgB338P99wDt94Khx0WdlQZTyUmEqhw4diydZtpXENjmCKlVru2LxU9bJivGipJoUSQIMUtHBORQ9i7F5580i8Ie/JJPycwY0bYUUWOEkGCaF5ApJQ++QR694b33vPlIXbs8FtIStKp6FwCaV5AJA75+fDAA75I3OLF8I9/wNSpSgIhUiJIAC0cEymFjRt9IjjvPFi2DK66SjWCQqahoXJQVVGROO3cCc884/cLqF4dFi3yE8OSEpQIykhVRUXi9MEHvkjc8uVw8snwy18qCaQYJYIy0MphkTh8/z3cfjsMHeo/+KdM8UlAUo4SQRnoCSGROHTt6ncK69cP7r8fqlQJOyIpgSaLy0hPCIkU45tv9heJu/NO/2joo48qCaQ4JQIRSYxx4/xq4Dvv9Mdt2/ofSXlKBCJSPv/7ny8Md9FF8NOf+oJxklaUCEpJawZEYrzxhu8FvPaanwf48EM47bSwo5JS0mRxKRVOFGvNgAh+r+DTToPHH4eGGbGnVCQpEcQptrKoJoolsvbu9ZVBFy2CkSN9b2D69LCjknLS0FCcYstLqzcgkfTxx36HsOuvh1WrfJE4yQjqEcShcF4gt14WL13TJuxwRJJr9254+GG/ZWTlyr5URM+eqg+UQZQIDkGb0EvkffMNPPQQ/PrXfk3AT38adkSSYBoaOgStIpZI2rHDzwXs3QvHHw95efDyy0oCGUqJ4CBih4SUBCQyZs2C5s3huuvg7bf9uVq1wo1JAqVEcBB6VFQiZcsWXxfozDNh1y6YNk1F4iJCcwQlUG9AIqdrV3jnHbjxRrj3XjjyyLAjkiRRIiiBegMSCZs2+S0iK1eGe+7xTwK10ZNxUaOhoYNQb0Ay2tix0KjR/iJxP/uZkkBEKRGIRM26ddCtG/zmN37DmB49wo5IQqZEIBIlr7/uy0K88YbfQH7OHP+EkESaEkExVGFUMtZJJ0HLlr5W0C23QEVNE4oSQbE0USwZY88eGDLEbx4Pfk5g2jRo0CDcuCSlKBGUQBPFkvaWLfNrAn7/e795jIrESQmUCIrQsJCkvV27/DqA006DTz6B55/3G8ccfnjYkUmKCjQRmFknM/vYzFaY2YAS2pxtZgvNbKmZvRtkPPHQsJCkvW+/hb//HS680PcKevRQpVA5qMBmisysAvA40AFYDcwzs0nOuWUxbY4BhgGdnHMrzez4oOKJh1YTS9ravh1GjYJrr/VF4hYvhhNOCDsqSRNB9ghaASucc58553YBY4AuRdpcBoxzzq0EcM59HWA8h6TegKSlmTP9I6DXX+9LRICSgJRKkImgJrAq5nh1wblYDYBjzWyGmX1kZj2LeyEz62Nm881s/vr16wMK11NvQNLG5s2+B9CuHeTnw1tvQfv2YUclaSjIh4iLG5R0xbz/GUB74AjgAzOb45z75Ad/yLkRwAiAnJycoq8hEk1du8KMGfCHP/g6QT/5SdgRSZoKMhGsBmrHHNcC1hbTZoNzbiuw1cxmAs2BTxCRA23Y4AvEVa4M993nJ4Fbtw47KklzQQ4NzQPqm1k9M6sEdAcmFWkzETjTzCqaWWUgF1geYEwi6ck5GDPGLwj785/9uTZtlAQkIQLrETjn8s2sHzAVqAA87ZxbamZ9C64Pd84tN7MpQB6wF3jKObckqJhE0tKaNX4uYNIkXx6iZ7FTaSJlFmihEefcZGBykXPDixw/BDwUZBwiaeu11/w6gN274eGH/SrhChXCjkoyjCpOiaSyU07x+wQ8+qj/XSQAKjFRQKUlJCXs2eNXBV91lT9u2NCXjFYSkAApERTQYjIJ3dKl8POfw003+aeDVCROkkSJIIYWk0kodu2Cu+/2ReL++1948UV49VUViZOkUSIQCdu338LQoX7ryGXL4NJLVSROkkqJQCQM27b5DWP27NlfJO6FF+C448KOTCJIiQBNFEuSvfMONG3qHwWdMcOfq1EjzIgk4pQI0ESxJMl338E118A55/ihn3feUZE4SQlaR1BAE8USuK5dfcno/v3hzjt9vSCRFHDQRGBmPwJaO+dmJykekcyyfr2vClq5MvzlL35VcMuWYUcl8gMHHRpyzu0F/pqkWEQyh3P+MdDYInGtWysJSEqKZ45gmpldZKbn2UTisno1XHCBrxF0yin7VwmLpKh4EsFNwMvALjPbbGZbzGxzwHEljZ4YkoSaNAkaN4a33/alIt5/H7Kzw45K5KAOOVnsnKuSjEDCoieGJKEaNIC2beGxx+Ckk8KORiQucT01ZGbdgLb4rSbfc85NCDKoZNMTQ1Jm+fnwyCOQlwfPPeeLxE2efMg/JpJKDjk0ZGbDgL7AYmAJ0NfMHg86MJGUl5fndwnr399vJK8icZKm4ukRtAOaOOccgJk9i08KItG0cyfcf7//ycqCf/0LLr5Y9YEkbcUzWfwxEDtuUhu/taRING3eDMOG+eJwy5b5YnFKApLG4ukRVAWWm9mHBcctgQ/MbBKAc+6CoIITSRlbt8KIEXDDDb4w3JIlUL162FGJJEQ8ieAIoHPMsQEPAPcEEpFIqpk+HX77W/j8c2je3NcKUhKQDBJPIqjonHs39oSZHVH0nEjG+fZbuPlmGDUK6teHd9+Fs84KOyqRhCtxjsDMfmdmi4FTzSwv5udzMmSOQIvJ5KAuvBCeeQZuvRUWLVISkIx1sB7Bi8AbwF+AATHntzjnMuLTU4vJ5ABffQVHHukLxQ0eDBUrwhlnhB2VSKBK7BE4575zzn3hnLvUOfdlzE9GJIFCWkwmgC8S989/+vIQhUXicnOVBCQStDGNyMqV8KtfQc+ecOqp0KtX2BGJJJU2ppFomzgRLr/c9wiGDoVrr/V7BohEiBKBRJNzfhFYw4Zw9tnw6KNQt27YUYmEQkNDEi35+fDAA3DFFf741FPh1VeVBCTSlAgkOhYt8hPAAwbAtm0qEidSQIlAMt+OHXD77ZCTA2vWwNixMG4cHH542JGJpITIJgItJouQLVvgySf91pHLlsFFF4UdkUhKCTQRmFknM/vYzFaY2YCDtGtpZnvM7OIg44mlxWQZ7vvv4eGHYc8eXyRu2TK/SjgrK+zIRFJOYInAzCoAj+ML1jUGLjWzxiW0ewCYGlQsJdFisgw1bRo0aQK33AIzZ/pzxx0XbkwiKSzIHkErYIVz7jPn3C5gDNClmHbXA68AXwcYi0TBpk1w9dXQsaMf/3/vPfjFL8KOSiTlBZkIagKrYo5XF5zbx8xqAhcCww/2QmbWx8zmm9n89evXJzxQyRAXXujLRNx2GyxcCD//edgRiaSFIBeUFbdlkyty/Ahwq3Nujx1khyfn3AhgBEBOTk7R1yi1woni3HoaL057//sfVKnii8Q99BBUqgQtWoQdlUhaCbJHsBq/rWWhWsDaIm1ygDFm9gVwMTDMzLoGGBOgieKM4Jyf/G3cGO64w59r1UpJQKQMguwRzAPqm1k9YA3QHbgstoFzrl7h72b2DPCac25CgDHto4niNPbFF3DNNX5SuG1b6NMn7IhE0lpgicA5l29m/fBPA1UAnnbOLTWzvgXXDzovIFKs8eN9eQgzeOwx+N3v4EeRXQ4jkhCBFp1zzk0GJhc5V2wCcM5dFWQskuYKi8RlZ8MvfwlDhsCJJ4YdlUhG0FcpSW27d8P99/tVwQANGsCECUoCIgmkRCCpa8ECPwE8cKBfIbxzZ9gRiWQkJQJJPdu3w5/+5JPA//7n5wVeegl+/OOwIxPJSEoEknq2boVRo+DKK32NoK5dw45IJKMpEUhq2LIFHnzQDwFVq+YTwKhRcOyxYUcmkvGUCCR8U6b4InEDBvj6QOCTgYgkhRKBhGfjRj/807mzLxHx/vt+/2ARSSptXi/h6dYNZs+GQYP8k0GaDBYJhRKBJNe6db5I3JFH+o1jKlWC5s3Djkok0jQ0JMnhHDz9NDRqtL9IXMuWSgIiKUCJQIL32Wdw7rnQq5f/4O/bN+yIRCRG5BKBNq1PsnHjoGlTmDsXnngC3nnHl4kQkZQRuTkC7UWQJIVF4po2hU6d4JFHoHbtQ/4xEUm+yPUIQHsRBGrXLrj3XrjsMp8M6teHV15REhBJYZFMBBKQ+fP9BPCgQf54165w4xGRuCgRSPlt3w633AK5ubBhA0ycCKNHa12ASJpQIpDy27rV7x/cqxcsXQoXXBB2RCJSCkoEUjabN8PgwfuLxC1fDiNGwDHHhB2ZiJSSEoGU3uuv+y0jBw7cXySuatVwYxKRMlMikPitX++3jDz/fDj6aF8nSEXiRNJe5NYRSDlcdBHMmQN33ul3EKtUKeyIRCQBlAjk4Nas8d/+jzwS/v53/yRQkyZhRyUiCaShISmeczByJDRuvL9I3BlnKAmIZCAlAjnQf/8L7dtDnz7+w/+668KOSEQCpEQgPzR2rK8P9NFH/nHQ6dPh5JPDjkpEAqQ5AvEKi8Q1bw6/+pWfD6hVK+yoRCQJ1COIul274K67oHv3/UXiXn5ZSUDSxvjx4zEz/vOf/wAwY8YMzj///B+0ueqqqxg7diwAu3fvZsCAAdSvX58mTZrQqlUr3njjjQNed9OmTXTo0IH69evToUMHvvnmm2Lff8iQITRp0oTs7GweeeSRfecHDRpEs2bNaNGiBeeeey5r167ddy0vL482bdqQnZ1N06ZN2bFjR3n/M5SLEkGUffihnwO4806oWFFF4iQtjR49mrZt2zJmzJi42g8aNIh169axZMkSlixZwquvvsqWLVsOaDd48GDat2/Pp59+Svv27Rk8ePABbZYsWcLIkSP58MMPWbRoEa+99hqffvopAP379ycvL4+FCxdy/vnnc/fddwOQn5/P5ZdfzvDhw1m6dCkzZszgsMMOK8d/gfJTIoiibdvg5puhTRv45ht49VV44QUViZO08/333/P+++8zatSouBLBtm3bGDlyJI8++ig/Lvj3Xr16dS655JID2k6cOJErr7wSgCuvvJIJEyYc0Gb58uW0bt2aypUrU7FiRdq1a8f48eMBOOqoo/a127p1K2YGwLRp02jWrBnNC7ZprVq1KhUqVCjdjSeYEkEUbd8Ozz/vnwpatsyvFBZJQxMmTKBTp040aNCArKwsFixYcND2K1asoE6dOj/4kI7Vu3dv5s+fD8BXX31FjRo1AKhRowZff/31Ae2bNGnCzJkz2bhxI9u2bWPy5MmsWrVq3/WBAwdSu3ZtXnjhhX09gk8++QQzo2PHjpx++uk8+OCDZbr3RAo0EZhZJzP72MxWmNmAYq73MLO8gp/ZZqadzIPy3Xdw332Qn+/rAi1f7reOLOH/ECLpYPTo0XTv3h2A7t27M3r06H3fvIsq6Xysp556ipycnLjfv1GjRtx666106NCBTp060bx5cypW3P8Mzn333ceqVavo0aMHjz32GOCHhmbNmsULL7zArFmzGD9+PNOnT4/7PYMQWCIwswrA40BnoDFwqZk1LtLsc6Cdc64ZcA8wIqh4Iu3VV/cvDJs1y5879thwYxIpp40bN/L222/Tu3dv6taty0MPPcRLL71EVlbWARO7mzZtolq1apxyyimsXLmy2DmBoqpXr866desAWLduHccff3yx7Xr16sWCBQuYOXMmWVlZ1K9f/4A2l112Ga+88goAtWrVol27dlSrVo3KlStz3nnnHbInE7QgewStgBXOuc+cc7uAMUCX2AbOudnOucK/sTmAHlVJpPXr4dJL/f4AVav6DeRVJE4yxNixY+nZsydffvklX3zxBatWraJevXps2rSJtWvXsnz5cgC+/PJLFi1aRIsWLahcuTK9evXihhtuYFfBwxHr1q3j+eefP+D1L7jgAp599lkAnn32Wbp06XJAG2DfkNHKlSsZN24cl156KcC+SWOASZMm0bBhQwA6duxIXl4e27ZtIz8/n3fffZfGjYt+R04y51wgP8DFwFMxx1cAjx2k/c2x7Ytc6wPMB+bXqVPHlcclw2e7S4bPLtdrpI0zz3TusMOcu/tu53buDDsakYRq166de+ONN35wbsiQIa5v375u1qxZLjc31zVv3tzl5OS4adOm7Wuzc+dO179/f3fyySe77Oxs16pVKzdlyhTnnHO9evVy8+bNc845t2HDBnfOOee4U045xZ1zzjlu48aNzjnn1qxZ4zp37rzv9dq2besaNWrkmjVr5t56661957t16+ays7Nd06ZN3fnnn+9Wr16979o///lP17hxY5edne369++f+P84xQDmuxI+f81fTzwz+w3Q0TnXu+D4CqCVc+76Ytr+AhgGtHXObTzY6+bk5LjCyZyy+H9PfgDAS9e0KfNrpLTVq/3mMEceCQsW+CeBsrPDjkpEQmZmHznnip0ACXJoaDVQO+a4FrC2aCMzawY8BXQ5VBKQg9i7F5580s8FFG4ef/rpSgIickhBJoJ5QH0zq2dmlYDuwKTYBmZWBxgHXOGc+yTAWDLbp5/COedA377QqhVcf0CnS0SkRIHVGnLO5ZtZP2AqUAF42jm31Mz6FlwfDtwBVAWGFTzalV9S10VK8PLL0LOnHwIaNQquvtrXDBIRiVOgReecc5OByUXODY/5vTfQO8gYYr04dyVzP99Ebr2sZL1lcAqLxJ12GnTpAn/7G5xwQthRiUgaitTK4okL1wDQpUXNkCMph507/XqASy7xyeCUU2DMGCUBESmzSCUCgNx6WVyWWyfsMMpmzhw/AXzPPXDEESoSJyIJEblEkJa2boU//AF+9jPYsgUmT4bnnlOROBFJCCWCdLBjhx/+ufZaWLoUOncOOyIRySDaoSxVffstPPoo/OlP+4vEHXNM2FGJSAZSjyAVTZjgF4bddRfMnu3PKQmISECUCFLJV1/5p4EuvBCOP94XiTvrrLCjEpEMp6GhVHLxxX77yHvvhVtugZC3rxORaFAiCNvKlX5vgCpVYOhQ/yRQ2CVpRSRSNDQUlr174fHHfVG4O+7w5047TUlARJJOiSAMH38M7dpBv35+A/kbbww7IhGJMCWCZPvXv6B5c1iyBP7xD5g6FerWDTsqEYkwJYJkKdwA6IwzoFs3vy7gqqtUKVREQqdEELQdO2DgQP9EkHNw8snw4ovw05+GHZmICKBEEKzZs/0E8P33+6eCVCRORFKQEkEQvv8ebrgB2raFbdtgyhR45hkViRORlKREEIRdu2DsWLjuOj8p3LFj2BGJiJRIC8oSZdMmvyDs9tshK8tPBh99dNhRiYgcknoEifDKK34h2L337i8SpyQgImlCiaA81q2Diy7yTwSdcALMn68icSKSdjQ0VB6XXALz5sHgwfDHP0JF/ecUkfSjT67S+vJLPwdQpYrfOOaII+DUU8OOSkSkzDQ0FK+9e/0Hf3Y2DBrkz7VooSQgImlPPYJ4/Oc/0Ls3vP8+dOrkN5IXEckQ6hEcypgxvkjc8uXw3HMweTKceGLYUYmIJIwSQUn27vX/27Il/OY3sGwZXHGFisSJSMZRIihq+3YYMMA/FlpYJO7556F69bAjExEJRGQSwYtzVzL3800Hb/Tee34C+IEHoGpV2L07KbGJiIQpMolg4sI1AHRpUfPAi1u2+LpAZ53lP/zffBOeegoqVUpylCIiyReZRACQWy+Ly3LrHHhh926YMAF+/3tYvBh++ctkhyYiEproPj66cSMMGeI3js/K8o+IVqkSdlQiIkkXaI/AzDqZ2cdmtsLMBhRz3cxsaMH1PDM7Pch4AD8B/PLLvkjcX/4CH3zgzysJiEhEBZYIzKwC8DjQGWgMXGpmjYs06wzUL/jpAzwRVDwAx3673u8XfMklULu2LxJ35plBvqWISMoLskfQCljhnPvMObcLGAN0KdKmC/Cc8+YAx5hZjaAC+v3IQX63sAcfhDlz/EIxEZGIC3KOoCawKuZ4NZAbR5uawLrYRmbWB99joE6dYiZ749D4hKOYccMdNOzUFBo0KNNriIhkoiATQXFLcF0Z2uCcGwGMAMjJyTngejz+/OtsILssf1REJKMFOTS0Gqgdc1wLWFuGNiIiEqAgE8E8oL6Z1TOzSkB3YFKRNpOAngVPD7UGvnPOrSv6QiIiEpzAhoacc/lm1g+YClQAnnbOLTWzvgXXhwOTgfOAFcA24Oqg4hERkeIFuqDMOTcZ/2Efe254zO8OuC7IGERE5OAiVWJCREQOpEQgIhJxSgQiIhGnRCAiEnHm52vTh5mtB74s4x+vBmxIYDjpQPccDbrnaCjPPZ/onDuuuAtplwjKw8zmO+dywo4jmXTP0aB7joag7llDQyIiEadEICIScVFLBCPCDiAEuudo0D1HQyD3HKk5AhEROVDUegQiIlKEEoGISMRlZCIws05m9rGZrTCzAcVcNzMbWnA9z8xODyPORIrjnnsU3Guemc02s7Tfp/NQ9xzTrqWZ7TGzi5MZXxDiuWczO9vMFprZUjN7N9kxJloc/7aPNrNXzWxRwT2ndRVjM3vazL42syUlXE/855dzLqN+8CWv/wucBFQCFgGNi7Q5D3gDv0Naa2Bu2HEn4Z5/Bhxb8HvnKNxzTLu38VVwLw477iT8PR8DLAPqFBwfH3bcSbjn24AHCn4/DtgEVAo79nLc81nA6cCSEq4n/PMrE3sErYAVzrnPnHO7gDFAlyJtugDPOW8OcIyZ1Uh2oAl0yHt2zs12zn1TcDgHvxtcOovn7xngeuAV4OtkBheQeO75MmCcc24lgHMu3e87nnt2QBUzM+BIfCLIT26YieOcm4m/h5Ik/PMrExNBTWBVzPHqgnOlbZNOSns/vfDfKNLZIe/ZzGoCFwLDyQzx/D03AI41sxlm9pGZ9UxadMGI554fAxrht7ldDNzonNubnPBCkfDPr0A3pgmJFXOu6DOy8bRJJ3Hfj5n9Ap8I2gYaUfDiuedHgFudc3v8l8W0F889VwTOANoDRwAfmNkc59wnQQcXkHjuuSOwEDgHOBl408zec85tDji2sCT88ysTE8FqoHbMcS38N4XStkkncd2PmTUDngI6O+c2Jim2oMRzzznAmIIkUA04z8zynXMTkhJh4sX7b3uDc24rsNXMZgLNgXRNBPHc89XAYOcH0FeY2edAQ+DD5ISYdAn//MrEoaF5QH0zq2dmlYDuwKQibSYBPQtm31sD3znn1iU70AQ65D2bWR1gHHBFGn87jHXIe3bO1XPO1XXO1QXGAtemcRKA+P5tTwTONLOKZlYZyAWWJznORIrnnlfie0CYWXXgVOCzpEaZXAn//Mq4HoFzLt/M+gFT8U8cPO2cW2pmfQuuD8c/QXIesALYhv9GkbbivOc7gKrAsIJvyPkujSs3xnnPGSWee3bOLTezKUAesBd4yjlX7GOI6SDOv+d7gGfMbDF+2ORW51zalqc2s9HA2UA1M1sN/Bk4DIL7/FKJCRGRiMvEoSERESkFJQIRkYhTIhARiTglAhGRiFMiEBGJOCUCkTIwsxvMbLmZvRB2LCLlpcdHRcrAzP6DX6H9eRxtKzjn9iQhLJEyUY9ApJTMbDi+LPIkM/vOzP5pZm+b2adm9tuCNmeb2Ttm9iK+EJpIylKPQKQMzOwLfC2jfvgKp62BnwD/xpd1aAC8DjSJp9cgEib1CETKb6JzbntBWYN38DX0AT5UEpB0oEQgUn5Fu9WFx1uTHYhIWSgRiJRfFzM73Myq4ouFzQs5HpFSUSIQKb8P8fMBc4B7nHPpvLeFRJAmi0XKwczuBL53zj0cdiwiZaUegYhIxKlHICISceoRiIhEnBKBiEjEKRGIiEScEoGISMQpEYiIRNz/Byv0Uiby8uXQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_auc(labels, scores):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(labels, scores)\n",
    "\n",
    "    auc = metrics.roc_auc_score(labels, scores)\n",
    "\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('fpr')\n",
    "    plt.ylabel('tpr')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.text(0.8, 0.2, f\"AUC:{auc:0.3f}\")\n",
    "\n",
    "plot_auc(y_test, score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5b4c569",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_posts[score_col] = clf.predict_proba(X_all)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87d2c490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>classification_flag</th>\n",
       "      <th>classification_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21808</th>\n",
       "      <td>Multiclass image classification - what approach to use and which models to consider? I'm working on an image classification project and I need to train a multiclass, multilabel classifier. The dataset is large and some of the images are mislabeled (for a given class, some labels are pretty easy to mix up). As an approach, I am using the following:\\n\\nConsidering the main available models, check their accuracy for the considered dataset at different learning rates. More in detail, my approach is to take a pretrained model, remove the last layer and fine-tune it on the dataset we're considering.\\nConsider the model and learning rate which gives the highest accuracy.\\nImprove the accuracy by grouping classes that are often confused. This is done by looking at the confusion matrix.\\n\\nDo you think this is a good approach to use? And which models should I consider in the first place? As of now I'm looking into the ResNet family (18, 34, 50, 101, 152 layers) and visual transformers.\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0.808390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17628</th>\n",
       "      <td>If your data is well balanced but small, I would recommend using a simpler algorithm to classify your data.\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0.807748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21681</th>\n",
       "      <td>What is the best approach/model for classifying document images with over 70+ classes of documents? What is the best approach/model for classifying document images with over 70+ classes of documents? I have tried LayoutLM which is a model that incorporated both NLP and Computer vision to classify but it's accuracy is not upto the mark. Are there any other models/methods I could try to come up with a better solution?\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0.708066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17078</th>\n",
       "      <td>Should one use an \"other\" category in image classification? In image classification, there are sometimes images that do not fit in any category.\\nFor example, if I build a CNN in Keras to classify Dogs and Cats, does it help (in terms of training time and performance) to create an \"other\" (or \"unclassified\") category in which images of houses, people, birds, etc., are classified? Is there any research paper that discusses this?\\nA similar question was asked before here, but, unfortunately, it has no answer.\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0.694076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>Classification is the automatic categorization of a new observation. This classification is based on a model produced from a training set of data containing observations whose classifications are given. Classification is especially useful for problems involving categorical data.\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0.676271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7964</th>\n",
       "      <td>Is there a Logistic Regression classifier that can perfectly classify the given data in this problem? I have the following problem.\\n\\nA bank wants to decide whether a customer can be given a loan,\\nbased on two features related to (i) the monthly salary of the customer, and (ii) his/her account balance. For simplicity, we model the two features with two binary variables $X1$, $X2$ and the class $Y$ (all of which can be either 0 or 1). $Y=1$ indicates that the customer can be given loan, and Y=0 indicates otherwise.\\nConsider the following dataset having four instances:\\n($X1 = 0$, $X2 = 0$, $Y = 0$)\\n($X1 = 0$, $X2 = 1$, $Y = 0$)\\n($X1 = 1$, $X2 = 0$, $Y = 0$)\\n($X1 = 1$, $X2 = 1$, $Y = 1$)\\nCan there be any logistic regression classifier using X1 and X2 as features, that can perfectly classify the given data?\\n\\nThe approach followed in the question was to calculate respective probabilities for Y=0 and Y=1 respectively.  The value of $p$ obtained was $0.25$ and $(1-p)$ as $0.75$. The $\\log(p/1-p)$ is coming as negative.\\nHowever, I don't understand what I need to do to understand whether there is a Logistic Regression classifier that can perfectly classify the given data.\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0.648970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7263</th>\n",
       "      <td>CNN output generally has more than one category in one-hot categorization? I'm a bit of a CNN newbie, and I'm trying to train one to image classify pictures of pretty similar looking particles. I'm making the inputs and labels by hand from a set of 48x48 grayscale images, and labeling them with a one-hot vector based on their position in the sequence (for example, the 400/1000th image might have a one-hot in the 4th position if I have 10 categories in the run). I'm using sigmoidal output activation and categorical cross entropy loss. I've played around with a few different optimizers, as well. I'm implementing in python keras. \\nUnfortunately, although I have pretty good accuracy numbers for the training and validation, when I actually look at the outputs being produced, it generally gives multiple categories, which is not at all what I want. For example, if I have 6 categories and a label of 3, it might give the following probability vector:\\n[ .99 .98  1.0  .99  0.02  0.05 ]\\nIt was my understanding that categorical cross entropy would not allow this type of categorization, and yet it is prevalent in my code. I am under the impression that I'm doing something fundamentally wrong, but I cant figure out what. Any help would be appreciated. \\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0.625950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6362</th>\n",
       "      <td>How to use CNN for making predictions on non-image data? I have a dataset which I have loaded as a data frame in Python. It consists of 21392 rows (the data instances, each row is one sample) and 1972 columns (the features). The last column i.e. column 1972 has string type labels (14 different categories of target labels). I would like to use a CNN to classify the data in this case and predict the target labels using the available features. This is a somewhat unconventional approach though it seems possible. However, I am very confused on how the methodology should be as I could not find any sample code/ pseudo code guiding on using CNN for Classifying non-image data, either in Tensorflow or Keras. Any help in this regard will be highly appreciated. Cheers!\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0.619778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14794</th>\n",
       "      <td>Yes, you can and the answer is One-Class Classification. A well-written resource to understand is this.\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0.614329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2408</th>\n",
       "      <td>Is there a technical name for image classification on amount instead of class? Is there a technical term for an image classifier that classifies on a single class but is classifying on an amount like how full a glass of water is rather than different classes?\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0.602599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 text  \\\n",
       "21808                                                                                                                                                                                                                                                                               Multiclass image classification - what approach to use and which models to consider? I'm working on an image classification project and I need to train a multiclass, multilabel classifier. The dataset is large and some of the images are mislabeled (for a given class, some labels are pretty easy to mix up). As an approach, I am using the following:\\n\\nConsidering the main available models, check their accuracy for the considered dataset at different learning rates. More in detail, my approach is to take a pretrained model, remove the last layer and fine-tune it on the dataset we're considering.\\nConsider the model and learning rate which gives the highest accuracy.\\nImprove the accuracy by grouping classes that are often confused. This is done by looking at the confusion matrix.\\n\\nDo you think this is a good approach to use? And which models should I consider in the first place? As of now I'm looking into the ResNet family (18, 34, 50, 101, 152 layers) and visual transformers.\\n   \n",
       "17628                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   If your data is well balanced but small, I would recommend using a simpler algorithm to classify your data.\\n   \n",
       "21681                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           What is the best approach/model for classifying document images with over 70+ classes of documents? What is the best approach/model for classifying document images with over 70+ classes of documents? I have tried LayoutLM which is a model that incorporated both NLP and Computer vision to classify but it's accuracy is not upto the mark. Are there any other models/methods I could try to come up with a better solution?\\n   \n",
       "17078                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Should one use an \"other\" category in image classification? In image classification, there are sometimes images that do not fit in any category.\\nFor example, if I build a CNN in Keras to classify Dogs and Cats, does it help (in terms of training time and performance) to create an \"other\" (or \"unclassified\") category in which images of houses, people, birds, etc., are classified? Is there any research paper that discusses this?\\nA similar question was asked before here, but, unfortunately, it has no answer.\\n   \n",
       "676                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Classification is the automatic categorization of a new observation. This classification is based on a model produced from a training set of data containing observations whose classifications are given. Classification is especially useful for problems involving categorical data.\\n   \n",
       "7964                                                                       Is there a Logistic Regression classifier that can perfectly classify the given data in this problem? I have the following problem.\\n\\nA bank wants to decide whether a customer can be given a loan,\\nbased on two features related to (i) the monthly salary of the customer, and (ii) his/her account balance. For simplicity, we model the two features with two binary variables $X1$, $X2$ and the class $Y$ (all of which can be either 0 or 1). $Y=1$ indicates that the customer can be given loan, and Y=0 indicates otherwise.\\nConsider the following dataset having four instances:\\n($X1 = 0$, $X2 = 0$, $Y = 0$)\\n($X1 = 0$, $X2 = 1$, $Y = 0$)\\n($X1 = 1$, $X2 = 0$, $Y = 0$)\\n($X1 = 1$, $X2 = 1$, $Y = 1$)\\nCan there be any logistic regression classifier using X1 and X2 as features, that can perfectly classify the given data?\\n\\nThe approach followed in the question was to calculate respective probabilities for Y=0 and Y=1 respectively.  The value of $p$ obtained was $0.25$ and $(1-p)$ as $0.75$. The $\\log(p/1-p)$ is coming as negative.\\nHowever, I don't understand what I need to do to understand whether there is a Logistic Regression classifier that can perfectly classify the given data.\\n   \n",
       "7263   CNN output generally has more than one category in one-hot categorization? I'm a bit of a CNN newbie, and I'm trying to train one to image classify pictures of pretty similar looking particles. I'm making the inputs and labels by hand from a set of 48x48 grayscale images, and labeling them with a one-hot vector based on their position in the sequence (for example, the 400/1000th image might have a one-hot in the 4th position if I have 10 categories in the run). I'm using sigmoidal output activation and categorical cross entropy loss. I've played around with a few different optimizers, as well. I'm implementing in python keras. \\nUnfortunately, although I have pretty good accuracy numbers for the training and validation, when I actually look at the outputs being produced, it generally gives multiple categories, which is not at all what I want. For example, if I have 6 categories and a label of 3, it might give the following probability vector:\\n[ .99 .98  1.0  .99  0.02  0.05 ]\\nIt was my understanding that categorical cross entropy would not allow this type of categorization, and yet it is prevalent in my code. I am under the impression that I'm doing something fundamentally wrong, but I cant figure out what. Any help would be appreciated. \\n   \n",
       "6362                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                How to use CNN for making predictions on non-image data? I have a dataset which I have loaded as a data frame in Python. It consists of 21392 rows (the data instances, each row is one sample) and 1972 columns (the features). The last column i.e. column 1972 has string type labels (14 different categories of target labels). I would like to use a CNN to classify the data in this case and predict the target labels using the available features. This is a somewhat unconventional approach though it seems possible. However, I am very confused on how the methodology should be as I could not find any sample code/ pseudo code guiding on using CNN for Classifying non-image data, either in Tensorflow or Keras. Any help in this regard will be highly appreciated. Cheers!\\n   \n",
       "14794                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Yes, you can and the answer is One-Class Classification. A well-written resource to understand is this.\\n   \n",
       "2408                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Is there a technical name for image classification on amount instead of class? Is there a technical term for an image classifier that classifies on a single class but is classifying on an amount like how full a glass of water is rather than different classes?\\n   \n",
       "\n",
       "       classification_flag  classification_score  \n",
       "21808                    0              0.808390  \n",
       "17628                    0              0.807748  \n",
       "21681                    0              0.708066  \n",
       "17078                    0              0.694076  \n",
       "676                      0              0.676271  \n",
       "7964                     0              0.648970  \n",
       "7263                     0              0.625950  \n",
       "6362                     0              0.619778  \n",
       "14794                    0              0.614329  \n",
       "2408                     0              0.602599  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "show_cols = ['text', flag_col, score_col]\n",
    "\n",
    "# Highest scoring examples that do not have the tag\n",
    "ai_posts[ai_posts[flag_col]==0].sort_values(score_col, ascending=False).head(10)[show_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84bcebbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>classification_flag</th>\n",
       "      <th>classification_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1644</th>\n",
       "      <td>Is there a measure of AI relative strength, modified by resources? For instance, Strength/Size$\\times$Speed, where size and speed refer to memory and processing.\\nWe now have very strong, narrow AI, but they tend to run on fast hardware without volume restrictions.\\nTo understand why I'm asking, this article on BBC may provide some insight: \"Which life form dominates Earth?\"  (If I was a betting man, I'd put money on tardigrades outlasting humans, and the secret of their success is that they require minimal resources and processing power, unlike higher-order automata.)\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>Do specific units exists for measuring the intelligence of a machine? We can measure the power of the machine with the number of operation per second or the frequency of the processor. But does units similar of IQ for humans exist for a AI?\\nI'm asking for a unit which can give countable result so something different from a Turing Test which only give a binary result.\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15065</th>\n",
       "      <td>How to use residual learning applied to fully connected networks? Is there any reason why skip connections would not provide the same benefits to fully connected layers as it does for convolutional?\\nI've read the ResNet paper and it says that the applications should extend to \"non-vision\" problems, so I decided to give it a try for a tabular data project I'm working on.\\nTry 1: My first try was to only skip connections when the input to a block matched the output size of the block (the block has depth - 1 number of layers with in_dim nodes plus a layer with out_dim nodes :\\nclass ResBlock(nn.Module):\\n    def __init__(self, depth, in_dim, out_dim, act='relu', act_first=True):\\n        super().__init__()\\n        self.residual = nn.Identity()\\n        self.block = block(depth, in_dim, out_dim, act)\\n        self.ada_pool = nn.AdaptiveAvgPool1d(out_dim)\\n        self.activate = get_act(act)\\n        self.apply_shortcut = (in_dim == out_dim)\\n        \\n    def forward(self, x):\\n        if self.apply_shortcut:\\n            residual = self.residual(x)\\n     \\n            x = self.block(x)\\n            return self.activate(x + residual)\\n        return self.activate(self.block(x))\\n\\nThe accompanying loss curve:\\n\\nTry 2: I thought to myself \"Great, it's doing something!\", so then I decided to reset and go for 30 epochs from scratch. I don't have the image saved, but this training only made it 5 epochs and then the training and validation loss curves exploded by several orders of magnitude:\\n\\nTry 3: Next, I decided to try to implement the paper's idea of reducing the input size to match the output when they don't match: y = F(x, {Wi}) + Mx. I chose average pooling in place of matrix M to accomplish this, and my loss curve became.\\n\\nThe only difference in my code is that I added average pooling so I could use shortcut connections when input and output sizes are different:\\nclass ResBlock(nn.Module):\\n    def __init__(self, depth, in_dim, out_dim, act='relu', act_first=True):\\n        super().__init__()\\n        self.residual = nn.Identity()\\n        self.block = block(depth, in_dim, out_dim, act)\\n        # squeeze/pad input  to output size\\n        self.ada_pool = nn.AdaptiveAvgPool1d(out_dim) \\n        self.activate = get_act(act)\\n        self.apply_pool = (in_dim != out_dim)\\n        \\n    def forward(self, x):\\n        # if in and out dims are different apply the padding/squeezing:\\n        if self.apply_pool:\\n            residual = self.ada_pool(self.residual(x).unsqueeze(0)).squeeze(0)\\n        else: residual = self.residual(x)\\n            \\n        x = self.block(x)\\n        return self.activate(x + residual)\\n)\\n\\nIs there a conceptual error in my application of residual learning? A bug in my code? Or is resididual learning just not applicable to this kind of data/network?\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11443</th>\n",
       "      <td>Language Learning feedback with AI Is there a program under development that uses AI technology, like Siri, to \"hold hands\" so to speak with a language learner and coach them on accent, colloqiual expressions, or to let them guide the language learning process using an archive of language knowledge? \\nAlso, could this sort of program be used to learn things in a language one already knows, or in a new language, say for the purposes of travel or to learn about related hyperlinks in an online database?\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18995</th>\n",
       "      <td>What should the value of $$ in the $w(n+1) = w(n) + \\rho*\\text{error}(i)x(i)$ formula of Least Mean Squares be? I am trying to better understand the Least Mean Squares algorithm, in order to implement it programmatically.\\nIf we consider its weight updating formula $$w(n+1) = w(n) + \\rho * \\text{error}(i)x(i),$$ where $w(n + 1)$ is the new weight of the classifier function, $w(n)$ is its current weight and $x(i)$ is the $i$th element of a training dataset, what should $\\rho$ be?\\nFrom what I have found online, $$ is supposed to be $0 &lt; \\rho &lt; \\frac{2}{trace(X^TX)}$, where $X$ is a matrix with all the training data the algorithm has processed at that point. One idea that I had, was to take $\\rho = \\frac{1}{trace(X^TX)} &lt; \\frac{2}{trace(X^TX)}$, but I do not know if that is correct. Also, one characteristic that this value has is that it changes with each iteration of the algorithm, as more samples are added to matrix $X$.\\nSo, what is a good value for $\\rho$? Should it change during the execution of the algorithm or should it stay the same?\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10081</th>\n",
       "      <td>What are causative and exploratory attacks in Adversarial Machine Learning? I've been researching Adversarial Machine Learning and I know that causative attacks are when an attacker manipulates training data. An exploratory attack is when the attacker wants to find out about the machine learning model. However, there is not a lot of information on how an attacker can manipulate only training data and not the test data set. \\nI have read about scenarios where the attacker performs an exploratory attack to find out about the ML model and then perform malicious input in order to tamper with the training data so that the model gives the wrong output. However shouldn't such input manipulation affect both the test and training data set? How does such tampering only affect the training data set and not the test data set?\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14175</th>\n",
       "      <td>Does Algorithmic Mechanism Design come under the field of AI? I see many papers in AAMAS talk about artificial intelligence and mechanism design simultaneously. I was wondering, for the sake of being pedantic, is mechanism design could be classified under AI.\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11857</th>\n",
       "      <td>Using AI to enhance customer service I'm trying to find out how AI can help with efficient customer service, in fact call routing to the right agent. My usecase is given context of a query from a customer and agents' expertise, how can we do the matching?\\nGenerally, how is this problem solved? What sub-topic within AI is suitable for this problems? Classification, recommender systems, ...? Any pointers to open-source projects would be very helpful.\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10756</th>\n",
       "      <td>How to use LSTM to generate a paragraph A LSTM model can be trained to generate text sequences by feeding the first word. After feeding the first word, the model will generate a sequence of words (a sentence). Feed the first word to get the second word, feed the first word + the second word to get the third word, and so on.\\nHowever, about the next sentence, what should be the next first word? The thing is to generate a paragraph of multiple sentences.\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20689</th>\n",
       "      <td>What kind of features does each node have as an input graph to a graph neural network? What kind of features does each node have as an input graph to a graph neural network? For example, we want to do image classification with GNN, what are the features of each pixel? Or if anyone could send me a link to implementing GNN on an example I would greatly appreciate it.\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    text  \\\n",
       "1644                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Is there a measure of AI relative strength, modified by resources? For instance, Strength/Size$\\times$Speed, where size and speed refer to memory and processing.\\nWe now have very strong, narrow AI, but they tend to run on fast hardware without volume restrictions.\\nTo understand why I'm asking, this article on BBC may provide some insight: \"Which life form dominates Earth?\"  (If I was a betting man, I'd put money on tardigrades outlasting humans, and the secret of their success is that they require minimal resources and processing power, unlike higher-order automata.)\\n   \n",
       "362                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Do specific units exists for measuring the intelligence of a machine? We can measure the power of the machine with the number of operation per second or the frequency of the processor. But does units similar of IQ for humans exist for a AI?\\nI'm asking for a unit which can give countable result so something different from a Turing Test which only give a binary result.\\n   \n",
       "15065  How to use residual learning applied to fully connected networks? Is there any reason why skip connections would not provide the same benefits to fully connected layers as it does for convolutional?\\nI've read the ResNet paper and it says that the applications should extend to \"non-vision\" problems, so I decided to give it a try for a tabular data project I'm working on.\\nTry 1: My first try was to only skip connections when the input to a block matched the output size of the block (the block has depth - 1 number of layers with in_dim nodes plus a layer with out_dim nodes :\\nclass ResBlock(nn.Module):\\n    def __init__(self, depth, in_dim, out_dim, act='relu', act_first=True):\\n        super().__init__()\\n        self.residual = nn.Identity()\\n        self.block = block(depth, in_dim, out_dim, act)\\n        self.ada_pool = nn.AdaptiveAvgPool1d(out_dim)\\n        self.activate = get_act(act)\\n        self.apply_shortcut = (in_dim == out_dim)\\n        \\n    def forward(self, x):\\n        if self.apply_shortcut:\\n            residual = self.residual(x)\\n     \\n            x = self.block(x)\\n            return self.activate(x + residual)\\n        return self.activate(self.block(x))\\n\\nThe accompanying loss curve:\\n\\nTry 2: I thought to myself \"Great, it's doing something!\", so then I decided to reset and go for 30 epochs from scratch. I don't have the image saved, but this training only made it 5 epochs and then the training and validation loss curves exploded by several orders of magnitude:\\n\\nTry 3: Next, I decided to try to implement the paper's idea of reducing the input size to match the output when they don't match: y = F(x, {Wi}) + Mx. I chose average pooling in place of matrix M to accomplish this, and my loss curve became.\\n\\nThe only difference in my code is that I added average pooling so I could use shortcut connections when input and output sizes are different:\\nclass ResBlock(nn.Module):\\n    def __init__(self, depth, in_dim, out_dim, act='relu', act_first=True):\\n        super().__init__()\\n        self.residual = nn.Identity()\\n        self.block = block(depth, in_dim, out_dim, act)\\n        # squeeze/pad input  to output size\\n        self.ada_pool = nn.AdaptiveAvgPool1d(out_dim) \\n        self.activate = get_act(act)\\n        self.apply_pool = (in_dim != out_dim)\\n        \\n    def forward(self, x):\\n        # if in and out dims are different apply the padding/squeezing:\\n        if self.apply_pool:\\n            residual = self.ada_pool(self.residual(x).unsqueeze(0)).squeeze(0)\\n        else: residual = self.residual(x)\\n            \\n        x = self.block(x)\\n        return self.activate(x + residual)\\n)\\n\\nIs there a conceptual error in my application of residual learning? A bug in my code? Or is resididual learning just not applicable to this kind of data/network?\\n   \n",
       "11443                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Language Learning feedback with AI Is there a program under development that uses AI technology, like Siri, to \"hold hands\" so to speak with a language learner and coach them on accent, colloqiual expressions, or to let them guide the language learning process using an archive of language knowledge? \\nAlso, could this sort of program be used to learn things in a language one already knows, or in a new language, say for the purposes of travel or to learn about related hyperlinks in an online database?\\n   \n",
       "18995                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                What should the value of $$ in the $w(n+1) = w(n) + \\rho*\\text{error}(i)x(i)$ formula of Least Mean Squares be? I am trying to better understand the Least Mean Squares algorithm, in order to implement it programmatically.\\nIf we consider its weight updating formula $$w(n+1) = w(n) + \\rho * \\text{error}(i)x(i),$$ where $w(n + 1)$ is the new weight of the classifier function, $w(n)$ is its current weight and $x(i)$ is the $i$th element of a training dataset, what should $\\rho$ be?\\nFrom what I have found online, $$ is supposed to be $0 < \\rho < \\frac{2}{trace(X^TX)}$, where $X$ is a matrix with all the training data the algorithm has processed at that point. One idea that I had, was to take $\\rho = \\frac{1}{trace(X^TX)} < \\frac{2}{trace(X^TX)}$, but I do not know if that is correct. Also, one characteristic that this value has is that it changes with each iteration of the algorithm, as more samples are added to matrix $X$.\\nSo, what is a good value for $\\rho$? Should it change during the execution of the algorithm or should it stay the same?\\n   \n",
       "10081                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        What are causative and exploratory attacks in Adversarial Machine Learning? I've been researching Adversarial Machine Learning and I know that causative attacks are when an attacker manipulates training data. An exploratory attack is when the attacker wants to find out about the machine learning model. However, there is not a lot of information on how an attacker can manipulate only training data and not the test data set. \\nI have read about scenarios where the attacker performs an exploratory attack to find out about the ML model and then perform malicious input in order to tamper with the training data so that the model gives the wrong output. However shouldn't such input manipulation affect both the test and training data set? How does such tampering only affect the training data set and not the test data set?\\n   \n",
       "14175                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Does Algorithmic Mechanism Design come under the field of AI? I see many papers in AAMAS talk about artificial intelligence and mechanism design simultaneously. I was wondering, for the sake of being pedantic, is mechanism design could be classified under AI.\\n   \n",
       "11857                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Using AI to enhance customer service I'm trying to find out how AI can help with efficient customer service, in fact call routing to the right agent. My usecase is given context of a query from a customer and agents' expertise, how can we do the matching?\\nGenerally, how is this problem solved? What sub-topic within AI is suitable for this problems? Classification, recommender systems, ...? Any pointers to open-source projects would be very helpful.\\n   \n",
       "10756                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         How to use LSTM to generate a paragraph A LSTM model can be trained to generate text sequences by feeding the first word. After feeding the first word, the model will generate a sequence of words (a sentence). Feed the first word to get the second word, feed the first word + the second word to get the third word, and so on.\\nHowever, about the next sentence, what should be the next first word? The thing is to generate a paragraph of multiple sentences.\\n   \n",
       "20689                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  What kind of features does each node have as an input graph to a graph neural network? What kind of features does each node have as an input graph to a graph neural network? For example, we want to do image classification with GNN, what are the features of each pixel? Or if anyone could send me a link to implementing GNN on an example I would greatly appreciate it.\\n   \n",
       "\n",
       "       classification_flag  classification_score  \n",
       "1644                     1              0.000630  \n",
       "362                      1              0.003532  \n",
       "15065                    1              0.005505  \n",
       "11443                    1              0.005824  \n",
       "18995                    1              0.006885  \n",
       "10081                    1              0.007989  \n",
       "14175                    1              0.009132  \n",
       "11857                    1              0.009539  \n",
       "10756                    1              0.010353  \n",
       "20689                    1              0.011733  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lowest scoring examples that do have the tag\n",
    "ai_posts[ai_posts[flag_col]==1].sort_values(score_col, ascending=True).head(10)[show_cols]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch] *",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
