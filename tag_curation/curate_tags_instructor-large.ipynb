{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a80d3834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_DIR = \"D:/ml_data/stackexchange/\"\n",
    "DATA_DIR = \"C:/Users/marinch/Source/repos/sentence-embedding-demos/tag_curation/ml_data/stackexchange\"\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89157077",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'ai_Posts.csv'\n",
    "\n",
    "ai_posts = pd.read_csv(csv_file)\n",
    "\n",
    "def combine_title_and_body(title, body):\n",
    "    # I'm having trouble getting over the fact that a missing string has a numeric type.\n",
    "    if title==title: # not NaN\n",
    "        text = title + ' ' + body\n",
    "    elif body == body:\n",
    "        text = body\n",
    "    else:\n",
    "        text = ''\n",
    "    return str(text)\n",
    "\n",
    "ai_posts['text'] = [ combine_title_and_body(row.title, row.body) for idx, row in ai_posts.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c8b0b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marinch\\Miniconda3\\envs\\mladsjune2023cuda117\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)66f5b/.gitattributes: 100%|██████████| 1.48k/1.48k [00:00<00:00, 296kB/s]\n",
      "Downloading (…)_Pooling/config.json: 100%|██████████| 270/270 [00:00<00:00, 54.1kB/s]\n",
      "Downloading (…)/2_Dense/config.json: 100%|██████████| 116/116 [00:00<00:00, 23.2kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 3.15M/3.15M [00:00<00:00, 16.6MB/s]\n",
      "Downloading (…)f46b666f5b/README.md: 100%|██████████| 66.3k/66.3k [00:00<00:00, 211kB/s]\n",
      "Downloading (…)6b666f5b/config.json: 100%|██████████| 1.53k/1.53k [00:00<00:00, 510kB/s]\n",
      "Downloading (…)ce_transformers.json: 100%|██████████| 122/122 [00:00<00:00, 40.5kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 1.34G/1.34G [03:44<00:00, 5.96MB/s]\n",
      "Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 6.49kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 370kB/s]\n",
      "Downloading spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 8.77MB/s]\n",
      "Downloading (…)66f5b/tokenizer.json: 100%|██████████| 2.42M/2.42M [00:00<00:00, 3.95MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 2.41k/2.41k [00:00<00:00, 602kB/s]\n",
      "Downloading (…)b666f5b/modules.json: 100%|██████████| 461/461 [00:00<00:00, 92.0kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "[[-6.15552627e-02  1.04199881e-02  5.88439591e-03  1.93768740e-02\n",
      "   5.71417958e-02  2.57655699e-02 -4.01915095e-05 -2.80044414e-02\n",
      "  -2.92965472e-02  4.91884910e-02  6.78200424e-02  2.18692347e-02\n",
      "   4.54528630e-02  1.50187118e-02 -4.84451987e-02 -3.25259827e-02\n",
      "  -3.56492773e-02  1.19935377e-02 -6.83915662e-03  3.03126238e-02\n",
      "   5.17491773e-02  3.48140411e-02  4.91033541e-03  6.68928623e-02\n",
      "   1.52824344e-02  3.54217030e-02  1.07743666e-02  6.89828917e-02\n",
      "   4.44019549e-02 -3.23419459e-02  1.24267926e-02 -2.15528030e-02\n",
      "  -1.62690841e-02 -4.15058322e-02 -2.42290879e-03 -3.07157799e-03\n",
      "   4.27047350e-02  1.56428497e-02  2.57813148e-02  5.92843294e-02\n",
      "  -1.99174136e-02  1.32361846e-02  1.08408080e-02 -4.00610566e-02\n",
      "  -1.36213598e-03 -1.57032683e-02 -2.53812186e-02 -1.31972935e-02\n",
      "  -7.83779938e-03 -1.14008989e-02 -4.82025407e-02 -2.58416254e-02\n",
      "  -4.98769991e-03  4.98239584e-02  1.19490102e-02 -5.55060469e-02\n",
      "  -2.82120276e-02 -3.32208797e-02  2.46765055e-02 -5.66114597e-02\n",
      "  -5.12201013e-03  1.95142906e-02 -2.12629866e-02  1.92354005e-02\n",
      "   2.46065054e-02 -4.58347723e-02  3.27664353e-02 -3.99055667e-02\n",
      "   5.31269424e-02  9.05539200e-04  4.53844890e-02 -2.51501296e-02\n",
      "   1.74822938e-03 -9.64769423e-02 -9.51785967e-03 -6.47392171e-03\n",
      "   3.51561457e-02  3.58432494e-02 -5.11278398e-02  4.30903137e-02\n",
      "   4.58191633e-02  1.91871654e-02  2.38421634e-02 -1.71816368e-02\n",
      "  -1.52623234e-02  5.40182479e-02 -5.58873974e-02  4.29563038e-02\n",
      "   8.48113745e-03  7.83621147e-03 -3.27342786e-02 -1.08465459e-02\n",
      "  -7.19641009e-03 -4.37383205e-02 -1.88113656e-02  5.16907461e-02\n",
      "   4.62869219e-02 -2.63639912e-02  3.73640954e-02  1.84657946e-02\n",
      "   5.99115640e-02  1.80126619e-04 -2.35873684e-02  5.71749359e-02\n",
      "   1.20532941e-02 -3.81674655e-02 -3.55241075e-02  2.34814477e-03\n",
      "  -4.45778072e-02  9.34026018e-03  5.85194491e-03 -3.56189236e-02\n",
      "  -2.23838426e-02 -1.38210494e-03  8.74637719e-03  2.08802447e-02\n",
      "   7.03728944e-02 -4.39636931e-02 -4.53046598e-02 -4.76960614e-02\n",
      "   4.33718599e-02 -1.97182293e-03 -5.65528078e-03 -2.16748081e-02\n",
      "  -7.46926367e-02  1.90407690e-02 -2.33457386e-02 -5.68974577e-02\n",
      "  -9.49267671e-03  4.25820565e-03  3.14501976e-03  1.90789755e-02\n",
      "  -1.00613944e-02 -6.33771271e-02  4.90879118e-02  2.97248457e-03\n",
      "  -7.01222792e-02  1.71163045e-02  1.05466843e-02  8.59851763e-02\n",
      "  -5.78762256e-02 -3.88501212e-02  4.20247437e-03 -1.92795359e-02\n",
      "  -4.11052927e-02  7.98567291e-03  4.75644246e-02 -4.87977639e-02\n",
      "  -3.62159945e-02 -2.10572537e-02  4.02226783e-02 -4.74730358e-02\n",
      "  -2.78858691e-02  8.39250907e-02 -9.76029132e-03  2.62570456e-02\n",
      "  -5.60530722e-02  1.52837196e-02  1.54583307e-03  2.02960195e-03\n",
      "  -3.28001268e-02  5.76916002e-02 -7.33235553e-02 -4.00819927e-02\n",
      "  -3.98107879e-02 -3.84523757e-02 -8.67153797e-03  1.05411708e-01\n",
      "  -2.86331028e-03 -1.91161372e-02 -5.60036562e-02  9.67337191e-03\n",
      "   5.51291108e-02  2.56364257e-03 -2.94723455e-02  5.84518351e-02\n",
      "   5.15934192e-02 -1.61305384e-03 -2.19461881e-02  5.65167665e-02\n",
      "   4.74953204e-02 -2.44090594e-02 -2.66009010e-02 -5.86747285e-03\n",
      "   2.24451330e-02 -2.23603239e-03  4.56710998e-03  3.27842459e-02\n",
      "   5.26622497e-03 -2.01674439e-02 -2.33968012e-02  4.43987325e-02\n",
      "  -1.51708331e-02  7.38916174e-03  2.71087177e-02 -2.46057939e-02\n",
      "  -1.87857132e-02 -5.61456138e-04 -3.28655578e-02 -1.21782273e-02\n",
      "   1.79727189e-03 -1.50850797e-02  2.52194237e-02  1.25257764e-02\n",
      "  -2.65356561e-04  1.23138232e-02 -6.45002630e-03  1.02272689e-01\n",
      "  -2.98037753e-02  5.94182536e-02 -2.78096367e-03 -3.49573642e-02\n",
      "   3.06671690e-02  5.42211086e-02  5.95246218e-02  4.14741263e-02\n",
      "  -4.06689337e-03 -3.94712500e-02  1.96131412e-02  5.96131310e-02\n",
      "   4.44265679e-02  4.40844037e-02 -5.12231514e-02 -3.00020445e-02\n",
      "   3.01150102e-02  2.40174048e-02 -3.39305848e-02 -1.70433987e-02\n",
      "   8.32552277e-03  2.66083349e-02  7.67714251e-03  1.76458564e-02\n",
      "  -2.06325320e-03  1.77012980e-02 -6.08421639e-02 -7.96776786e-02\n",
      "   4.99934405e-02  2.96638496e-02 -4.47008200e-03  1.65794306e-02\n",
      "  -2.35370528e-02 -3.23977089e-03  2.61382610e-02 -1.34953381e-02\n",
      "  -1.60201788e-02 -1.08793825e-02 -1.77004952e-02 -6.53112866e-03\n",
      "   6.91720173e-02 -4.63659801e-02  4.15586568e-02  1.24583654e-02\n",
      "  -1.88734397e-04  2.47693472e-02 -3.62277217e-02  5.47523797e-02\n",
      "   1.54009983e-01  6.00455841e-03 -2.70665437e-02  4.70894873e-02\n",
      "   4.09195609e-02  4.31693904e-02  6.22434504e-02 -2.51828562e-02\n",
      "   6.71826899e-02  1.89108979e-02  3.67507823e-02  7.62735680e-02\n",
      "   5.01052302e-04 -7.33284187e-03  1.95556246e-02  8.43793601e-02\n",
      "   1.24929156e-02 -2.75657908e-03  4.97817174e-02 -1.73069742e-02\n",
      "   2.77005285e-02 -2.63486356e-02 -2.21686866e-02  3.95562639e-03\n",
      "  -9.68611799e-03  3.96470763e-02 -8.72508064e-03 -1.07546300e-02\n",
      "  -2.70988774e-02 -1.17305405e-02 -1.16984174e-02  4.52318527e-02\n",
      "  -9.12858453e-03 -1.14591690e-02  8.29536002e-03 -3.94435264e-02\n",
      "   8.80731829e-03 -3.67274582e-02 -4.45834696e-02 -2.38478296e-02\n",
      "   1.73519496e-02  2.46788189e-02 -9.24503282e-02  3.40846740e-03\n",
      "  -8.58144239e-02 -1.69283859e-02  8.74706451e-03 -2.66723079e-03\n",
      "  -3.10086343e-03 -6.62742853e-02  1.74709763e-02 -6.20296896e-02\n",
      "  -7.71831945e-02 -4.30789962e-02 -6.97873011e-02 -2.76594162e-02\n",
      "  -7.36039728e-02  2.61303894e-02  4.94785607e-02  1.88994557e-02\n",
      "   2.05077492e-02  5.93992462e-03 -2.71200538e-02 -4.64439690e-02\n",
      "   2.66322978e-02  2.63824463e-02  3.03616491e-03 -4.70094830e-02\n",
      "  -8.68524332e-03 -1.94979680e-03 -1.47214429e-02 -3.10322605e-02\n",
      "  -3.54933105e-02  7.64071643e-02  9.24097970e-02  1.11720525e-02\n",
      "   6.86150463e-03  2.67613661e-02 -4.66881394e-02 -4.80800979e-02\n",
      "  -1.76523402e-02 -5.05446680e-02 -2.54300497e-02 -2.59506721e-02\n",
      "  -2.86576152e-02 -3.34676616e-02 -3.07256430e-02  6.79465570e-03\n",
      "  -5.43393195e-02 -2.23255553e-03  1.03655094e-02  3.52348536e-02\n",
      "   2.40201596e-02 -2.09923997e-03 -8.59064907e-02 -4.86475937e-02\n",
      "   3.41627039e-02  9.51630808e-03  2.42882688e-03 -6.15580827e-02\n",
      "  -2.23672763e-02  1.49234375e-02 -6.16902905e-03 -2.94565596e-02\n",
      "  -8.48871469e-03  3.98517512e-02  3.54111344e-02 -1.31471381e-02\n",
      "  -2.31656004e-02 -2.86290608e-02  1.44813051e-02 -3.19011253e-03\n",
      "   5.59896231e-03 -6.02383465e-02 -5.41782901e-02  6.31063012e-03\n",
      "  -3.27197160e-03  6.00864552e-02 -4.93385345e-02 -1.23744896e-02\n",
      "  -2.60731578e-02  3.88635360e-02  3.19503136e-02  2.37053186e-02\n",
      "  -2.05829665e-02  1.42387617e-02 -3.58667299e-02 -3.98508683e-02\n",
      "   8.55066720e-03 -2.32857838e-02  1.41011262e-02  5.81302755e-02\n",
      "  -4.17654170e-03  7.78259197e-03  8.50560665e-02  2.76554618e-02\n",
      "   4.23116349e-02  2.45192815e-02 -2.62568872e-02  3.76733541e-02\n",
      "  -1.03408806e-02  2.60650273e-02  6.19981717e-03 -1.73711013e-02\n",
      "  -5.55875339e-02 -1.02811217e-01 -8.27027299e-03 -6.74284995e-03\n",
      "  -5.95137365e-02  1.29434895e-02  4.41100933e-02 -7.02070165e-03\n",
      "  -3.03075407e-02 -9.03240405e-03  2.10526530e-02  2.01296937e-02\n",
      "  -3.11780698e-03  4.94987629e-02 -2.36510076e-02  2.80551594e-02\n",
      "  -2.48605236e-02  5.25815552e-03 -5.47549613e-02 -1.80020723e-02\n",
      "  -6.72236551e-03  7.68097416e-02  2.41172425e-02  6.28411695e-02\n",
      "   4.77913171e-02 -1.15464833e-02 -4.14417237e-02  2.10504942e-02\n",
      "   6.09488599e-02 -2.36857999e-02 -3.18970941e-02  2.34900787e-03\n",
      "  -2.75846594e-03  1.48617651e-03 -4.22429293e-03  5.57198795e-03\n",
      "   2.00943612e-02  5.29720820e-02 -3.99871245e-02 -1.41997440e-02\n",
      "   3.94999646e-02 -1.47230811e-02 -4.10685502e-03 -6.41633719e-02\n",
      "  -2.31138282e-02  1.63526461e-03  6.87346468e-03  5.51297739e-02\n",
      "   1.13907391e-02  3.55854519e-02  5.87924346e-02  2.42435914e-02\n",
      "  -3.97644006e-02 -7.16552064e-02  4.69529592e-02 -3.05531686e-03\n",
      "  -4.91016470e-02 -9.50928405e-02 -1.41103957e-02  2.90551092e-02\n",
      "   2.07553729e-02 -2.56225723e-03 -2.63764914e-02 -5.93052991e-03\n",
      "   6.81198090e-02 -2.53772512e-02  6.08022884e-02  4.24165539e-02\n",
      "   4.66698818e-02  3.79461460e-02 -1.22388741e-02  6.11324497e-02\n",
      "  -1.82264987e-02 -8.81061610e-03  2.42136996e-02  2.62034480e-02\n",
      "  -1.55039011e-02 -2.20747236e-02 -5.16003110e-02  2.53373366e-02\n",
      "   3.05230375e-02  1.20210359e-02  8.25990066e-02 -2.68187206e-02\n",
      "  -3.36164124e-02 -3.96278389e-02  2.64574699e-02 -4.73223440e-02\n",
      "   5.45928255e-02  4.71893363e-02  5.40369935e-02 -3.63412052e-02\n",
      "  -4.38812040e-02 -9.25775338e-03 -1.49381887e-02  1.94572546e-02\n",
      "  -4.68943045e-02 -2.96848733e-02 -6.92514852e-02  2.51878574e-02\n",
      "  -1.31793991e-02 -3.26385386e-02 -8.38335156e-02  1.62501372e-02\n",
      "   5.05867740e-03 -3.85647789e-02  4.18353267e-02  4.50653546e-02\n",
      "   4.53344658e-02  3.85494716e-02  5.27763106e-02  9.01091099e-03\n",
      "  -2.32415255e-02  4.14123423e-02 -3.90885323e-02 -1.84995346e-02\n",
      "  -2.91617308e-02 -6.02056868e-02 -3.62731032e-02  4.92624613e-03\n",
      "  -1.51348021e-02 -1.77912526e-02 -6.56068465e-03  3.74852903e-02\n",
      "  -4.98752668e-03  3.45563330e-02  8.38177092e-03  1.23971244e-02\n",
      "   1.30274538e-02 -5.76015487e-02 -1.41846370e-02 -3.29240188e-02\n",
      "  -6.02640584e-02 -4.08707224e-02  6.09732643e-02 -5.65142371e-03\n",
      "  -2.64281519e-02  1.45490067e-02  1.39951427e-02  2.01470442e-02\n",
      "   1.63884014e-02 -4.30176072e-02  8.81801080e-03  9.79693327e-03\n",
      "  -4.37083431e-02 -1.07098566e-02 -2.09241882e-02 -1.68447625e-02\n",
      "   2.54024211e-02 -4.39964309e-02  2.77971923e-02  2.39688493e-02\n",
      "   4.46381094e-03 -4.09839675e-02  1.39753716e-02 -1.02954293e-02\n",
      "  -4.48161773e-02  1.04085496e-02 -2.32339539e-02  8.22256505e-03\n",
      "   1.08464053e-02 -7.12043047e-03 -2.48804055e-02  1.47036705e-02\n",
      "  -1.03130238e-02  5.29496670e-02  2.34216247e-02 -3.16518210e-02\n",
      "   2.24910695e-02 -1.01565346e-02  2.24805474e-02 -6.64025098e-02\n",
      "   2.63604447e-02 -2.33393181e-02  2.29447167e-02 -1.88058559e-02\n",
      "  -2.10312800e-03 -4.88403216e-02  4.41654623e-02 -2.42530350e-02\n",
      "  -3.33837792e-02  6.30347664e-03  1.08947628e-03  1.65918143e-03\n",
      "   1.43814608e-02 -6.16016658e-03  2.33820453e-02 -6.41303658e-02\n",
      "   2.14748476e-02  1.68789141e-02 -1.88098755e-02 -1.45088276e-02\n",
      "   4.35655639e-02 -3.56806554e-02 -1.71170812e-02  4.00117692e-03\n",
      "  -1.24642057e-02  3.74952182e-02  3.54862623e-02  2.71978811e-03\n",
      "   4.88897301e-02 -1.42481476e-02 -2.37889737e-02  1.45645291e-02\n",
      "  -5.29264659e-02 -3.16047519e-02 -2.55868044e-02  6.24940381e-04\n",
      "   1.23044690e-02  1.52396671e-02  5.92731452e-03 -6.96792752e-02\n",
      "  -4.38257158e-02  3.32457684e-02  4.29933295e-02  3.41573469e-02\n",
      "   5.74664678e-03  6.92842947e-03  2.19891705e-02  5.40519953e-02\n",
      "  -3.47650871e-02 -6.38605095e-03 -1.06168529e-02  5.59756532e-03\n",
      "   2.51517314e-02  1.97776407e-03 -9.76094231e-03  1.29118143e-02\n",
      "  -5.10915853e-02 -4.22592647e-02  6.32157102e-02  6.68454096e-02\n",
      "   3.72742787e-02 -1.31203625e-02 -3.29280123e-02  3.23109031e-02\n",
      "   2.64141168e-02 -4.51177247e-02  6.29258007e-02 -3.71047505e-03\n",
      "  -3.95429432e-02  5.86496964e-02 -5.98640740e-03  1.91448089e-02\n",
      "   3.20215225e-02  5.33388481e-02 -2.62014307e-02  2.29458027e-02\n",
      "  -1.26508772e-02  6.65134285e-03  6.07208051e-02 -3.29924449e-02\n",
      "  -2.04965342e-02 -5.14310263e-02  6.54849932e-02 -4.22492325e-02\n",
      "   9.26519334e-02  1.99730285e-02 -1.83647815e-02  1.88230863e-03\n",
      "  -4.16838527e-02 -6.10365607e-02  2.76884958e-02 -3.12236175e-02\n",
      "   5.57781458e-02 -3.40827890e-02 -5.36403693e-02  3.83231491e-02\n",
      "   1.50124589e-02 -6.74923360e-02  6.30307794e-02  1.23500992e-02\n",
      "   5.98304607e-02  2.07509492e-02  3.15652192e-02 -3.75223532e-02\n",
      "   3.68293859e-02 -6.04589507e-02  9.98122897e-03 -3.74645405e-02\n",
      "   4.14431049e-03  2.01168600e-02 -1.38435857e-02 -6.81752339e-03\n",
      "   3.87795619e-03 -8.58293008e-03 -1.53929344e-03  4.07204032e-02\n",
      "  -5.60819507e-02 -6.66264966e-02  2.24500261e-02  1.80751905e-02\n",
      "  -1.88874104e-03  1.47162611e-02 -2.16904879e-02  8.98237620e-03\n",
      "   3.33474614e-02 -1.17769474e-02 -2.53784508e-02  7.49978051e-03\n",
      "   1.59923304e-02 -5.08552343e-02 -2.55495664e-02  3.99981104e-02\n",
      "   2.41953181e-03 -1.39974123e-02  1.12951584e-02  1.01209385e-03\n",
      "  -1.35120150e-04  1.50756724e-02  3.66247469e-03  3.56586166e-02\n",
      "  -2.39739679e-02 -5.98191982e-03 -1.14465049e-02  7.17897946e-03\n",
      "  -7.58272875e-03 -1.56441219e-02  2.08631940e-02  4.67960946e-02\n",
      "   9.20427497e-03  9.97737516e-03 -1.69361960e-02 -3.79866771e-02\n",
      "   2.12063938e-02 -3.93209755e-02  2.39590071e-02  4.52188728e-03\n",
      "  -4.90422100e-02 -2.53686775e-02 -5.13280481e-02 -3.11175715e-02\n",
      "   3.69731300e-02 -3.32236551e-02  2.64319647e-02  3.13280337e-02\n",
      "   5.02047986e-02 -3.51830348e-02 -9.47780162e-02 -3.81823741e-02\n",
      "  -2.52813064e-02  8.34161788e-03  1.06830802e-02 -2.85213199e-02\n",
      "   1.14974352e-02 -1.63908321e-02 -5.35691455e-02  1.44921513e-02\n",
      "   1.44734317e-02  1.46977501e-02  3.46375965e-02  4.89880741e-02\n",
      "  -2.99605671e-02  7.35144038e-03  1.49103869e-02 -2.81756017e-02\n",
      "   4.02826779e-02  1.23249050e-02  2.03392226e-02  4.75965440e-02\n",
      "   4.34238538e-02  8.02048016e-03 -1.76124671e-03 -6.28188848e-02\n",
      "  -4.67858799e-02 -3.76614109e-02  1.02270283e-02  4.39474136e-02]]\n"
     ]
    }
   ],
   "source": [
    "from InstructorEmbedding import INSTRUCTOR\n",
    "model = INSTRUCTOR('hkunlp/instructor-large')\n",
    "sentence = \"3D ActionSLAM: wearable person tracking in multi-floor environments\"\n",
    "instruction = \"Represent the Science title:\"\n",
    "embeddings = model.encode([[instruction,sentence]])\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46628214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.81402332e-02  7.20523810e-03  3.80407553e-03  2.02455875e-02\n",
      "   5.94068356e-02  2.72778720e-02  6.19684299e-03 -2.05716472e-02\n",
      "  -2.82432772e-02  4.79073972e-02  6.48194477e-02  2.16513593e-02\n",
      "   3.92502211e-02  1.52985528e-02 -4.94726710e-02 -3.80594581e-02\n",
      "  -3.35397832e-02  6.98450999e-03 -3.59800365e-03  3.02730370e-02\n",
      "   5.00123277e-02  3.51080820e-02  7.43056182e-03  6.42502084e-02\n",
      "   1.81745104e-02  3.68924849e-02  3.47956107e-03  7.42488801e-02\n",
      "   4.09231633e-02 -3.48232128e-02  7.89864163e-04 -2.35165656e-02\n",
      "  -1.73166264e-02 -3.85985561e-02  6.08439231e-03 -1.34084066e-02\n",
      "   4.17104699e-02  2.39232164e-02  2.01099534e-02  6.13473356e-02\n",
      "  -2.37492174e-02  1.38548920e-02  1.28523670e-02 -4.10010330e-02\n",
      "   1.93893327e-03 -2.00247765e-02 -2.65862010e-02 -5.64958202e-03\n",
      "  -1.22418152e-02 -6.95753051e-03 -4.64767553e-02 -2.35219914e-02\n",
      "  -1.75058830e-03  4.74112928e-02  1.43560255e-02 -5.33968769e-02\n",
      "  -2.49953922e-02 -3.41698043e-02  2.12819446e-02 -5.54798618e-02\n",
      "  -6.07642764e-03  1.67022701e-02 -2.37789191e-02  1.95305068e-02\n",
      "   2.65425351e-02 -4.15356755e-02  3.30321640e-02 -4.78067510e-02\n",
      "   5.54973446e-02  3.44021432e-03  4.82149944e-02 -3.01246643e-02\n",
      "   2.29005993e-04 -9.55153108e-02 -8.56260024e-03 -1.06888637e-02\n",
      "   3.37271914e-02  3.76317017e-02 -4.87352386e-02  4.64003161e-02\n",
      "   4.45234329e-02  2.16162726e-02  2.64507569e-02 -1.98821239e-02\n",
      "  -1.37477722e-02  5.09353764e-02 -5.43045327e-02  4.21525165e-02\n",
      "   4.13359841e-03  7.67170545e-03 -2.93952208e-02 -4.36705559e-05\n",
      "   9.34712181e-04 -3.97856794e-02 -2.56489720e-02  5.00871390e-02\n",
      "   4.02405746e-02 -2.30130907e-02  3.01412661e-02  1.97012983e-02\n",
      "   5.51150776e-02  3.51512269e-03 -2.43372284e-02  4.82217558e-02\n",
      "   8.34128726e-03 -3.41509953e-02 -3.22333872e-02 -1.41526936e-04\n",
      "  -4.32548113e-02  7.35097704e-03  6.01404626e-03 -3.80346365e-02\n",
      "  -2.90167872e-02 -2.40759109e-03  1.14211598e-02  1.95779894e-02\n",
      "   7.13054538e-02 -4.59168218e-02 -5.08887209e-02 -4.58761789e-02\n",
      "   4.56900597e-02 -1.89685423e-04 -1.17221158e-02 -2.50433758e-02\n",
      "  -6.51849061e-02  1.40970480e-02 -2.75975112e-02 -5.56901060e-02\n",
      "  -1.25312796e-02  8.97073466e-03  4.09809919e-03  2.54247412e-02\n",
      "  -1.67183653e-02 -6.17675781e-02  4.95109223e-02 -3.65224230e-04\n",
      "  -7.70418495e-02  1.64221954e-02  6.78875204e-03  8.49314928e-02\n",
      "  -5.19718938e-02 -4.50546816e-02  2.96923192e-03 -1.81028228e-02\n",
      "  -3.35707143e-02  1.12229297e-02  4.55725193e-02 -5.51876649e-02\n",
      "  -3.72957028e-02 -1.87343415e-02  4.97495644e-02 -4.60859127e-02\n",
      "  -2.64896564e-02  8.49296376e-02 -9.85083729e-03  2.68857181e-02\n",
      "  -5.25793806e-02  1.81802604e-02  4.75154817e-03  3.66671430e-03\n",
      "  -3.15230154e-02  5.88801317e-02 -7.38612041e-02 -3.86233665e-02\n",
      "  -4.03334089e-02 -3.70682813e-02 -9.21308994e-03  1.11813776e-01\n",
      "  -1.49372686e-03 -1.67265367e-02 -5.30557334e-02  1.57684926e-02\n",
      "   4.75085787e-02 -4.24036197e-03 -2.96728332e-02  5.85847832e-02\n",
      "   4.53141369e-02 -1.06532732e-03 -2.79366206e-02  5.61075173e-02\n",
      "   4.83874679e-02 -2.55739279e-02 -2.17906088e-02 -4.01845155e-03\n",
      "   1.89414788e-02 -1.36130338e-03  4.23292769e-03  3.85383479e-02\n",
      "   6.70790439e-03 -2.56895088e-02 -2.86407974e-02  4.43498902e-02\n",
      "  -1.25997383e-02  5.73127065e-03  2.57211346e-02 -2.79322490e-02\n",
      "  -1.74154676e-02  2.91830697e-03 -3.32391039e-02 -1.28956717e-02\n",
      "  -3.66610195e-03 -1.81318242e-02  3.31710391e-02  1.51320053e-02\n",
      "   3.57648567e-03  1.24177486e-02 -1.49345044e-02  9.66002569e-02\n",
      "  -3.22209783e-02  4.97511588e-02 -2.52760132e-03 -3.67380492e-02\n",
      "   3.37658003e-02  5.03016859e-02  5.76885231e-02  4.08502221e-02\n",
      "  -1.36448792e-03 -4.03612293e-02  2.00654529e-02  5.60739152e-02\n",
      "   4.13760506e-02  4.53548171e-02 -4.98984866e-02 -3.43325138e-02\n",
      "   2.58928724e-02  2.81461887e-02 -3.22165862e-02 -2.15296280e-02\n",
      "   1.24437809e-02  4.03814949e-02  7.69688236e-03  1.76413041e-02\n",
      "   1.61953922e-03  2.10286900e-02 -6.10832125e-02 -8.38182718e-02\n",
      "   4.67847660e-02  2.97211539e-02 -1.02345794e-02  1.73082482e-02\n",
      "  -2.44350024e-02  7.20950263e-03  2.44712941e-02 -9.88743268e-03\n",
      "  -1.66144688e-02 -6.96781371e-03 -1.10061970e-02 -2.66300957e-03\n",
      "   6.85315356e-02 -4.46326025e-02  4.08182256e-02  1.72667007e-03\n",
      "   7.75744906e-04  2.37107538e-02 -3.24830487e-02  5.51195852e-02\n",
      "   1.55880392e-01  1.01398900e-02 -2.89237779e-02  4.73661013e-02\n",
      "   3.71661410e-02  3.52352336e-02  6.63867518e-02 -1.89321078e-02\n",
      "   6.31483421e-02  1.22742914e-02  4.49528508e-02  7.62770772e-02\n",
      "   3.35163833e-03 -4.53638146e-03  2.00120714e-02  8.40209350e-02\n",
      "   1.64782386e-02 -2.20945035e-03  5.11370450e-02 -1.69415120e-02\n",
      "   3.03049386e-02 -2.91449316e-02 -1.84178855e-02  5.41154714e-03\n",
      "  -1.28036859e-02  4.96593304e-02 -8.66591930e-03 -5.52198477e-03\n",
      "  -2.67602541e-02 -1.75919030e-02 -6.91272831e-03  4.02601883e-02\n",
      "  -1.23004159e-02 -1.57238580e-02  7.32182479e-03 -3.66385505e-02\n",
      "   2.47897487e-03 -3.75923738e-02 -4.08755690e-02 -2.29976475e-02\n",
      "   2.33619455e-02  2.27248743e-02 -9.12425295e-02  7.91308470e-04\n",
      "  -8.17713886e-02 -1.52736884e-02  8.99446011e-03 -2.33859988e-03\n",
      "  -3.87796969e-03 -6.17522411e-02  1.82383414e-02 -5.87400347e-02\n",
      "  -7.15585575e-02 -4.25773561e-02 -7.56738707e-02 -2.99817752e-02\n",
      "  -7.11908266e-02  3.14758420e-02  4.34353054e-02  1.87707562e-02\n",
      "   2.63888370e-02  3.71301221e-03 -3.09655406e-02 -4.34107594e-02\n",
      "   2.43492406e-02  2.48159803e-02  3.62607697e-03 -5.28067052e-02\n",
      "  -1.17632784e-02 -3.24192690e-03 -2.49465648e-02 -2.58103590e-02\n",
      "  -3.28895152e-02  7.66943842e-02  8.35492983e-02  1.08788125e-02\n",
      "   1.69822667e-02  3.13521288e-02 -5.23583218e-02 -4.84965667e-02\n",
      "  -2.24161875e-02 -4.89824079e-02 -2.76790000e-02 -2.63848398e-02\n",
      "  -3.00475881e-02 -2.78080478e-02 -3.58688012e-02  2.01239693e-03\n",
      "  -5.66730425e-02 -3.49071994e-03  1.67902198e-03  3.96161601e-02\n",
      "   2.20848992e-02 -2.39172135e-03 -8.99095386e-02 -5.57843633e-02\n",
      "   3.87659781e-02  3.09822452e-03 -2.42214440e-03 -6.38679564e-02\n",
      "  -2.07798686e-02  1.20077319e-02  4.62849013e-04 -2.37781312e-02\n",
      "  -1.48846023e-02  4.01342362e-02  3.25797200e-02 -1.52197229e-02\n",
      "  -2.67280843e-02 -2.34635267e-02  1.78911015e-02 -4.49425355e-03\n",
      "   6.56281365e-03 -6.39528111e-02 -5.30785955e-02  6.37568021e-03\n",
      "  -1.83309056e-03  5.94219677e-02 -4.39299196e-02 -1.52698262e-02\n",
      "  -2.07197852e-02  3.49442102e-02  4.18822132e-02  2.96688359e-02\n",
      "  -2.60509364e-02  1.24986079e-02 -4.05342467e-02 -4.60795164e-02\n",
      "   1.07465880e-02 -1.66205317e-02  1.70899648e-02  6.03133775e-02\n",
      "  -1.27632348e-02  9.93527099e-03  8.31994638e-02  2.47008540e-02\n",
      "   4.57043871e-02  2.85577904e-02 -2.28717271e-02  3.87331396e-02\n",
      "  -5.92640927e-03  2.58112587e-02  1.07046282e-02 -2.13107634e-02\n",
      "  -5.68847023e-02 -1.03845254e-01 -1.56066045e-02 -2.37020664e-03\n",
      "  -5.53983152e-02  1.41239958e-02  4.16721776e-02 -4.94214380e-03\n",
      "  -3.33841853e-02 -1.31457727e-02  2.10676156e-02  2.68434752e-02\n",
      "  -7.51979277e-03  5.21299504e-02 -2.76771300e-02  2.89556906e-02\n",
      "  -2.77455654e-02  1.34863518e-03 -5.38735762e-02 -1.68409050e-02\n",
      "  -5.56399999e-03  7.27962032e-02  2.13956740e-02  6.11245707e-02\n",
      "   4.87289913e-02 -1.38505585e-02 -4.72114496e-02  1.94927305e-02\n",
      "   6.93553537e-02 -2.55603474e-02 -2.96267718e-02  1.84706948e-03\n",
      "  -3.83176259e-03 -1.33285462e-03  2.91076931e-03 -4.17420315e-03\n",
      "   1.77704245e-02  5.87972105e-02 -4.15218882e-02 -1.12346997e-02\n",
      "   3.54997441e-02 -1.21636866e-02 -3.24738445e-03 -6.75864741e-02\n",
      "  -2.14202497e-02  4.92023770e-04  7.63963303e-03  5.20434789e-02\n",
      "   1.05564408e-02  3.66635770e-02  6.22056685e-02  2.42977086e-02\n",
      "  -3.37244049e-02 -7.64773637e-02  5.22039868e-02 -1.65876106e-03\n",
      "  -4.78511192e-02 -9.22142714e-02 -8.98012333e-03  2.94548273e-02\n",
      "   2.64397487e-02 -6.55518938e-03 -2.94543300e-02 -9.96269938e-03\n",
      "   6.66878894e-02 -2.17092931e-02  5.89128435e-02  4.50228341e-02\n",
      "   4.00608480e-02  4.38636988e-02 -1.21998815e-02  5.86493127e-02\n",
      "  -2.82373670e-02 -9.98216495e-03  1.69934668e-02  2.91365795e-02\n",
      "  -6.76102610e-03 -2.15368811e-02 -4.41377200e-02  3.06432527e-02\n",
      "   3.04348506e-02  1.31427459e-02  8.31827149e-02 -2.82439012e-02\n",
      "  -3.23298238e-02 -3.09661590e-02  3.02492380e-02 -4.68960255e-02\n",
      "   5.53342998e-02  4.52478081e-02  5.77115491e-02 -3.29295285e-02\n",
      "  -4.31207158e-02 -7.95106404e-03 -1.25172241e-02  1.51822101e-02\n",
      "  -4.36458290e-02 -2.45560110e-02 -6.52806088e-02  2.25012917e-02\n",
      "  -1.06938798e-02 -3.13907713e-02 -8.33940953e-02  2.31356788e-02\n",
      "   3.67501308e-03 -3.96588631e-02  3.87003981e-02  4.38621305e-02\n",
      "   4.22128215e-02  3.71916294e-02  5.13500385e-02  1.31055824e-02\n",
      "  -2.13621669e-02  3.79450954e-02 -3.86999175e-02 -2.96189990e-02\n",
      "  -2.92930920e-02 -6.45178631e-02 -3.57626490e-02  1.64252662e-04\n",
      "  -1.22195957e-02 -1.38677051e-02 -6.31082803e-03  3.45949382e-02\n",
      "  -2.01017479e-03  3.02026048e-02  7.86230899e-03  1.14573501e-02\n",
      "   1.48635283e-02 -6.32371008e-02 -1.21748503e-02 -2.90642828e-02\n",
      "  -5.72008416e-02 -3.95113751e-02  5.10269813e-02 -8.54405109e-03\n",
      "  -2.69517060e-02  1.20676551e-02  1.31518301e-02  2.50985343e-02\n",
      "   2.03869622e-02 -4.39010113e-02  1.04849385e-02  7.48372683e-03\n",
      "  -4.18148600e-02 -8.24713428e-03 -1.63528118e-02 -1.30722318e-02\n",
      "   2.10767258e-02 -3.89261656e-02  2.73122545e-02  2.21273415e-02\n",
      "   2.60084821e-03 -4.36834730e-02  1.23442449e-02 -1.74374189e-02\n",
      "  -4.43109237e-02  6.19191630e-03 -2.59326287e-02  1.17598996e-02\n",
      "   1.32206604e-02  6.56879347e-05 -2.21733712e-02  1.34821227e-02\n",
      "  -1.48424599e-02  4.63303924e-02  2.76194811e-02 -3.29413787e-02\n",
      "   2.08653305e-02 -8.51942319e-03  1.83525681e-02 -7.32432604e-02\n",
      "   2.47549918e-02 -2.21970174e-02  2.71180831e-02 -1.92867555e-02\n",
      "  -7.99038913e-03 -4.26915623e-02  4.47556637e-02 -1.47815393e-02\n",
      "  -3.67784537e-02  8.60197004e-03  4.86511308e-05  2.82438099e-03\n",
      "   1.27170170e-02  2.57738866e-03  2.71055531e-02 -5.78247868e-02\n",
      "   2.72608381e-02  1.49550168e-02 -9.72249266e-03 -1.69346947e-02\n",
      "   4.83349413e-02 -3.86091620e-02 -1.90529823e-02  2.83870613e-03\n",
      "  -1.53419385e-02  3.64241228e-02  3.73153053e-02  9.51674243e-04\n",
      "   5.02773784e-02 -1.70381349e-02 -1.94393042e-02  1.84496660e-02\n",
      "  -5.53628542e-02 -3.72986421e-02 -2.02295445e-02  1.08190230e-03\n",
      "   1.32682975e-02  1.93757974e-02  5.51666738e-03 -7.23927021e-02\n",
      "  -5.38529195e-02  3.33567932e-02  3.98947708e-02  2.94989161e-02\n",
      "   6.70321146e-03  9.62834246e-03  1.45433927e-02  5.21575361e-02\n",
      "  -3.05347163e-02 -3.29344394e-03 -6.72716973e-03  8.61796644e-03\n",
      "   3.35118026e-02  1.66914007e-03 -8.71827267e-03  1.60078518e-02\n",
      "  -5.62244840e-02 -4.47718948e-02  6.11947961e-02  6.62449747e-02\n",
      "   3.72280329e-02 -1.34120490e-02 -3.63522284e-02  2.50775274e-02\n",
      "   2.71300934e-02 -4.76711467e-02  6.64300695e-02 -7.48352264e-04\n",
      "  -3.59023437e-02  5.54666184e-02  2.57118931e-03  1.92204863e-02\n",
      "   3.49944457e-02  5.74424602e-02 -3.46615836e-02  2.99339667e-02\n",
      "  -9.09153279e-03  1.95558509e-03  6.18274324e-02 -3.14562358e-02\n",
      "  -1.24397874e-02 -5.65198883e-02  6.06989525e-02 -4.50200960e-02\n",
      "   9.42784473e-02  1.59950387e-02 -1.75958946e-02 -2.31031259e-03\n",
      "  -4.06326093e-02 -5.87513931e-02  2.27603596e-02 -2.98899673e-02\n",
      "   5.68593927e-02 -3.10534053e-02 -5.96246235e-02  4.08679508e-02\n",
      "   2.18336377e-02 -6.20613359e-02  6.55732453e-02  1.30990716e-02\n",
      "   6.26493543e-02  2.35514753e-02  2.51108278e-02 -3.52510698e-02\n",
      "   4.16687205e-02 -6.39053956e-02  7.73442350e-03 -3.49766724e-02\n",
      "   7.70594506e-03  1.47706568e-02 -1.44723682e-02 -1.04434360e-02\n",
      "   7.84313306e-03 -1.04321437e-02 -1.54901668e-03  3.99198830e-02\n",
      "  -5.03712520e-02 -6.96551427e-02  1.81796849e-02  1.56422313e-02\n",
      "  -7.88587239e-03  1.69165041e-02 -2.10620835e-02  2.67736847e-03\n",
      "   3.46136428e-02 -4.53496492e-03 -2.51985509e-02  8.95006291e-04\n",
      "   1.66916698e-02 -5.25580496e-02 -2.86832023e-02  3.19846682e-02\n",
      "   7.88568053e-03 -1.21673178e-02  1.28689557e-02 -1.86769816e-03\n",
      "   5.51887671e-04  1.30423075e-02  4.24013194e-03  3.58549505e-02\n",
      "  -2.60336809e-02 -6.69290498e-03 -1.72169972e-02  5.86491497e-03\n",
      "  -9.78455693e-03 -1.14251217e-02  1.73149146e-02  4.25337367e-02\n",
      "   8.20584688e-03  1.76936444e-02 -1.98405404e-02 -3.71182673e-02\n",
      "   2.17678752e-02 -4.02680971e-02  2.30468735e-02  4.74226847e-03\n",
      "  -5.47371097e-02 -2.96983887e-02 -4.77662310e-02 -2.98024807e-02\n",
      "   3.73192094e-02 -3.26062478e-02  2.71794163e-02  2.72671841e-02\n",
      "   5.10604791e-02 -4.19475995e-02 -8.77371579e-02 -3.67274433e-02\n",
      "  -2.64927000e-02  1.30109275e-02  8.72100797e-03 -2.59619560e-02\n",
      "   5.86190587e-03 -1.91063546e-02 -4.80579063e-02  1.50747551e-02\n",
      "   1.05287265e-02  8.82847141e-03  3.24362814e-02  5.51356450e-02\n",
      "  -2.67047677e-02  8.00575595e-03  9.34923626e-03 -2.91466713e-02\n",
      "   3.75466868e-02  1.00712432e-02  2.76912432e-02  4.92471792e-02\n",
      "   4.43484187e-02  4.53764200e-03 -4.44925437e-03 -6.42685071e-02\n",
      "  -4.88042273e-02 -4.08312194e-02  3.81208421e-03  4.81991954e-02]]\n"
     ]
    }
   ],
   "source": [
    "instruction = \"Represent the AI document for classification:\"\n",
    "embeddings = model.encode([[instruction,sentence]])\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f6bea5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['What is \"backprop\"? What does \"backprop\" mean? Is the \"backprop\" term basically the same as \"backpropagation\" or does it have a different meaning?\\n',\n",
       "       'How does noise affect generalization? Does increasing the noise in data help to improve the learning ability of a network? Does it make any difference or does it depend on the problem being solved? How is it affect the generalization process overall?\\n',\n",
       "       '\"Backprop\" is the same as \"backpropagation\": it\\'s just a shorter way to say it. It is sometimes abbreviated as \"BP\".\\n',\n",
       "       ...,\n",
       "       \"One of the key features of ChatGPT is its ability to maintain context and provide coherent responses to prompts that refer to previous information. This is enabled by the use of a recurrent neural network (RNN) architecture, which allows the model to process input text in a sequential manner and maintain a state or hidden representation that captures information about the previous inputs.\\nIn the case of ChatGPT, the RNN architecture is composed of multiple layers of transformer blocks, which are a type of neural network layer designed for processing sequential data. Each transformer block contains multiple self-attention layers, which allow the model to attend to different parts of the input text at the same time and weight the importance of different pieces of information in the context of the current input.\\nWhen ChatGPT receives a new input, it processes the input using the transformer blocks and updates its internal state to incorporate information from the new input. This updated state is then used to generate a response, which takes into account the context of the previous inputs and provides a coherent and relevant response to the current prompt.\\nIn summary, ChatGPT's ability to maintain context and provide relevant responses to prompts that refer to previous information is enabled by its use of a recurrent neural network architecture and self-attention layers, which allow it to process input text sequentially and incorporate information from previous inputs into its responses.\\n\",\n",
       "       \"My cross entropy loss gradient calculation is wrong according to the answer key Given a neural network model for Covid-19 classification with $C=1$ for positive and $C=0$ for negative\\n\\nLet $x_1 = 6$ and $x_2=2$ find\\n\\nProbability if the patient got Covid-19 $p\\\\left(C=1 | x; w,b\\\\right)$\\nProbability if the patient got Covid-19 $p\\\\left(C=1 | x; w,b\\\\right)$\\nFind the gradient of $CE_{Loss}$\\n\\nMy attempt\\nFor the first problem\\n$$\\n\\\\begin{aligned}\\nO_1 &= \\\\text{ReLU} \\\\left( b_1 + \\\\sum{x_iw_i} \\\\right) \\\\\\\\\\n&= \\\\text{ReLU} \\\\left( 0.2 + 6 \\\\cdot 0.3 - 0.2 \\\\cdot 2 \\\\right) \\\\\\\\\\n&= \\\\text{ReLU} \\\\left( 1.6 \\\\right) \\\\\\\\\\n&= 1.6 \\\\\\\\\\n\\\\end{aligned}\\n$$\\n$$\\n\\\\begin{aligned}\\nO_2 &= \\\\text{Sig} \\\\left( -0.6 - 0.1 \\\\cdot 2 - 0.2 \\\\cdot 6 \\\\right) \\\\\\\\\\n&= \\\\text{Sig} \\\\left( -2 \\\\right) \\\\\\\\\\n&\\\\approx 0.8808 \\\\\\\\\\n\\\\end{aligned}\\n$$\\n$$\\n\\\\begin{aligned}\\nO_3 &= \\\\text{Sig} \\\\left( 0.6 - 0.3 \\\\cdot 0.8808 + 1.6 \\\\cdot 0.5 \\\\right) \\\\\\\\\\n&\\\\approx 0.75690 \\\\\\\\\\n\\\\end{aligned}\\n$$\\n$$\\n\\\\begin{aligned}\\np\\\\left(C=1 | x; w,b\\\\right) = O_3 \\\\approx 0.75690\\n\\\\end{aligned}\\n$$\\nFor the second problem\\n$$\\n\\\\begin{aligned}\\np\\\\left(C=0 | x; w,b\\\\right) = 1 - \\\\left(C=1 | x; w,b\\\\right) = 0.2431\\n\\\\end{aligned}\\n$$\\nFor the third problem\\nSince it is a lot of things to calculate I'll take $\\\\frac{\\\\delta L}{\\\\delta w_6}$ as example\\n$$\\n\\\\begin{aligned}\\n\\\\frac{\\\\delta L}{\\\\delta w_6} &= \\\\frac{\\\\delta L}{\\\\delta \\\\hat{y}} \\\\cdot \\\\frac{\\\\delta \\\\hat{y}}{\\\\delta w_6} \\\\\\\\\\n&= \\\\left(\\\\hat{y}-y\\\\right)O_2\\\\left(O_3 \\\\left(1-O_3\\\\right)\\\\right)\\\\\\\\\\n&= \\\\left(O_3-y\\\\right)O_2\\\\left(O_3 \\\\left(1-O_3\\\\right)\\\\right)\\n\\\\end{aligned}\\n$$\\nThe answer key says it should be $\\\\left(O_3-y\\\\right)O_2$. Where did I go wrong?\\n\",\n",
       "       \"Constraint Satisfaction Problem for 8-puzzle My question is more related to the fundamentals of the constraint satisfaction problem.\\nIn 8-puzzle, we have a 3X3 board with 8 numbers on it and a blank space. The initial state of the puzzle might be numbers out of their place and we need to move the blank space such that numbers come to the correct tiles and space ends up at the lower right corner.\\nSo in CSP, we have variables. In this case they can be zij which represents each tile in 3X3 board and possible actions A for blank tile. Let's consider blank space to be represented by 0. Then, domain would be 0 <=zij<= 8 and A={[1,0],[0,1],[-1,0],[0,-1]}.\\nNow my question is do we put constraints on the final state of the variables? Like:\\n - zij = 3(i-1)+j  [for all (i,j) E {(1,1),(1,2),...,(3,1),(3,2)}\\n - z33 = 0\\n This constraint tells which number should be at which tile in the final state.\\nOr should the constraint be on any arbitrary state of the game? In that case, the constraints would be like:\\n - 0 <= zij <= 8 [for all 1<=i<=3 and 1<=j<=3]\\n We will need a new variable p = (x,y) coordinate of 0. The domain of P would be from (1,1) to (3,3). And constraints would be:\\n - chose move 'm' from A if 1<=p[0]+A[m][0]<=3 and 1<=p[1]+A[m][1]<=3.\\nTo ask my question in short, in constraint satisfaction problems, do we write constraints on the final state of the game or on any arbitrary state?\\n\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_posts_text_array = ai_posts['text'].values\n",
    "ai_posts_text_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52151d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace each array element with a list of two elements\n",
    "ai_posts_text_array = [ [instruction, text] for text in ai_posts_text_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c47aff6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featurization took  3550.359 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# ai_posts['vector'] = sentxformer.encode(ai_posts['text'].values).tolist()\n",
    "ai_posts['vector'] = model.encode(ai_posts_text_array).tolist()\n",
    "\n",
    "end = time.time()\n",
    "print(f'featurization took {end - start: 0.3f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efd03df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        neural-networks;backpropagation;terminology;de...\n",
       "1        neural-networks;machine-learning;statistical-a...\n",
       "2                                                      NaN\n",
       "3        neural-networks;hyperparameter-optimization;ar...\n",
       "4                 philosophy;definitions;intelligent-agent\n",
       "                               ...                        \n",
       "23174                                                  NaN\n",
       "23175                                                  NaN\n",
       "23176                                                  NaN\n",
       "23177                             neural-networks;homework\n",
       "23178              search;constraint-satisfaction-problems\n",
       "Name: tags, Length: 23179, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurized_file = DATA_DIR + csv_file.replace('.csv', '_featurized_instructor-large.csv')\n",
    "\n",
    "ai_posts.to_csv(featurized_file, index=False) # ~400MB\n",
    "\n",
    "# ai_posts = pd.read_csv(featurized_file)\n",
    "# ai_posts['vector'] = [eval(v) for v in ai_posts['vector']]\n",
    "\n",
    "ai_posts['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8728e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('neural-networks', 2333),\n",
       " ('reinforcement-learning', 2200),\n",
       " ('machine-learning', 2100),\n",
       " ('deep-learning', 1804),\n",
       " ('convolutional-neural-networks', 1067),\n",
       " ('natural-language-processing', 628),\n",
       " ('reference-request', 453),\n",
       " ('computer-vision', 450),\n",
       " ('deep-rl', 446),\n",
       " ('comparison', 430),\n",
       " ('classification', 426),\n",
       " ('training', 411),\n",
       " ('terminology', 376),\n",
       " ('q-learning', 354),\n",
       " ('recurrent-neural-networks', 334),\n",
       " ('python', 324),\n",
       " ('tensorflow', 320),\n",
       " ('dqn', 309),\n",
       " ('papers', 306),\n",
       " ('image-recognition', 278),\n",
       " ('long-short-term-memory', 270),\n",
       " ('ai-design', 265),\n",
       " ('datasets', 251),\n",
       " ('objective-functions', 250),\n",
       " ('keras', 240),\n",
       " ('game-ai', 238),\n",
       " ('backpropagation', 236),\n",
       " ('math', 227),\n",
       " ('generative-adversarial-networks', 220),\n",
       " ('object-detection', 210),\n",
       " ('optimization', 207),\n",
       " ('definitions', 197),\n",
       " ('gradient-descent', 188),\n",
       " ('transformer', 186),\n",
       " ('applications', 184),\n",
       " ('markov-decision-process', 183),\n",
       " ('pytorch', 180),\n",
       " ('philosophy', 179),\n",
       " ('agi', 178),\n",
       " ('policy-gradients', 178),\n",
       " ('genetic-algorithms', 172),\n",
       " ('deep-neural-networks', 169),\n",
       " ('activation-functions', 155),\n",
       " ('search', 154),\n",
       " ('data-preprocessing', 148),\n",
       " ('image-processing', 148),\n",
       " ('time-series', 134),\n",
       " ('research', 127),\n",
       " ('algorithm', 126),\n",
       " ('evolutionary-algorithms', 123),\n",
       " ('autoencoders', 123),\n",
       " ('regression', 122),\n",
       " ('prediction', 121),\n",
       " ('rewards', 120),\n",
       " ('unsupervised-learning', 119),\n",
       " ('generative-model', 118),\n",
       " ('hyperparameter-optimization', 117),\n",
       " ('image-segmentation', 110),\n",
       " ('attention', 109),\n",
       " ('proofs', 109),\n",
       " ('monte-carlo-tree-search', 107),\n",
       " ('actor-critic-methods', 107),\n",
       " ('implementation', 106),\n",
       " ('object-recognition', 105),\n",
       " ('algorithm-request', 103),\n",
       " ('models', 98),\n",
       " ('value-functions', 98),\n",
       " ('variational-autoencoder', 97),\n",
       " ('proximal-policy-optimization', 96),\n",
       " ('overfitting', 91),\n",
       " ('supervised-learning', 91),\n",
       " ('hyper-parameters', 85),\n",
       " ('convolution', 85),\n",
       " ('bert', 84),\n",
       " ('feedforward-neural-networks', 83),\n",
       " ('function-approximation', 82),\n",
       " ('weights', 82),\n",
       " ('convergence', 82),\n",
       " ('sutton-barto', 80),\n",
       " ('reward-functions', 79),\n",
       " ('graph-neural-networks', 78),\n",
       " ('word-embedding', 77),\n",
       " ('policies', 77),\n",
       " ('monte-carlo-methods', 76),\n",
       " ('architecture', 76),\n",
       " ('probability-distribution', 76),\n",
       " ('geometric-deep-learning', 74),\n",
       " ('open-ai', 73),\n",
       " ('computational-learning-theory', 72),\n",
       " ('loss', 72),\n",
       " ('neat', 71),\n",
       " ('temporal-difference-methods', 70),\n",
       " ('transfer-learning', 69),\n",
       " ('intelligent-agent', 68),\n",
       " ('multilayer-perceptrons', 68),\n",
       " ('minimax', 68),\n",
       " ('notation', 67),\n",
       " ('yolo', 66),\n",
       " ('pattern-recognition', 65),\n",
       " ('chat-bots', 64)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_tags = []\n",
    "for tag_str in ai_posts['tags'].values:\n",
    "    if tag_str == tag_str: # not NaN\n",
    "        tags = tag_str.split(';')\n",
    "        all_tags.extend(tags)\n",
    "    \n",
    "\n",
    "len(all_tags) # 36223\n",
    "\n",
    "Counter(all_tags).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60ebb6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a variety of interesting and reasonably common tags\n",
    "target_tags = ['philosophy', 'proofs', 'q-learning', 'deep-rl',  'superintelligence', 'classification']\n",
    "\n",
    "for tt in target_tags:\n",
    "    flag_col = tt + '_flag'\n",
    "    ai_posts[flag_col] = [1 if tt in str(tag_str) else 0 for tag_str in ai_posts['tags'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bf94dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "tag_name = 'classification'\n",
    "flag_col = tag_name + '_flag'\n",
    "score_col = tag_name + '_score'\n",
    "\n",
    "X_all = [v for v in ai_posts['vector']]\n",
    "y_all = [f for f in ai_posts[flag_col]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.25, random_state=42)\n",
    "\n",
    "Cs = np.logspace(-4, 4, 5)\n",
    "\n",
    "clf = LogisticRegressionCV(Cs=Cs, max_iter=10000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "score_test = clf.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3143c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_auc(labels, scores):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(labels, scores)\n",
    "\n",
    "    auc = metrics.roc_auc_score(labels, scores)\n",
    "\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('fpr')\n",
    "    plt.ylabel('tpr')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.text(0.8, 0.2, f\"AUC:{auc:0.3f}\")\n",
    "\n",
    "plot_auc(y_test, score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5b4c569",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_posts[score_col] = clf.predict_proba(X_all)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87d2c490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>classification_flag</th>\n",
       "      <th>classification_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5178</th>\n",
       "      <td>How should I label the classes in RNA? I have a project, which is the keyboard biometrics of users.\\nsuppose I have 3 users, \\nI do not know how to label in two types of class, (+ 1, -1). \\nIf I want to verify the identity to user1, my idea of ​​class designation would be:\\n       TIMES                LABEL\\nuser 1\\n9.4  9.2  1.0  3.4  0.5      1\\n9.4  9.2  1.0  3.4  0.5      1\\n9.4  9.2  1.0  3.4  0.5      1\\n9.4  9.2  1.0  3.4  0.5      1\\n9.4  9.2  1.0  3.4  0.5      1\\n\\nuser 2\\n0.1  3.2  1.0  1.2  1.7      -1\\n3.4  1.2  3.0  1.1  2.8      -1\\n2.4  2.2  3.0  1.6  2.9      -1\\n1.4  3.2  2.0  2.6  3.6      -1\\n3.4  0.2   3.0  2.7  3.5     -1\\n\\nuser N\\n0.2  1.4  4.5  3.7  2.9      -1\\n9.2  1.5  7.6  2.6  2.6      -1\\n9.3  1.6  7.5  2.9  3.4      -1\\n9.8  3.8  6.6  2.8  2.5      -1\\n9.8  2.8  1.7  3.8  1.6      -1\\n\\nbut as my system has more and more users classes -1 will be too many compared to classes +1,\\nHow should I label the classes?\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0.579249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22664</th>\n",
       "      <td>Fine tuning a Deep Learning model post training I have trained a CNN in a binary classification problem, however the original problem has 6 different classes, of which, I am only interested in classifying one, so if it is that certain class or not.in this case, let's say class 2.\\nAfter looking closely into the model's performance on test dataset, I have found that the model confuses class 2 with class 1 often. Is it common practice, to make a balanced dataset from the data that I have only from class 1 and class 2, and further train the model on that dataset? Are there any pieces of research/papers on this? If no, what other possible solutions would there be, of course other than making a new model?\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0.543569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15362</th>\n",
       "      <td>How to verify classification model trained on classification dataset on a detection dataset for classification purpose? I am working on a problem that involves two tasks - detection and classification. There is no single dataset for both tasks. I am training two models, separate on detection dataset and another on classification dataset. I use the images from the detection dataset as input and get classification predictions on top of detected bounding boxes.\\nDataset description :\\n\\nClassification - Image of the single object (E.g. Car) in the center with a classification label.\\nDetection - Image with multiple objects (E.g. 4 Cars) with bounding box annotations.\\n\\nTask - Detect objects(e.g. cars) from detection datasets and classify them into various categories.\\nHow do I verify whether the classification model trained on the classification dataset is working on images from detection dataset? (In terms of classification accuracy)\\nI cannot manually label the images from the detection dataset for individual class labels. (Need expert domain knowledge)\\nHow do I verify my classification model?\\nIs there any technique to do this ? Like domain transfer or any weakly-supervised method ?\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0.542196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21808</th>\n",
       "      <td>Multiclass image classification - what approach to use and which models to consider? I'm working on an image classification project and I need to train a multiclass, multilabel classifier. The dataset is large and some of the images are mislabeled (for a given class, some labels are pretty easy to mix up). As an approach, I am using the following:\\n\\nConsidering the main available models, check their accuracy for the considered dataset at different learning rates. More in detail, my approach is to take a pretrained model, remove the last layer and fine-tune it on the dataset we're considering.\\nConsider the model and learning rate which gives the highest accuracy.\\nImprove the accuracy by grouping classes that are often confused. This is done by looking at the confusion matrix.\\n\\nDo you think this is a good approach to use? And which models should I consider in the first place? As of now I'm looking into the ResNet family (18, 34, 50, 101, 152 layers) and visual transformers.\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0.439736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9894</th>\n",
       "      <td>Do models train better if the labelling information is more specific (or dense)? I'm working on a project where there is a limited dataset of videos (about 200). We want to train a model that can detect a single class in the videos. That class can be of multiple different types of shapes (thin wire, a huge area of the screen, etc).\\nThere are three options on how we can label this data:\\n\\nImage classification (somewhere in the image is this class)\\nBounding box (in this area, there is the class)\\nSemantic segmentation (these pixels are the class)\\n\\nMy assumption is that if the model was trained on semantic segmentation data it would perform slightly better than bounding box data.  I'm also assuming it would perform way better than if the model only learned on image classification data.  Is that correct?\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0.435120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10269</th>\n",
       "      <td>Multi label Classification using Keras I am trying to build a Multi label classification model, having dataset with different input numerical values and specific label...\\nEg:\\nValue Label\\n35   X\\n35.8 X\\n29   Y\\n29.8 Y\\n39   AA\\n41   CB\\nSo depending on input numerical value the model should specify its label....please note that the input values won't necessarily follow exact dataset values....eg dataset has 35 and 34.8 as input values with X as label. So if model has 35.4 as input label, the X should be output label. Bottom line is that the output label is based on range of input values instead of fixed one.. \\nCan anyone help me with quick solution (example Jupyter notebook will be highly appreciated)\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0.427339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17078</th>\n",
       "      <td>Should one use an \"other\" category in image classification? In image classification, there are sometimes images that do not fit in any category.\\nFor example, if I build a CNN in Keras to classify Dogs and Cats, does it help (in terms of training time and performance) to create an \"other\" (or \"unclassified\") category in which images of houses, people, birds, etc., are classified? Is there any research paper that discusses this?\\nA similar question was asked before here, but, unfortunately, it has no answer.\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0.415247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14785</th>\n",
       "      <td>How do I label images for deep learning classification? I have roughly 30,000 images of two categories, which are 'crops' and 'weeds.' An example of what I have can be found below:\\n\\nThe goal will use my training images to detect weeds among crops, given an orthomosaic GIS image of a given field. I guess you could say that I'm trying to detect certain objects in the field.\\nAs I'm new to deep learning, how would one go about generating training labels for this task? Can I just label the entire photo as a 'weed' using some type of text file, or do I actually have to draw bounding boxes (around weeds) on each image that will be used for training? If so, is there an easier way than going through all 30,000 of my images?\\nI'm very new to this, so any specific details would really help a lot!\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0.406713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21048</th>\n",
       "      <td>Why does the SVM perform poorly on test data that has a different class distribution than the training data? Do you know why the SVM performs poorly on test data that has a different class distribution than the training data? The training data has around 15 classes, and the additional testing data has around 6 classes (a subset of 15 classes). I found that the accuracy of new testing data is around 3% (all predicted labels belong to the same class).\\nHow can I deal with this problem?\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0.389859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6975</th>\n",
       "      <td>\\nFor example, if I have the crowd images labeled based on their crowd density (low, moderate, high), and I'm not interested in the count, but the density class (low, moderate, high), can't I train the network to classify the data based on these classes as a classification network?\\n\\nYes you can, all you need is enough correctly labelled training data.\\nA good rule of thumb is if a human expert can assign the correct label from an image (and purely from the image, not using extra information) then it is a realistic goal to train a CNN to perform the same labelling. \\n\\nwhy there hasn't been attempts to tackle this type of task as a classification problem.\\n\\nProbably because there are no natural, and likely no widely accepted, classes in this case (I may be wrong, maybe some international society has defined classes you could use). If you use a regression, you can map it to a particular problem case - e.g. sending an alert to someone responsible for traffic and safety when crowd density hits some threshold - by setting numerical boundaries to your classes. Using classification and mapping back the other way is harder.\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0.375335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        text  \\\n",
       "5178                                                                                                                                                                                                                                                           How should I label the classes in RNA? I have a project, which is the keyboard biometrics of users.\\nsuppose I have 3 users, \\nI do not know how to label in two types of class, (+ 1, -1). \\nIf I want to verify the identity to user1, my idea of ​​class designation would be:\\n       TIMES                LABEL\\nuser 1\\n9.4  9.2  1.0  3.4  0.5      1\\n9.4  9.2  1.0  3.4  0.5      1\\n9.4  9.2  1.0  3.4  0.5      1\\n9.4  9.2  1.0  3.4  0.5      1\\n9.4  9.2  1.0  3.4  0.5      1\\n\\nuser 2\\n0.1  3.2  1.0  1.2  1.7      -1\\n3.4  1.2  3.0  1.1  2.8      -1\\n2.4  2.2  3.0  1.6  2.9      -1\\n1.4  3.2  2.0  2.6  3.6      -1\\n3.4  0.2   3.0  2.7  3.5     -1\\n\\nuser N\\n0.2  1.4  4.5  3.7  2.9      -1\\n9.2  1.5  7.6  2.6  2.6      -1\\n9.3  1.6  7.5  2.9  3.4      -1\\n9.8  3.8  6.6  2.8  2.5      -1\\n9.8  2.8  1.7  3.8  1.6      -1\\n\\nbut as my system has more and more users classes -1 will be too many compared to classes +1,\\nHow should I label the classes?\\n   \n",
       "22664                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Fine tuning a Deep Learning model post training I have trained a CNN in a binary classification problem, however the original problem has 6 different classes, of which, I am only interested in classifying one, so if it is that certain class or not.in this case, let's say class 2.\\nAfter looking closely into the model's performance on test dataset, I have found that the model confuses class 2 with class 1 often. Is it common practice, to make a balanced dataset from the data that I have only from class 1 and class 2, and further train the model on that dataset? Are there any pieces of research/papers on this? If no, what other possible solutions would there be, of course other than making a new model?\\n   \n",
       "15362  How to verify classification model trained on classification dataset on a detection dataset for classification purpose? I am working on a problem that involves two tasks - detection and classification. There is no single dataset for both tasks. I am training two models, separate on detection dataset and another on classification dataset. I use the images from the detection dataset as input and get classification predictions on top of detected bounding boxes.\\nDataset description :\\n\\nClassification - Image of the single object (E.g. Car) in the center with a classification label.\\nDetection - Image with multiple objects (E.g. 4 Cars) with bounding box annotations.\\n\\nTask - Detect objects(e.g. cars) from detection datasets and classify them into various categories.\\nHow do I verify whether the classification model trained on the classification dataset is working on images from detection dataset? (In terms of classification accuracy)\\nI cannot manually label the images from the detection dataset for individual class labels. (Need expert domain knowledge)\\nHow do I verify my classification model?\\nIs there any technique to do this ? Like domain transfer or any weakly-supervised method ?\\n   \n",
       "21808                                                                                                                                                                                                                      Multiclass image classification - what approach to use and which models to consider? I'm working on an image classification project and I need to train a multiclass, multilabel classifier. The dataset is large and some of the images are mislabeled (for a given class, some labels are pretty easy to mix up). As an approach, I am using the following:\\n\\nConsidering the main available models, check their accuracy for the considered dataset at different learning rates. More in detail, my approach is to take a pretrained model, remove the last layer and fine-tune it on the dataset we're considering.\\nConsider the model and learning rate which gives the highest accuracy.\\nImprove the accuracy by grouping classes that are often confused. This is done by looking at the confusion matrix.\\n\\nDo you think this is a good approach to use? And which models should I consider in the first place? As of now I'm looking into the ResNet family (18, 34, 50, 101, 152 layers) and visual transformers.\\n   \n",
       "9894                                                                                                                                                                                                                                                                                                                                                                                                      Do models train better if the labelling information is more specific (or dense)? I'm working on a project where there is a limited dataset of videos (about 200). We want to train a model that can detect a single class in the videos. That class can be of multiple different types of shapes (thin wire, a huge area of the screen, etc).\\nThere are three options on how we can label this data:\\n\\nImage classification (somewhere in the image is this class)\\nBounding box (in this area, there is the class)\\nSemantic segmentation (these pixels are the class)\\n\\nMy assumption is that if the model was trained on semantic segmentation data it would perform slightly better than bounding box data.  I'm also assuming it would perform way better than if the model only learned on image classification data.  Is that correct?\\n   \n",
       "10269                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Multi label Classification using Keras I am trying to build a Multi label classification model, having dataset with different input numerical values and specific label...\\nEg:\\nValue Label\\n35   X\\n35.8 X\\n29   Y\\n29.8 Y\\n39   AA\\n41   CB\\nSo depending on input numerical value the model should specify its label....please note that the input values won't necessarily follow exact dataset values....eg dataset has 35 and 34.8 as input values with X as label. So if model has 35.4 as input label, the X should be output label. Bottom line is that the output label is based on range of input values instead of fixed one.. \\nCan anyone help me with quick solution (example Jupyter notebook will be highly appreciated)\\n   \n",
       "17078                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Should one use an \"other\" category in image classification? In image classification, there are sometimes images that do not fit in any category.\\nFor example, if I build a CNN in Keras to classify Dogs and Cats, does it help (in terms of training time and performance) to create an \"other\" (or \"unclassified\") category in which images of houses, people, birds, etc., are classified? Is there any research paper that discusses this?\\nA similar question was asked before here, but, unfortunately, it has no answer.\\n   \n",
       "14785                                                                                                                                                                                                                                                                                                                                                                                                                      How do I label images for deep learning classification? I have roughly 30,000 images of two categories, which are 'crops' and 'weeds.' An example of what I have can be found below:\\n\\nThe goal will use my training images to detect weeds among crops, given an orthomosaic GIS image of a given field. I guess you could say that I'm trying to detect certain objects in the field.\\nAs I'm new to deep learning, how would one go about generating training labels for this task? Can I just label the entire photo as a 'weed' using some type of text file, or do I actually have to draw bounding boxes (around weeds) on each image that will be used for training? If so, is there an easier way than going through all 30,000 of my images?\\nI'm very new to this, so any specific details would really help a lot!\\n   \n",
       "21048                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Why does the SVM perform poorly on test data that has a different class distribution than the training data? Do you know why the SVM performs poorly on test data that has a different class distribution than the training data? The training data has around 15 classes, and the additional testing data has around 6 classes (a subset of 15 classes). I found that the accuracy of new testing data is around 3% (all predicted labels belong to the same class).\\nHow can I deal with this problem?\\n   \n",
       "6975                                                                      \\nFor example, if I have the crowd images labeled based on their crowd density (low, moderate, high), and I'm not interested in the count, but the density class (low, moderate, high), can't I train the network to classify the data based on these classes as a classification network?\\n\\nYes you can, all you need is enough correctly labelled training data.\\nA good rule of thumb is if a human expert can assign the correct label from an image (and purely from the image, not using extra information) then it is a realistic goal to train a CNN to perform the same labelling. \\n\\nwhy there hasn't been attempts to tackle this type of task as a classification problem.\\n\\nProbably because there are no natural, and likely no widely accepted, classes in this case (I may be wrong, maybe some international society has defined classes you could use). If you use a regression, you can map it to a particular problem case - e.g. sending an alert to someone responsible for traffic and safety when crowd density hits some threshold - by setting numerical boundaries to your classes. Using classification and mapping back the other way is harder.\\n   \n",
       "\n",
       "       classification_flag  classification_score  \n",
       "5178                     0              0.579249  \n",
       "22664                    0              0.543569  \n",
       "15362                    0              0.542196  \n",
       "21808                    0              0.439736  \n",
       "9894                     0              0.435120  \n",
       "10269                    0              0.427339  \n",
       "17078                    0              0.415247  \n",
       "14785                    0              0.406713  \n",
       "21048                    0              0.389859  \n",
       "6975                     0              0.375335  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "show_cols = ['text', flag_col, score_col]\n",
    "\n",
    "# Highest scoring examples that do not have the tag\n",
    "ai_posts[ai_posts[flag_col]==0].sort_values(score_col, ascending=False).head(10)[show_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84bcebbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>classification_flag</th>\n",
       "      <th>classification_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1644</th>\n",
       "      <td>Is there a measure of AI relative strength, modified by resources? For instance, Strength/Size$\\times$Speed, where size and speed refer to memory and processing.\\nWe now have very strong, narrow AI, but they tend to run on fast hardware without volume restrictions.\\nTo understand why I'm asking, this article on BBC may provide some insight: \"Which life form dominates Earth?\"  (If I was a betting man, I'd put money on tardigrades outlasting humans, and the secret of their success is that they require minimal resources and processing power, unlike higher-order automata.)\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11443</th>\n",
       "      <td>Language Learning feedback with AI Is there a program under development that uses AI technology, like Siri, to \"hold hands\" so to speak with a language learner and coach them on accent, colloqiual expressions, or to let them guide the language learning process using an archive of language knowledge? \\nAlso, could this sort of program be used to learn things in a language one already knows, or in a new language, say for the purposes of travel or to learn about related hyperlinks in an online database?\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17709</th>\n",
       "      <td>Why do we use the softmax instead of no activation function? Why do we use the softmax activation function on the last layer?\\nSuppose $i$ is the index that has the highest value (in the case when we don't use softmax at all). If we use softmax and take $i$th value, it would be the highest value because $e$ is an increasing function, so that's why I am asking this question. Taking argmax(vec) and argmax(softmax(vec)) would give us the same value.\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16816</th>\n",
       "      <td>What is the definition of the hinge loss function? I came across the hinge loss function for training a neural network model, but I did not know the analytical form for the same.\\nI can write the mean squared error loss function (which is more often used for regression) as\\n$$\\sum\\limits_{i=1}^{N}(y_i - \\hat{y_i})^2$$\\nwhere $y_i$ is the desired output in the dataset, $\\hat{y_i}$ is the actual output by the model, and $N$ is the total number of instances in our dataset.\\nSimilarly, what is the (basic) expression for hinge loss function?\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <td>Detect patterns in sequences of actions I have to analyse sequences of actions that look more or less like this JSON blob. The question I'm trying to answer is whether there are recurring (sub)patterns that different users adopt when asked to perform a certain specific task -- in this case, the task is to build a mathematical formula using this editor. In particular I'd like to know if there are multiple significantly different ways in which people build the same expression.\\nI thought of creating a Markov model, but that would only give me the most likely sequence of actions of length N. An obvious alternative would be to build trees and count how many times a certain path occurs in the dataset. However, the nature of the expression-building process means that the sequences can be polluted by many confounding, non-significant actions (such as streaks of UNDO-REDO, deleting symbols, and the likes).\\nI might go the \"longest common subsequence\" route, but I'm not sure that would tell me if there are \"significantly different\" ways of building the same expression (in quotes because, for now, I don't have a rigorous definition of \"significantly different\", but, for example, one way would be to drag and drop-in-place all the symbols in the correct order, and another way would be to drag all the symbols onto the canvas, and then place them in the correct spots).\\nI thought this might be a nice challenge for some AI algorithm, but I'm quite a noob at that, so I'm open to suggestions.\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17192</th>\n",
       "      <td>Why the number of training points to densely cover the space grows exponentially with the dimension? In this lecture (minute 42), the professor says that the number of training examples we need to densely cover the space of training vectors grows exponentially with the dimension of the space. So we need $4^2=16$ training data points if we're working on $2D$ space. I'd like to ask why this is true and how is it proved/achieved? The professor was talking before about K-Nearest Neighbors and he was using $L^{1}$ and $L^{2}$ metrics. I don't think these metrics induce a topology that makes a discrete set of points dense in the ambient space.\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16557</th>\n",
       "      <td>Why don't those developing AI Deepfake detectors use two detectors at once so as to catch deepfakes in one or the other? Why don't those developing AI Deepfake detectors use two differently trained detectors at once that way if the Deepfake was trained to fool one of the detectors the other would catch it and vice-versa?\\nTo be clear this is really a question of can deepfakes be made to fool multiple high-accuracy detectors at the same time. And if so then how many can they fool before they become human detectable from noticeable noise?\\nI've heard of papers where they injected a certain noise into their deepfake videos which allows them to fool a given detector (https://arxiv.org/abs/2009.09213, https://delaat.net/rp/2019-2020/p74/report.pdf), so I thought well if they simply used two high-accuracy detectors then any pattern of noise used to fool one detector would interfere with the pattern of noise used to fool the other detector.\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5401</th>\n",
       "      <td>Can current AI techniques distinguish a fake old paper from a real one? It is an easy matter to make a paper look old, for example, using any of the techniques explained on this page of WikiHow: https://www.wikihow.com/Make-Paper-Look-Old.\\nIs current AI sufficient to distinguish a fake old paper from a real one?\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11684</th>\n",
       "      <td>How to update edge features in a graph using a loss function? Given a directed, edge attributed graph G, where the edge attribute is a probability value, and a particular node N (with binary features f1 and f2) in G, the algorithm that I want to implement is as follows:\\n\\nList all the outgoing edges from N, let this list be called edgelist_N.\\nFor all the edges in edgelist_N, randomly assign to the edge attribute a probability value such that the sum of all the probabilities assigned to the edges in the edgelist_N equals to 1.\\nTake the top x edges (x can be a hyperparameter).\\nList the nodes in which the edges from step 3 are incoming.\\nConstruct a subgraph with node N, the nodes from step 4 and the edges from step 3. \\nEmbed the subgraph (preferably using a GNN) and obtain it's embedding and use it with a classifier to predict say f1/f2. \\nPropagate the loss so as to update the edge probabilities, that was assigned randomly in step 2.\\n\\nI do not understand how to do step 7, i.e. update the edge attribute with the loss, so that edges which are more relevant in constructing the subgraph can be assigned a higher probability value.\\nAny suggestion would be highly appreciated.\\nThank you very much.\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15411</th>\n",
       "      <td>Where can I find pre-trained agents able to play games with multiple stages like exploration, dialog, combat? My goal is to create an ML model to be able to classify different game stages, e.g., dialog with a non-player character, exploration, combat with enemy, in-game menu etc.\\nIn order to do that, I am looking for an agent pre-trained on such a game. I am intending to develop a model using this pre-trained agent to produce a data set (frames-labels) and finally I will use that data set to train a model to classify those different stages.\\nI could only find a pre-trained model for the Doom, however, it is not much appropriate for my case because it does not have different game stages (it is merely based on running &amp; shooting).\\nTraining my own Reinforcement Learning Agent is a whole another workload in terms of both time and GPU such a game needs.\\nAny single idea could help me a lot. Thanks!\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 text  \\\n",
       "1644                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Is there a measure of AI relative strength, modified by resources? For instance, Strength/Size$\\times$Speed, where size and speed refer to memory and processing.\\nWe now have very strong, narrow AI, but they tend to run on fast hardware without volume restrictions.\\nTo understand why I'm asking, this article on BBC may provide some insight: \"Which life form dominates Earth?\"  (If I was a betting man, I'd put money on tardigrades outlasting humans, and the secret of their success is that they require minimal resources and processing power, unlike higher-order automata.)\\n   \n",
       "11443                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Language Learning feedback with AI Is there a program under development that uses AI technology, like Siri, to \"hold hands\" so to speak with a language learner and coach them on accent, colloqiual expressions, or to let them guide the language learning process using an archive of language knowledge? \\nAlso, could this sort of program be used to learn things in a language one already knows, or in a new language, say for the purposes of travel or to learn about related hyperlinks in an online database?\\n   \n",
       "17709                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Why do we use the softmax instead of no activation function? Why do we use the softmax activation function on the last layer?\\nSuppose $i$ is the index that has the highest value (in the case when we don't use softmax at all). If we use softmax and take $i$th value, it would be the highest value because $e$ is an increasing function, so that's why I am asking this question. Taking argmax(vec) and argmax(softmax(vec)) would give us the same value.\\n   \n",
       "16816                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                What is the definition of the hinge loss function? I came across the hinge loss function for training a neural network model, but I did not know the analytical form for the same.\\nI can write the mean squared error loss function (which is more often used for regression) as\\n$$\\sum\\limits_{i=1}^{N}(y_i - \\hat{y_i})^2$$\\nwhere $y_i$ is the desired output in the dataset, $\\hat{y_i}$ is the actual output by the model, and $N$ is the total number of instances in our dataset.\\nSimilarly, what is the (basic) expression for hinge loss function?\\n   \n",
       "2298   Detect patterns in sequences of actions I have to analyse sequences of actions that look more or less like this JSON blob. The question I'm trying to answer is whether there are recurring (sub)patterns that different users adopt when asked to perform a certain specific task -- in this case, the task is to build a mathematical formula using this editor. In particular I'd like to know if there are multiple significantly different ways in which people build the same expression.\\nI thought of creating a Markov model, but that would only give me the most likely sequence of actions of length N. An obvious alternative would be to build trees and count how many times a certain path occurs in the dataset. However, the nature of the expression-building process means that the sequences can be polluted by many confounding, non-significant actions (such as streaks of UNDO-REDO, deleting symbols, and the likes).\\nI might go the \"longest common subsequence\" route, but I'm not sure that would tell me if there are \"significantly different\" ways of building the same expression (in quotes because, for now, I don't have a rigorous definition of \"significantly different\", but, for example, one way would be to drag and drop-in-place all the symbols in the correct order, and another way would be to drag all the symbols onto the canvas, and then place them in the correct spots).\\nI thought this might be a nice challenge for some AI algorithm, but I'm quite a noob at that, so I'm open to suggestions.\\n   \n",
       "17192                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Why the number of training points to densely cover the space grows exponentially with the dimension? In this lecture (minute 42), the professor says that the number of training examples we need to densely cover the space of training vectors grows exponentially with the dimension of the space. So we need $4^2=16$ training data points if we're working on $2D$ space. I'd like to ask why this is true and how is it proved/achieved? The professor was talking before about K-Nearest Neighbors and he was using $L^{1}$ and $L^{2}$ metrics. I don't think these metrics induce a topology that makes a discrete set of points dense in the ambient space.\\n   \n",
       "16557                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Why don't those developing AI Deepfake detectors use two detectors at once so as to catch deepfakes in one or the other? Why don't those developing AI Deepfake detectors use two differently trained detectors at once that way if the Deepfake was trained to fool one of the detectors the other would catch it and vice-versa?\\nTo be clear this is really a question of can deepfakes be made to fool multiple high-accuracy detectors at the same time. And if so then how many can they fool before they become human detectable from noticeable noise?\\nI've heard of papers where they injected a certain noise into their deepfake videos which allows them to fool a given detector (https://arxiv.org/abs/2009.09213, https://delaat.net/rp/2019-2020/p74/report.pdf), so I thought well if they simply used two high-accuracy detectors then any pattern of noise used to fool one detector would interfere with the pattern of noise used to fool the other detector.\\n   \n",
       "5401                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Can current AI techniques distinguish a fake old paper from a real one? It is an easy matter to make a paper look old, for example, using any of the techniques explained on this page of WikiHow: https://www.wikihow.com/Make-Paper-Look-Old.\\nIs current AI sufficient to distinguish a fake old paper from a real one?\\n   \n",
       "11684                                                                                                                                                                                                                                                                                              How to update edge features in a graph using a loss function? Given a directed, edge attributed graph G, where the edge attribute is a probability value, and a particular node N (with binary features f1 and f2) in G, the algorithm that I want to implement is as follows:\\n\\nList all the outgoing edges from N, let this list be called edgelist_N.\\nFor all the edges in edgelist_N, randomly assign to the edge attribute a probability value such that the sum of all the probabilities assigned to the edges in the edgelist_N equals to 1.\\nTake the top x edges (x can be a hyperparameter).\\nList the nodes in which the edges from step 3 are incoming.\\nConstruct a subgraph with node N, the nodes from step 4 and the edges from step 3. \\nEmbed the subgraph (preferably using a GNN) and obtain it's embedding and use it with a classifier to predict say f1/f2. \\nPropagate the loss so as to update the edge probabilities, that was assigned randomly in step 2.\\n\\nI do not understand how to do step 7, i.e. update the edge attribute with the loss, so that edges which are more relevant in constructing the subgraph can be assigned a higher probability value.\\nAny suggestion would be highly appreciated.\\nThank you very much.\\n   \n",
       "15411                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Where can I find pre-trained agents able to play games with multiple stages like exploration, dialog, combat? My goal is to create an ML model to be able to classify different game stages, e.g., dialog with a non-player character, exploration, combat with enemy, in-game menu etc.\\nIn order to do that, I am looking for an agent pre-trained on such a game. I am intending to develop a model using this pre-trained agent to produce a data set (frames-labels) and finally I will use that data set to train a model to classify those different stages.\\nI could only find a pre-trained model for the Doom, however, it is not much appropriate for my case because it does not have different game stages (it is merely based on running & shooting).\\nTraining my own Reinforcement Learning Agent is a whole another workload in terms of both time and GPU such a game needs.\\nAny single idea could help me a lot. Thanks!\\n   \n",
       "\n",
       "       classification_flag  classification_score  \n",
       "1644                     1              0.006850  \n",
       "11443                    1              0.007639  \n",
       "17709                    1              0.009153  \n",
       "16816                    1              0.009729  \n",
       "2298                     1              0.010670  \n",
       "17192                    1              0.010794  \n",
       "16557                    1              0.011186  \n",
       "5401                     1              0.011937  \n",
       "11684                    1              0.012667  \n",
       "15411                    1              0.013029  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lowest scoring examples that do have the tag\n",
    "ai_posts[ai_posts[flag_col]==1].sort_values(score_col, ascending=True).head(10)[show_cols]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mladsjune2023cuda117",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
