{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["# Access stereotype data\n","\n","For MLADS June 2023 tutorial\n","\n","The attached lakeHouse embeddingLH has a shortcut to this container https://stereotypes4syn.blob.core.windows.net/stereotypes\n","\n","Demonstration in Trident of\n","- testing a shortcut by writing a file to it\n","- ... then:\n","- reading a raw file from github\n","- writing it to Lakehouse files\n","- unpickling it\n","- de-vectorizing the vector column, so it can be converted to a Spark DF\n","- ... write it back to a Table \n","- ... save it in ADLS storage. "]},{"cell_type":"code","execution_count":11,"metadata":{"cellStatus":"{\"John-Mark Agosta\":{\"queued_time\":\"2023-05-03T17:46:22.8699314Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-03T17:46:23.1511838Z\",\"execution_finish_time\":\"2023-05-03T17:46:23.5087186Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}","jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-05-03T17:46:23.5087186Z","execution_start_time":"2023-05-03T17:46:23.1511838Z","livy_statement_state":"available","parent_msg_id":"18f27ffd-c1d0-4dca-965a-83d52c1fc6ea","queued_time":"2023-05-03T17:46:22.8699314Z","session_id":"1e090f6f-6214-4af6-b1d7-18adcd1b65d6","session_start_time":null,"spark_jobs":{"jobs":[],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":0,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":13},"text/plain":["StatementMeta(, 1e090f6f-6214-4af6-b1d7-18adcd1b65d6, 13, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["import numpy as np\n","import pandas as pd\n","import json\n","import time\n","import pickle\n","import os\n","import subprocess\n","import requests\n","\n","from numpy.random import Generator, PCG64\n","rng = Generator(PCG64())\n"]},{"cell_type":"code","execution_count":14,"metadata":{"cellStatus":"{\"John-Mark Agosta\":{\"queued_time\":\"2023-05-03T17:51:22.3426897Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-03T17:51:22.6868615Z\",\"execution_finish_time\":\"2023-05-03T17:51:23.0568505Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}","jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-05-03T17:51:23.0568505Z","execution_start_time":"2023-05-03T17:51:22.6868615Z","livy_statement_state":"available","parent_msg_id":"55183369-5d88-4688-8991-ad6b640b83a1","queued_time":"2023-05-03T17:51:22.3426897Z","session_id":"1e090f6f-6214-4af6-b1d7-18adcd1b65d6","session_start_time":null,"spark_jobs":{"jobs":[],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":0,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":16},"text/plain":["StatementMeta(, 1e090f6f-6214-4af6-b1d7-18adcd1b65d6, 16, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["# Config from \"02 - Data Tranformation\" tutorial.  (Not sure what these do)\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.binSize\", \"1073741824\")"]},{"cell_type":"code","execution_count":15,"metadata":{"cellStatus":"{\"John-Mark Agosta\":{\"queued_time\":\"2023-05-03T17:55:41.0159477Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-03T17:55:41.33182Z\",\"execution_finish_time\":\"2023-05-03T17:55:44.0460078Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}","jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-05-03T17:55:44.0460078Z","execution_start_time":"2023-05-03T17:55:41.33182Z","livy_statement_state":"available","parent_msg_id":"868b493e-6e15-4d5e-abb7-683496e58538","queued_time":"2023-05-03T17:55:41.0159477Z","session_id":"1e090f6f-6214-4af6-b1d7-18adcd1b65d6","session_start_time":null,"spark_jobs":{"jobs":[],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":0,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":17},"text/plain":["StatementMeta(, 1e090f6f-6214-4af6-b1d7-18adcd1b65d6, 17, Finished, Available)"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["drw-r----- 3 trusted-service-user trusted-service-user 4096 May  3 17:35 /lakehouse/default/Files/stereotypes\n"]},{"data":{"text/plain":["0"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["# Check that the shortcut created in the LakeHouse pane exists\n","os.system('ls -ld /lakehouse/default/Files/stereotypes')"]},{"cell_type":"code","execution_count":17,"metadata":{"cellStatus":"{\"John-Mark Agosta\":{\"queued_time\":\"2023-05-03T17:56:36.4680585Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-03T17:56:36.7706128Z\",\"execution_finish_time\":\"2023-05-03T17:56:38.5408501Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}","collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-05-03T17:56:38.5408501Z","execution_start_time":"2023-05-03T17:56:36.7706128Z","livy_statement_state":"available","parent_msg_id":"f30556d2-f393-4f96-ba34-b96eb60b49e1","queued_time":"2023-05-03T17:56:36.4680585Z","session_id":"1e090f6f-6214-4af6-b1d7-18adcd1b65d6","session_start_time":null,"spark_jobs":{"jobs":[],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":0,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":19},"text/plain":["StatementMeta(, 1e090f6f-6214-4af6-b1d7-18adcd1b65d6, 19, Finished, Available)"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["total 4\n","-rwxrwx--- 1 trusted-service-user trusted-service-user 1701 May  3 17:56 tst_df.parquet\n"]},{"data":{"text/plain":["0"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["# Create a dataset and check that the shortcut is writable, and appears in the linked Storage\n","tst_df  = pd.DataFrame({'data': rng.binomial(10, 0.8, size=30) }, index = range(30))\n","# Write the pandas df back to the files folder\n","# This is cool, since the shortcut connects back to the external blob storage, and the data frame appears there. (e.g in Azure Storage Explorer.)\n","tst_df.to_parquet('/lakehouse/default/Files/stereotypes/tst_df.parquet')\n","# And tst that it's visible in the filesystem\n","os.system('ls -l /lakehouse/default/Files/stereotypes')"]},{"cell_type":"code","execution_count":18,"metadata":{"cellStatus":"{\"John-Mark Agosta\":{\"queued_time\":\"2023-05-03T17:58:30.772145Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-03T17:58:31.1245231Z\",\"execution_finish_time\":\"2023-05-03T17:58:33.1164658Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}","jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-05-03T17:58:33.1164658Z","execution_start_time":"2023-05-03T17:58:31.1245231Z","livy_statement_state":"available","parent_msg_id":"903b594a-7021-45d4-92ff-fbdbf33c484f","queued_time":"2023-05-03T17:58:30.772145Z","session_id":"1e090f6f-6214-4af6-b1d7-18adcd1b65d6","session_start_time":null,"spark_jobs":{"jobs":[],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":0,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":20},"text/plain":["StatementMeta(, 1e090f6f-6214-4af6-b1d7-18adcd1b65d6, 20, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["# Use the python requests package as an alternative to \"wget\" to read a github file\n","headers = {'Accept': 'application/vnd.github.v3.raw'}\n","# Note that the URL needs to end in \"raw=true\"  or you'll just get an html page for the file\n","stereotype_data_url = \"\"\"https://github.com/rmhorton/sentence-embedding-demos/blob/main/bias_detection/stereotype_data_long_float16.pkl?raw=true\"\"\"\n","r = requests.get(stereotype_data_url, headers=headers)"]},{"cell_type":"code","execution_count":19,"metadata":{"cellStatus":"{\"John-Mark Agosta\":{\"queued_time\":\"2023-05-03T18:01:26.9820169Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-03T18:01:27.2999078Z\",\"execution_finish_time\":\"2023-05-03T18:01:31.2155903Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}","jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-05-03T18:01:31.2155903Z","execution_start_time":"2023-05-03T18:01:27.2999078Z","livy_statement_state":"available","parent_msg_id":"9c1922f4-dfb6-4af5-8eba-1ed64ddb762e","queued_time":"2023-05-03T18:01:26.9820169Z","session_id":"1e090f6f-6214-4af6-b1d7-18adcd1b65d6","session_start_time":null,"spark_jobs":{"jobs":[],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":0,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":21},"text/plain":["StatementMeta(, 1e090f6f-6214-4af6-b1d7-18adcd1b65d6, 21, Finished, Available)"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["total 16108\n","-rwxrwx--- 1 trusted-service-user trusted-service-user 16490524 May  3 18:01 stereotype_data_long_float16.pkl\n","-rw-r----- 1 trusted-service-user trusted-service-user     1701 May  3 17:56 tst_df.parquet\n"]},{"data":{"text/plain":["0"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["# Save the bits using the Spark path to the Files directory\n","with open('/lakehouse/default/Files/stereotypes/stereotype_data_long_float16.pkl', 'wb') as sdl:\n","    sdl.write(r.content)\n","# Yes it made it there. You can also see it in the Trident file explorer on the right. \n","os.system('ls -l /lakehouse/default/Files/stereotypes')"]},{"cell_type":"code","execution_count":21,"metadata":{"cellStatus":"{\"John-Mark Agosta\":{\"queued_time\":\"2023-05-03T18:02:04.5875077Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-03T18:02:04.8958429Z\",\"execution_finish_time\":\"2023-05-03T18:02:05.279294Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}","jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-05-03T18:02:05.279294Z","execution_start_time":"2023-05-03T18:02:04.8958429Z","livy_statement_state":"available","parent_msg_id":"e401b8e2-b684-4b94-9525-2fe1dcaf35cc","queued_time":"2023-05-03T18:02:04.5875077Z","session_id":"1e090f6f-6214-4af6-b1d7-18adcd1b65d6","session_start_time":null,"spark_jobs":{"jobs":[],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":0,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":23},"text/plain":["StatementMeta(, 1e090f6f-6214-4af6-b1d7-18adcd1b65d6, 23, Finished, Available)"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["pandas.core.frame.DataFrame"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["# Unpickle the file - you get the pandas DF back. \n","with open('/lakehouse/default/Files/stereotypes/stereotype_data_long_float16.pkl', 'rb') as pkl_fd:\n","    stereotype_data =pickle.load(pkl_fd)\n","type(stereotype_data)"]},{"attachments":{},"cell_type":"markdown","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### Convert the data into a spark-usable Table\n","\n","Having copied the pkl file from Github and converted it to a pandas DataFrame, we then unpack the vector field (from string to multiple columns),\n","convert it to a Spark df and write it as a table. "]},{"cell_type":"code","execution_count":22,"metadata":{"cellStatus":"{\"John-Mark Agosta\":{\"queued_time\":\"2023-05-03T18:09:36.8629936Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-03T18:09:37.1766061Z\",\"execution_finish_time\":\"2023-05-03T18:09:38.1716395Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}","jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-05-03T18:09:38.1716395Z","execution_start_time":"2023-05-03T18:09:37.1766061Z","livy_statement_state":"available","parent_msg_id":"527e8740-4e57-4c81-8152-206bbe759b37","queued_time":"2023-05-03T18:09:36.8629936Z","session_id":"1e090f6f-6214-4af6-b1d7-18adcd1b65d6","session_start_time":null,"spark_jobs":{"jobs":[],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":0,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":24},"text/plain":["StatementMeta(, 1e090f6f-6214-4af6-b1d7-18adcd1b65d6, 24, Finished, Available)"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 9578 entries, 0 to 9577\n","Columns: 775 entries, stereotype_id to v767\n","dtypes: float64(768), int64(1), object(6)\n","memory usage: 56.7+ MB\n"]}],"source":["# The Spark DF doesn't know what to do with the vector column, so we convert it into multiple columns. \n","# unpack the vector column as individual columns -- assuming all are the same length.\n","embedding_size = len(stereotype_data.loc[0, 'vector'])\n","cases = stereotype_data.shape[0]\n","vectors = stereotype_data['vector']\n","pre_allocated_array = np.empty((cases, embedding_size))\n","\n","# Copy each row into the np array. \n","for k in range(cases):\n","    pre_allocated_array[k,:] = np.array(vectors[k])\n","v_col_names = [f'v{z}' for z in range(embedding_size)]\n","par_fd = pd.DataFrame(pre_allocated_array, columns=v_col_names)\n","# vstack the new columns in place of the previous vector column. \n","stereotype_vector_data = pd.concat([stereotype_data.drop('vector', axis=1), par_fd], axis=1)\n","stereotype_vector_data.info()"]},{"cell_type":"code","execution_count":23,"metadata":{"cellStatus":"{\"John-Mark Agosta\":{\"queued_time\":\"2023-05-03T18:09:58.3847976Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-03T18:09:58.7242534Z\",\"execution_finish_time\":\"2023-05-03T18:10:07.4079062Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}","jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-05-03T18:10:07.4079062Z","execution_start_time":"2023-05-03T18:09:58.7242534Z","livy_statement_state":"available","parent_msg_id":"8513ec2a-4fef-40a8-b343-63ad5a434d12","queued_time":"2023-05-03T18:09:58.3847976Z","session_id":"1e090f6f-6214-4af6-b1d7-18adcd1b65d6","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-05-03T18:10:06.052GMT","dataRead":8246,"dataWritten":0,"description":"Delta: Job group for statement 25:\n# Write the python dataframe to a spark table. \n# Hmm - this doesn't look like a table, instead, like a file (?)\nsdf_stereo_data = spark.createDataFrame(stereotype_vector_data)\nsdf_stereo_data.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/stereotype_data\"): Compute snapshot for version: 0","jobGroup":"25","jobId":15,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":51,"numTasks":52,"rowCount":50,"stageIds":[19,20,21],"status":"SUCCEEDED","submissionTime":"2023-05-03T18:10:06.004GMT"},{"completionTime":"2023-05-03T18:10:05.963GMT","dataRead":6494,"dataWritten":8246,"description":"Delta: Job group for statement 25:\n# Write the python dataframe to a spark table. \n# Hmm - this doesn't look like a table, instead, like a file (?)\nsdf_stereo_data = spark.createDataFrame(stereotype_vector_data)\nsdf_stereo_data.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/stereotype_data\"): Compute snapshot for version: 0","jobGroup":"25","jobId":14,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":54,"stageIds":[17,18],"status":"SUCCEEDED","submissionTime":"2023-05-03T18:10:05.022GMT"},{"completionTime":"2023-05-03T18:10:04.830GMT","dataRead":60328,"dataWritten":6494,"description":"Delta: Job group for statement 25:\n# Write the python dataframe to a spark table. \n# Hmm - this doesn't look like a table, instead, like a file (?)\nsdf_stereo_data = spark.createDataFrame(stereotype_vector_data)\nsdf_stereo_data.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/stereotype_data\"): Compute snapshot for version: 0","jobGroup":"25","jobId":13,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":8,"stageIds":[16],"status":"SUCCEEDED","submissionTime":"2023-05-03T18:10:04.746GMT"},{"completionTime":"2023-05-03T18:10:03.990GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 25:\n# Write the python dataframe to a spark table. \n# Hmm - this doesn't look like a table, instead, like a file (?)\nsdf_stereo_data = spark.createDataFrame(stereotype_vector_data)\nsdf_stereo_data.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/stereotype_data\")","jobGroup":"25","jobId":12,"killedTasksSummary":{},"name":"","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":0,"numCompletedStages":0,"numCompletedTasks":0,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":0,"rowCount":0,"stageIds":[],"status":"SUCCEEDED","submissionTime":"2023-05-03T18:10:03.990GMT"},{"completionTime":"2023-05-03T18:10:03.798GMT","dataRead":29648143,"dataWritten":29647000,"description":"Job group for statement 25:\n# Write the python dataframe to a spark table. \n# Hmm - this doesn't look like a table, instead, like a file (?)\nsdf_stereo_data = spark.createDataFrame(stereotype_vector_data)\nsdf_stereo_data.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/stereotype_data\")","jobGroup":"25","jobId":11,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":8,"numTasks":9,"rowCount":19156,"stageIds":[15,14],"status":"SUCCEEDED","submissionTime":"2023-05-03T18:10:01.270GMT"},{"completionTime":"2023-05-03T18:10:01.201GMT","dataRead":0,"dataWritten":29648143,"description":"Job group for statement 25:\n# Write the python dataframe to a spark table. \n# Hmm - this doesn't look like a table, instead, like a file (?)\nsdf_stereo_data = spark.createDataFrame(stereotype_vector_data)\nsdf_stereo_data.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/stereotype_data\")","jobGroup":"25","jobId":10,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":8,"numCompletedStages":1,"numCompletedTasks":8,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":8,"rowCount":9578,"stageIds":[13],"status":"SUCCEEDED","submissionTime":"2023-05-03T18:10:00.203GMT"}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":6,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":25},"text/plain":["StatementMeta(, 1e090f6f-6214-4af6-b1d7-18adcd1b65d6, 25, Finished, Available)"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n"]}],"source":["# Write the python dataframe to a spark table. \n","# Hmm - this doesn't look like a table, instead, like a file in the Tables section (?)\n","# .. but the lakehouse did do an automatic conversion to a table, visible in the Tables directory. \n","sdf_stereo_data = spark.createDataFrame(stereotype_vector_data)\n","sdf_stereo_data.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/stereotype_data\")"]},{"cell_type":"code","execution_count":24,"metadata":{"cellStatus":"{\"John-Mark Agosta\":{\"queued_time\":\"2023-05-03T18:23:13.0325296Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-03T18:23:13.3959467Z\",\"execution_finish_time\":\"2023-05-03T18:23:22.0745172Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}","jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-05-03T18:23:22.0745172Z","execution_start_time":"2023-05-03T18:23:13.3959467Z","livy_statement_state":"available","parent_msg_id":"383fa9d1-0bde-453f-a5bf-3f083deab600","queued_time":"2023-05-03T18:23:13.0325296Z","session_id":"1e090f6f-6214-4af6-b1d7-18adcd1b65d6","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-05-03T18:23:20.724GMT","dataRead":8246,"dataWritten":0,"description":"Delta: Job group for statement 26:\n#Instead, lets try writing the Spark df out when there's a shortcut to a subfolder in Tables\nsdf_stereo_data.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/stereotypetable/stereotype\")\n: Compute snapshot for version: 0","jobGroup":"26","jobId":21,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":51,"numTasks":52,"rowCount":50,"stageIds":[30,28,29],"status":"SUCCEEDED","submissionTime":"2023-05-03T18:23:20.685GMT"},{"completionTime":"2023-05-03T18:23:20.666GMT","dataRead":6496,"dataWritten":8246,"description":"Delta: Job group for statement 26:\n#Instead, lets try writing the Spark df out when there's a shortcut to a subfolder in Tables\nsdf_stereo_data.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/stereotypetable/stereotype\")\n: Compute snapshot for version: 0","jobGroup":"26","jobId":20,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":54,"stageIds":[27,26],"status":"SUCCEEDED","submissionTime":"2023-05-03T18:23:19.988GMT"},{"completionTime":"2023-05-03T18:23:19.850GMT","dataRead":60328,"dataWritten":6496,"description":"Delta: Job group for statement 26:\n#Instead, lets try writing the Spark df out when there's a shortcut to a subfolder in Tables\nsdf_stereo_data.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/stereotypetable/stereotype\")\n: Compute snapshot for version: 0","jobGroup":"26","jobId":19,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":8,"stageIds":[25],"status":"SUCCEEDED","submissionTime":"2023-05-03T18:23:19.702GMT"},{"completionTime":"2023-05-03T18:23:18.612GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 26:\n#Instead, lets try writing the Spark df out when there's a shortcut to a subfolder in Tables\nsdf_stereo_data.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/stereotypetable/stereotype\")\n","jobGroup":"26","jobId":18,"killedTasksSummary":{},"name":"","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":0,"numCompletedStages":0,"numCompletedTasks":0,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":0,"rowCount":0,"stageIds":[],"status":"SUCCEEDED","submissionTime":"2023-05-03T18:23:18.612GMT"},{"completionTime":"2023-05-03T18:23:18.440GMT","dataRead":29648143,"dataWritten":29647000,"description":"Job group for statement 26:\n#Instead, lets try writing the Spark df out when there's a shortcut to a subfolder in Tables\nsdf_stereo_data.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/stereotypetable/stereotype\")\n","jobGroup":"26","jobId":17,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":8,"numTasks":9,"rowCount":19156,"stageIds":[24,23],"status":"SUCCEEDED","submissionTime":"2023-05-03T18:23:14.955GMT"},{"completionTime":"2023-05-03T18:23:14.899GMT","dataRead":0,"dataWritten":29648143,"description":"Job group for statement 26:\n#Instead, lets try writing the Spark df out when there's a shortcut to a subfolder in Tables\nsdf_stereo_data.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/stereotypetable/stereotype\")\n","jobGroup":"26","jobId":16,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":8,"numCompletedStages":1,"numCompletedTasks":8,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":8,"rowCount":9578,"stageIds":[22],"status":"SUCCEEDED","submissionTime":"2023-05-03T18:23:14.137GMT"}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":6,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":26},"text/plain":["StatementMeta(, 1e090f6f-6214-4af6-b1d7-18adcd1b65d6, 26, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["#Instead, lets try writing the Spark df out when there's a shortcut to a subfolder in Tables\n","sdf_stereo_data.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/stereotypetable/stereotype\")\n","# Strange, it seems to have moved the spark df to the new shortcut. \n"]}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","name":"synapse_pyspark"},"language_info":{"name":"python"},"notebook_environment":{},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"synapse_widget":{"state":{},"version":"0.1"},"trident":{"lakehouse":{"default_lakehouse":"563b4686-f32e-4690-b6eb-2c3b0a08d762","default_lakehouse_name":"embeddingLH","default_lakehouse_workspace_id":"fdf4860d-963e-4e5e-8584-cba7f8547520","known_lakehouses":[{"id":"563b4686-f32e-4690-b6eb-2c3b0a08d762"}]}},"widgets":{}},"nbformat":4,"nbformat_minor":0}
