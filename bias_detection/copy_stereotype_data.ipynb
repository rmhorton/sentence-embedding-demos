{"cells":[{"cell_type":"markdown","source":["# Access stereotype data\n","\n","For MLADS June 2023 tutorial\n","\n","The attached lakeHouse embeddingLH has a shortcut to this container https://stereotypes4syn.blob.core.windows.net/stereotypes\n","\n","Demonstration in Trident of\n","- testing a shortcut by writing a file to it\n","- ... then:\n","- reading a raw file from github\n","- writing it to Lakehouse files\n","- unpickling it\n","- de-vectorizing the vector column, so it can be converted to a Spark DF\n","- ... write it back to a Table \n","- ... save it in ADLS storage. "],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["import numpy as np\r\n","import pandas as pd\r\n","import json\r\n","import time\r\n","import pickle\r\n","import os\r\n","import subprocess\r\n","import requests\r\n","\r\n","from numpy.random import Generator, PCG64\r\n","rng = Generator(PCG64())\r\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"1e090f6f-6214-4af6-b1d7-18adcd1b65d6","statement_id":13,"state":"finished","livy_statement_state":"available","queued_time":"2023-05-03T17:46:22.8699314Z","session_start_time":null,"execution_start_time":"2023-05-03T17:46:23.1511838Z","execution_finish_time":"2023-05-03T17:46:23.5087186Z","spark_jobs":{"numbers":{"RUNNING":0,"UNKNOWN":0,"FAILED":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"18f27ffd-c1d0-4dca-965a-83d52c1fc6ea"},"text/plain":"StatementMeta(, 1e090f6f-6214-4af6-b1d7-18adcd1b65d6, 13, Finished, Available)"},"metadata":{}}],"execution_count":11,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"John-Mark Agosta\":{\"queued_time\":\"2023-05-03T17:46:22.8699314Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-03T17:46:23.1511838Z\",\"execution_finish_time\":\"2023-05-03T17:46:23.5087186Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}"}},{"cell_type":"code","source":["# Config from \"02 - Data Tranformation\" tutorial.  (Not sure what these do)\r\n","spark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.binSize\", \"1073741824\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"1e090f6f-6214-4af6-b1d7-18adcd1b65d6","statement_id":16,"state":"finished","livy_statement_state":"available","queued_time":"2023-05-03T17:51:22.3426897Z","session_start_time":null,"execution_start_time":"2023-05-03T17:51:22.6868615Z","execution_finish_time":"2023-05-03T17:51:23.0568505Z","spark_jobs":{"numbers":{"RUNNING":0,"UNKNOWN":0,"FAILED":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"55183369-5d88-4688-8991-ad6b640b83a1"},"text/plain":"StatementMeta(, 1e090f6f-6214-4af6-b1d7-18adcd1b65d6, 16, Finished, Available)"},"metadata":{}}],"execution_count":14,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"John-Mark Agosta\":{\"queued_time\":\"2023-05-03T17:51:22.3426897Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-03T17:51:22.6868615Z\",\"execution_finish_time\":\"2023-05-03T17:51:23.0568505Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}"}},{"cell_type":"code","source":["# Check that the shortcut created in the LakeHouse pane exists\r\n","os.system('ls -ld /lakehouse/default/Files/stereotypes')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"1e090f6f-6214-4af6-b1d7-18adcd1b65d6","statement_id":17,"state":"finished","livy_statement_state":"available","queued_time":"2023-05-03T17:55:41.0159477Z","session_start_time":null,"execution_start_time":"2023-05-03T17:55:41.33182Z","execution_finish_time":"2023-05-03T17:55:44.0460078Z","spark_jobs":{"numbers":{"RUNNING":0,"UNKNOWN":0,"FAILED":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"868b493e-6e15-4d5e-abb7-683496e58538"},"text/plain":"StatementMeta(, 1e090f6f-6214-4af6-b1d7-18adcd1b65d6, 17, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["drw-r----- 3 trusted-service-user trusted-service-user 4096 May  3 17:35 /lakehouse/default/Files/stereotypes\n"]},{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"0"},"metadata":{}}],"execution_count":15,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"John-Mark Agosta\":{\"queued_time\":\"2023-05-03T17:55:41.0159477Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-03T17:55:41.33182Z\",\"execution_finish_time\":\"2023-05-03T17:55:44.0460078Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}"}},{"cell_type":"code","source":["# Create a dataset and check that the shortcut is writable, and appears in the linked Storage\r\n","tst_df  = pd.DataFrame({'data': rng.binomial(10, 0.8, size=30) }, index = range(30))\r\n","# Write the pandas df back to the files folder\r\n","# This is cool, since the shortcut connects back to the external blob storage, and the data frame appears there. (e.g in Azure Storage Explorer.)\r\n","tst_df.to_parquet('/lakehouse/default/Files/stereotypes/tst_df.parquet')\r\n","# And tst that it's visible in the filesystem\r\n","os.system('ls -l /lakehouse/default/Files/stereotypes')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"1e090f6f-6214-4af6-b1d7-18adcd1b65d6","statement_id":19,"state":"finished","livy_statement_state":"available","queued_time":"2023-05-03T17:56:36.4680585Z","session_start_time":null,"execution_start_time":"2023-05-03T17:56:36.7706128Z","execution_finish_time":"2023-05-03T17:56:38.5408501Z","spark_jobs":{"numbers":{"RUNNING":0,"UNKNOWN":0,"FAILED":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"f30556d2-f393-4f96-ba34-b96eb60b49e1"},"text/plain":"StatementMeta(, 1e090f6f-6214-4af6-b1d7-18adcd1b65d6, 19, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["total 4\n-rwxrwx--- 1 trusted-service-user trusted-service-user 1701 May  3 17:56 tst_df.parquet\n"]},{"output_type":"execute_result","execution_count":38,"data":{"text/plain":"0"},"metadata":{}}],"execution_count":17,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"John-Mark Agosta\":{\"queued_time\":\"2023-05-03T17:56:36.4680585Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-03T17:56:36.7706128Z\",\"execution_finish_time\":\"2023-05-03T17:56:38.5408501Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}","collapsed":false}},{"cell_type":"code","source":["# Use the python requests package as an alternative to \"wget\" to read a github file\r\n","headers = {'Accept': 'application/vnd.github.v3.raw'}\r\n","# Note that the URL needs to end in \"raw=true\"  or you'll just get an html page for the file\r\n","stereotype_data_url = \"\"\"https://github.com/rmhorton/sentence-embedding-demos/blob/main/bias_detection/stereotype_data_long_float16.pkl?raw=true\"\"\"\r\n","r = requests.get(stereotype_data_url, headers=headers)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"1e090f6f-6214-4af6-b1d7-18adcd1b65d6","statement_id":20,"state":"finished","livy_statement_state":"available","queued_time":"2023-05-03T17:58:30.772145Z","session_start_time":null,"execution_start_time":"2023-05-03T17:58:31.1245231Z","execution_finish_time":"2023-05-03T17:58:33.1164658Z","spark_jobs":{"numbers":{"RUNNING":0,"UNKNOWN":0,"FAILED":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"903b594a-7021-45d4-92ff-fbdbf33c484f"},"text/plain":"StatementMeta(, 1e090f6f-6214-4af6-b1d7-18adcd1b65d6, 20, Finished, Available)"},"metadata":{}}],"execution_count":18,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"John-Mark Agosta\":{\"queued_time\":\"2023-05-03T17:58:30.772145Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-03T17:58:31.1245231Z\",\"execution_finish_time\":\"2023-05-03T17:58:33.1164658Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}"}},{"cell_type":"code","source":["# Save the bits using the Spark path to the Files directory\r\n","with open('/lakehouse/default/Files/stereotypes/stereotype_data_long_float16.pkl', 'wb') as sdl:\r\n","    sdl.write(r.content)\r\n","# Yes it made it there. You can also see it in the Trident file explorer on the right. \r\n","os.system('ls -l /lakehouse/default/Files/stereotypes')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"1e090f6f-6214-4af6-b1d7-18adcd1b65d6","statement_id":21,"state":"finished","livy_statement_state":"available","queued_time":"2023-05-03T18:01:26.9820169Z","session_start_time":null,"execution_start_time":"2023-05-03T18:01:27.2999078Z","execution_finish_time":"2023-05-03T18:01:31.2155903Z","spark_jobs":{"numbers":{"RUNNING":0,"UNKNOWN":0,"FAILED":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"9c1922f4-dfb6-4af5-8eba-1ed64ddb762e"},"text/plain":"StatementMeta(, 1e090f6f-6214-4af6-b1d7-18adcd1b65d6, 21, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["total 16108\n-rwxrwx--- 1 trusted-service-user trusted-service-user 16490524 May  3 18:01 stereotype_data_long_float16.pkl\n-rw-r----- 1 trusted-service-user trusted-service-user     1701 May  3 17:56 tst_df.parquet\n"]},{"output_type":"execute_result","execution_count":42,"data":{"text/plain":"0"},"metadata":{}}],"execution_count":19,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"John-Mark Agosta\":{\"queued_time\":\"2023-05-03T18:01:26.9820169Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-03T18:01:27.2999078Z\",\"execution_finish_time\":\"2023-05-03T18:01:31.2155903Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}"}},{"cell_type":"code","source":["# Unpickle the file - you get the pandas DF back. \r\n","with open('/lakehouse/default/Files/stereotypes/stereotype_data_long_float16.pkl', 'rb') as pkl_fd:\r\n","    stereotype_data =pickle.load(pkl_fd)\r\n","type(stereotype_data)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"1e090f6f-6214-4af6-b1d7-18adcd1b65d6","statement_id":23,"state":"finished","livy_statement_state":"available","queued_time":"2023-05-03T18:02:04.5875077Z","session_start_time":null,"execution_start_time":"2023-05-03T18:02:04.8958429Z","execution_finish_time":"2023-05-03T18:02:05.279294Z","spark_jobs":{"numbers":{"RUNNING":0,"UNKNOWN":0,"FAILED":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"e401b8e2-b684-4b94-9525-2fe1dcaf35cc"},"text/plain":"StatementMeta(, 1e090f6f-6214-4af6-b1d7-18adcd1b65d6, 23, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":46,"data":{"text/plain":"pandas.core.frame.DataFrame"},"metadata":{}}],"execution_count":21,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"John-Mark Agosta\":{\"queued_time\":\"2023-05-03T18:02:04.5875077Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-03T18:02:04.8958429Z\",\"execution_finish_time\":\"2023-05-03T18:02:05.279294Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}"}},{"cell_type":"markdown","source":["### Convert the data into a spark-usable Table\r\n","\r\n","Having copied the pkl file from Github and converted it to a pandas DataFrame, we then unpack the vector field (from string to multiple columns),\r\n","convert it to a Spark df and write it as a table. "],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["# The Spark DF doesn't know what to do with the vector column, so we convert it into multiple columns. \r\n","# unpack the vector column as individual columns -- assuming all are the same length.\r\n","embedding_size = len(stereotype_data.loc[0, 'vector'])\r\n","cases = stereotype_data.shape[0]\r\n","vectors = stereotype_data['vector']\r\n","pre_allocated_array = np.empty((cases, embedding_size))\r\n","\r\n","# Copy each row into the np array. \r\n","for k in range(cases):\r\n","    pre_allocated_array[k,:] = np.array(vectors[k])\r\n","v_col_names = [f'v{z}' for z in range(embedding_size)]\r\n","par_fd = pd.DataFrame(pre_allocated_array, columns=v_col_names)\r\n","# vstack the new columns in place of the previous vector column. \r\n","stereotype_vector_data = pd.concat([stereotype_data.drop('vector', axis=1), par_fd], axis=1)\r\n","stereotype_vector_data.info()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"1e090f6f-6214-4af6-b1d7-18adcd1b65d6","statement_id":24,"state":"finished","livy_statement_state":"available","queued_time":"2023-05-03T18:09:36.8629936Z","session_start_time":null,"execution_start_time":"2023-05-03T18:09:37.1766061Z","execution_finish_time":"2023-05-03T18:09:38.1716395Z","spark_jobs":{"numbers":{"RUNNING":0,"UNKNOWN":0,"FAILED":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"527e8740-4e57-4c81-8152-206bbe759b37"},"text/plain":"StatementMeta(, 1e090f6f-6214-4af6-b1d7-18adcd1b65d6, 24, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\nInt64Index: 9578 entries, 0 to 9577\nColumns: 775 entries, stereotype_id to v767\ndtypes: float64(768), int64(1), object(6)\nmemory usage: 56.7+ MB\n"]}],"execution_count":22,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"John-Mark Agosta\":{\"queued_time\":\"2023-05-03T18:09:36.8629936Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-03T18:09:37.1766061Z\",\"execution_finish_time\":\"2023-05-03T18:09:38.1716395Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}"}},{"cell_type":"code","source":["# Write the python dataframe to a spark table. \r\n","# Hmm - this doesn't look like a table, instead, like a file in the Tables section (?)\r\n","# .. but the lakehouse did do an automatic conversion to a table, visible in the Tables directory. \r\n","sdf_stereo_data = spark.createDataFrame(stereotype_vector_data)\r\n","sdf_stereo_data.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/stereotype_data\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"1e090f6f-6214-4af6-b1d7-18adcd1b65d6","statement_id":25,"state":"finished","livy_statement_state":"available","queued_time":"2023-05-03T18:09:58.3847976Z","session_start_time":null,"execution_start_time":"2023-05-03T18:09:58.7242534Z","execution_finish_time":"2023-05-03T18:10:07.4079062Z","spark_jobs":{"numbers":{"RUNNING":0,"UNKNOWN":0,"FAILED":0,"SUCCEEDED":6},"jobs":[{"dataWritten":0,"dataRead":8246,"rowCount":50,"jobId":15,"name":"toString at String.java:2994","description":"Delta: Job group for statement 25:\n# Write the python dataframe to a spark table. \n# Hmm - this doesn't look like a table, instead, like a file (?)\nsdf_stereo_data = spark.createDataFrame(stereotype_vector_data)\nsdf_stereo_data.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/stereotype_data\"): Compute snapshot for version: 0","submissionTime":"2023-05-03T18:10:06.004GMT","completionTime":"2023-05-03T18:10:06.052GMT","stageIds":[19,20,21],"jobGroup":"25","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":8246,"dataRead":6494,"rowCount":54,"jobId":14,"name":"toString at String.java:2994","description":"Delta: Job group for statement 25:\n# Write the python dataframe to a spark table. \n# Hmm - this doesn't look like a table, instead, like a file (?)\nsdf_stereo_data = spark.createDataFrame(stereotype_vector_data)\nsdf_stereo_data.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/stereotype_data\"): Compute snapshot for version: 0","submissionTime":"2023-05-03T18:10:05.022GMT","completionTime":"2023-05-03T18:10:05.963GMT","stageIds":[17,18],"jobGroup":"25","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":6494,"dataRead":60328,"rowCount":8,"jobId":13,"name":"toString at String.java:2994","description":"Delta: Job group for statement 25:\n# Write the python dataframe to a spark table. \n# Hmm - this doesn't look like a table, instead, like a file (?)\nsdf_stereo_data = spark.createDataFrame(stereotype_vector_data)\nsdf_stereo_data.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/stereotype_data\"): Compute snapshot for version: 0","submissionTime":"2023-05-03T18:10:04.746GMT","completionTime":"2023-05-03T18:10:04.830GMT","stageIds":[16],"jobGroup":"25","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":12,"name":"","description":"Job group for statement 25:\n# Write the python dataframe to a spark table. \n# Hmm - this doesn't look like a table, instead, like a file (?)\nsdf_stereo_data = spark.createDataFrame(stereotype_vector_data)\nsdf_stereo_data.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/stereotype_data\")","submissionTime":"2023-05-03T18:10:03.990GMT","completionTime":"2023-05-03T18:10:03.990GMT","stageIds":[],"jobGroup":"25","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":29647000,"dataRead":29648143,"rowCount":19156,"jobId":11,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 25:\n# Write the python dataframe to a spark table. \n# Hmm - this doesn't look like a table, instead, like a file (?)\nsdf_stereo_data = spark.createDataFrame(stereotype_vector_data)\nsdf_stereo_data.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/stereotype_data\")","submissionTime":"2023-05-03T18:10:01.270GMT","completionTime":"2023-05-03T18:10:03.798GMT","stageIds":[15,14],"jobGroup":"25","status":"SUCCEEDED","numTasks":9,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":29648143,"dataRead":0,"rowCount":9578,"jobId":10,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 25:\n# Write the python dataframe to a spark table. \n# Hmm - this doesn't look like a table, instead, like a file (?)\nsdf_stereo_data = spark.createDataFrame(stereotype_vector_data)\nsdf_stereo_data.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/stereotype_data\")","submissionTime":"2023-05-03T18:10:00.203GMT","completionTime":"2023-05-03T18:10:01.201GMT","stageIds":[13],"jobGroup":"25","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"8513ec2a-4fef-40a8-b343-63ad5a434d12"},"text/plain":"StatementMeta(, 1e090f6f-6214-4af6-b1d7-18adcd1b65d6, 25, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n"]}],"execution_count":23,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"John-Mark Agosta\":{\"queued_time\":\"2023-05-03T18:09:58.3847976Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-03T18:09:58.7242534Z\",\"execution_finish_time\":\"2023-05-03T18:10:07.4079062Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}"}},{"cell_type":"code","source":["#Instead, lets try writing the Spark df out when there's a shortcut to a subfolder in Tables\r\n","sdf_stereo_data.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/stereotypetable/stereotype\")\r\n","# Strange, it seems to have moved the spark df to the new shortcut. \r\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"1e090f6f-6214-4af6-b1d7-18adcd1b65d6","statement_id":26,"state":"finished","livy_statement_state":"available","queued_time":"2023-05-03T18:23:13.0325296Z","session_start_time":null,"execution_start_time":"2023-05-03T18:23:13.3959467Z","execution_finish_time":"2023-05-03T18:23:22.0745172Z","spark_jobs":{"numbers":{"RUNNING":0,"UNKNOWN":0,"FAILED":0,"SUCCEEDED":6},"jobs":[{"dataWritten":0,"dataRead":8246,"rowCount":50,"jobId":21,"name":"toString at String.java:2994","description":"Delta: Job group for statement 26:\n#Instead, lets try writing the Spark df out when there's a shortcut to a subfolder in Tables\nsdf_stereo_data.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/stereotypetable/stereotype\")\n: Compute snapshot for version: 0","submissionTime":"2023-05-03T18:23:20.685GMT","completionTime":"2023-05-03T18:23:20.724GMT","stageIds":[30,28,29],"jobGroup":"26","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":8246,"dataRead":6496,"rowCount":54,"jobId":20,"name":"toString at String.java:2994","description":"Delta: Job group for statement 26:\n#Instead, lets try writing the Spark df out when there's a shortcut to a subfolder in Tables\nsdf_stereo_data.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/stereotypetable/stereotype\")\n: Compute snapshot for version: 0","submissionTime":"2023-05-03T18:23:19.988GMT","completionTime":"2023-05-03T18:23:20.666GMT","stageIds":[27,26],"jobGroup":"26","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":6496,"dataRead":60328,"rowCount":8,"jobId":19,"name":"toString at String.java:2994","description":"Delta: Job group for statement 26:\n#Instead, lets try writing the Spark df out when there's a shortcut to a subfolder in Tables\nsdf_stereo_data.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/stereotypetable/stereotype\")\n: Compute snapshot for version: 0","submissionTime":"2023-05-03T18:23:19.702GMT","completionTime":"2023-05-03T18:23:19.850GMT","stageIds":[25],"jobGroup":"26","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":18,"name":"","description":"Job group for statement 26:\n#Instead, lets try writing the Spark df out when there's a shortcut to a subfolder in Tables\nsdf_stereo_data.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/stereotypetable/stereotype\")\n","submissionTime":"2023-05-03T18:23:18.612GMT","completionTime":"2023-05-03T18:23:18.612GMT","stageIds":[],"jobGroup":"26","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":29647000,"dataRead":29648143,"rowCount":19156,"jobId":17,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 26:\n#Instead, lets try writing the Spark df out when there's a shortcut to a subfolder in Tables\nsdf_stereo_data.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/stereotypetable/stereotype\")\n","submissionTime":"2023-05-03T18:23:14.955GMT","completionTime":"2023-05-03T18:23:18.440GMT","stageIds":[24,23],"jobGroup":"26","status":"SUCCEEDED","numTasks":9,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":29648143,"dataRead":0,"rowCount":9578,"jobId":16,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 26:\n#Instead, lets try writing the Spark df out when there's a shortcut to a subfolder in Tables\nsdf_stereo_data.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/stereotypetable/stereotype\")\n","submissionTime":"2023-05-03T18:23:14.137GMT","completionTime":"2023-05-03T18:23:14.899GMT","stageIds":[22],"jobGroup":"26","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"383fa9d1-0bde-453f-a5bf-3f083deab600"},"text/plain":"StatementMeta(, 1e090f6f-6214-4af6-b1d7-18adcd1b65d6, 26, Finished, Available)"},"metadata":{}}],"execution_count":24,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"John-Mark Agosta\":{\"queued_time\":\"2023-05-03T18:23:13.0325296Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-05-03T18:23:13.3959467Z\",\"execution_finish_time\":\"2023-05-03T18:23:22.0745172Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}"}}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{"default_lakehouse":"563b4686-f32e-4690-b6eb-2c3b0a08d762","known_lakehouses":[{"id":"563b4686-f32e-4690-b6eb-2c3b0a08d762"}],"default_lakehouse_workspace_id":"fdf4860d-963e-4e5e-8584-cba7f8547520","default_lakehouse_name":"embeddingLH"}}},"nbformat":4,"nbformat_minor":0}